{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumshin-dev/python_conda_jupyter/blob/main/codeit/%EB%AF%B8%EC%85%9810.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf684675",
        "outputId": "e96389c0-5068-4778-9625-9ebaa255bbdc"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uHexIfa1BdV",
        "outputId": "eb1bf10e-6da5-4c21-bd77-2f074b41921e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import re\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from gensim.models import Word2Vec, FastText\n",
        "\n",
        "# í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ\n",
        "nltk.download('wordnet')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgQA0Uv-g9PT",
        "outputId": "fef4a8c1-cb97-4e41-ac37-46c54ab0adf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# GPU ì„¤ì •\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_af2Pk0cSnC"
      },
      "source": [
        "# 1.ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLdKSQXCcRMB"
      },
      "outputs": [],
      "source": [
        "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "news_data = fetch_20newsgroups(subset='all',\n",
        "# headers, footers, quotesë¥¼ ì œê±°í•´ì•¼ ëª¨ë¸ì´ í…ìŠ¤íŠ¸ ë‚´ìš©ì—ë§Œ ì§‘ì¤‘í•œë‹¤.\n",
        "                               remove=('headers',\n",
        "                                       'footers',\n",
        "                                       'quotes')\n",
        "                               )\n",
        "texts = news_data.data     # ë¬¸ì„œ ë‚´ìš©-ë‰´ìŠ¤ ë³¸ë¬¸(ë¦¬ìŠ¤íŠ¸)\n",
        "labels = news_data.target  # ë ˆì´ë¸”-ì¹´í…Œê³ ë¦¬ ì •ë‹µ(0~19 ìˆ«ì)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# news_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "print('ë°ì´í„° ê°œìˆ˜:', type(news_data.data), len(news_data.data))  # ë‰´ìŠ¤ ë³¸ë¬¸ ë¦¬ìŠ¤íŠ¸, ê°œìˆ˜ ì¶œë ¥\n",
        "print('ì¹´í…Œê³ ë¦¬ ê°œìˆ˜:', type(news_data.target), len(news_data.target))\n",
        "print(news_data.target_names)                    # ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥\n",
        "print(f\"Unique labels: {set(labels)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY3An8GtQo3f",
        "outputId": "dced48e4-f0b0-49d7-afb6-dec644fedfbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë°ì´í„° ê°œìˆ˜: <class 'list'> 18846\n",
            "ì¹´í…Œê³ ë¦¬ ê°œìˆ˜: <class 'numpy.ndarray'> 18846\n",
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "Unique labels: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGxNxGKLdJ0K"
      },
      "outputs": [],
      "source": [
        "# ë°ì´í„° ë¶„í• \n",
        "train_inputs, test_inputs, train_targets, test_targets = train_test_split(texts, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo5fKPzBkiGp"
      },
      "outputs": [],
      "source": [
        "# stop_words = stopwords.words('english')\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬\n",
        "def clean_text(text):\n",
        "    # ì†Œë¬¸ì ë³€í™˜\n",
        "    text = text.lower()\n",
        "    # ì •ê·œì‹: ì•ŒíŒŒë²³ê³¼ ê³µë°±ë§Œ ë‚¨ê¹€\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # ë¶ˆìš©ì–´ ì œê±°\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # ê³µë°± ê¸°ì¤€ ê°„ë‹¨ í† í°í™”\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# ë°ì´í„°ì— ì ìš© (1)\n",
        "train_inputs = [clean_text(text) for text in train_inputs]\n",
        "test_inputs = [clean_text(text) for text in test_inputs]\n",
        "\n",
        "# ë‹¨ì–´ í† í°í™”ë§Œ ì§„í–‰ (2)\n",
        "# train_sentences = [word_tokenize(text) for text in train_inputs]\n",
        "# test_sentences = [word_tokenize(text) for text in test_inputs]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISNLj8m-1GEw"
      },
      "source": [
        "# Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktjo91291FXB"
      },
      "outputs": [],
      "source": [
        "# ë‹¨ì–´ í† í°í™”\n",
        "train_sentences = [word_tokenize(text) for text in train_inputs]\n",
        "test_sentences = [word_tokenize(text) for text in test_inputs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI5SM6TM1Iyp"
      },
      "outputs": [],
      "source": [
        "# Word2Vec ëª¨ë¸ í•™ìŠµ\n",
        "word2vec_model = Word2Vec(\n",
        "    sentences=train_sentences,  # í•™ìŠµ ë°ì´í„° (í† í°í™”ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸)\n",
        "    vector_size=128,            # ì„ë² ë”© ì°¨ì› (128ì°¨ì› ë²¡í„°)\n",
        "    window=5,                   # ì£¼ë³€ ë‹¨ì–´ ê³ ë ¤ ë²”ìœ„ (context window)\n",
        "    min_count=1,                # ìµœì†Œ ë“±ì¥ íšŸìˆ˜ (1íšŒ ì´ìƒ ë“±ì¥í•œ ë‹¨ì–´ë§Œ í•™ìŠµ)\n",
        "    sg=1                        # í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ (1=Skip-gram, 0=CBOW)\n",
        ")\n",
        "# ì„ë² ë”© í–‰ë ¬ ì´ˆê¸°í™”\n",
        "word2vec_matrix = np.zeros((len(word2vec_model.wv) + 1, 128))\n",
        "# ë‹¨ì–´ â†’ ì¸ë±ìŠ¤ ë§¤í•‘ (word2idx)\n",
        "word2idx_word2vec = {word: idx + 1 for idx, word in enumerate(word2vec_model.wv.index_to_key)}\n",
        "\n",
        "for word, idx in word2idx_word2vec.items():\n",
        "    word2vec_matrix[idx] = word2vec_model.wv[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_xwAKqJ1FZi"
      },
      "outputs": [],
      "source": [
        "# Dataset í´ë˜ìŠ¤ ì •ì˜\n",
        "class TextEmbeddingDataset(Dataset):\n",
        "    def __init__(self, texts, labels, word2idx, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = word_tokenize(self.texts[idx])\n",
        "        # OOV ë‹¨ì–´ëŠ” 0\n",
        "        encoded = [self.word2idx.get(word, 0) for word in tokens]  # OOV ë‹¨ì–´ëŠ” 0\n",
        "        if len(encoded) < self.max_len:\n",
        "            encoded += [0] * (self.max_len - len(encoded))# íŒ¨ë”©\n",
        "        else:\n",
        "            encoded = encoded[:self.max_len]# ì˜ë¼ë‚´ê¸°\n",
        "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzbsBVPo1Fbm"
      },
      "outputs": [],
      "source": [
        "# DataLoader ì¤€ë¹„\n",
        "max_len = 280 # ë¶„ìœ„ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì„¤ì •(95%:281.25)\n",
        "\n",
        "# Word2Vec Dataset\n",
        "train_dataset_word2vec = TextEmbeddingDataset(train_inputs, train_targets, word2idx_word2vec, max_len)\n",
        "test_dataset_word2vec = TextEmbeddingDataset(test_inputs, test_targets, word2idx_word2vec, max_len)\n",
        "\n",
        "# Word2Vec DataLoader\n",
        "train_loader_word2vec = DataLoader(train_dataset_word2vec, batch_size=64, shuffle=True)\n",
        "test_loader_word2vec = DataLoader(test_dataset_word2vec, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRzmVt02p5Ih"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for texts, labels in loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)  # ì¶œë ¥: (batch_size, 20)\n",
        "        loss = criterion(outputs, labels)  # ë ˆì´ë¸”ì€ ì •ìˆ˜í˜• (0~19)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            predictions = torch.argmax(outputs, dim=1)  # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ì¸ë±ìŠ¤\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pjsOc5h1FeS"
      },
      "outputs": [],
      "source": [
        "class EmbeddingLSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, output_dim, num_layers=2, dropout=0.5):\n",
        "        super(EmbeddingLSTM, self).__init__()\n",
        "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.tensor(embedding_matrix, dtype=torch.float).to(device),\n",
        "            freeze=False # í•™ìŠµ ì¤‘ ì„ë² ë”© ì—…ë°ì´íŠ¸ ê°€ëŠ¥\n",
        "            )\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
        "                            batch_first=True, dropout=dropout, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim) # ì–‘ë°©í–¥ LSTM + hidden_dim*2\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        output = self.fc(hidden[-1])\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3xQwbUruN73"
      },
      "outputs": [],
      "source": [
        "hidden_dim = 128\n",
        "output_dim = len(set(labels))# 20ê°œ ì¹´í…Œê³ ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnH7G4zh6aNI",
        "outputId": "7661fa6a-f1a0-4a82-f87a-bdc87076b38d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Epoch 1, Loss: 2.0690\n",
            "Epoch 2, Loss: 1.2437\n",
            "Epoch 3, Loss: 0.7738\n",
            "Epoch 4, Loss: 0.4933\n",
            "Epoch 5, Loss: 0.3334\n",
            "Epoch 6, Loss: 0.2397\n",
            "Epoch 7, Loss: 0.1821\n",
            "Epoch 8, Loss: 0.1709\n",
            "Epoch 9, Loss: 0.1524\n",
            "Epoch 10, Loss: 0.1287\n",
            "Test Accuracy (Word2Vec with LSTM): 0.6430\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model_word2vec = EmbeddingLSTM(word2vec_matrix, hidden_dim, output_dim).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_word2vec.parameters(), lr=0.005)\n",
        "\n",
        "for epoch in range(10):\n",
        "    loss = train(model_word2vec, train_loader_word2vec, loss_fn, optimizer)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "accuracy = evaluate(model_word2vec, test_loader_word2vec)\n",
        "print(f\"Test Accuracy (Word2Vec with LSTM): {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW9C6QEEgJ5l"
      },
      "source": [
        "# FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZuvNuKigLJb"
      },
      "outputs": [],
      "source": [
        "# FastText ëª¨ë¸ í•™ìŠµ\n",
        "fasttext_model = FastText(sentences=train_sentences,\n",
        "                          vector_size=128,\n",
        "                          window=5,\n",
        "                          min_count=1,\n",
        "                          sg=1)\n",
        "fasttext_matrix = np.zeros((len(fasttext_model.wv) + 1, 128))\n",
        "\n",
        "word2idx_fasttext = {word: idx + 1 for idx, word in enumerate(fasttext_model.wv.index_to_key)}\n",
        "\n",
        "for word, idx in word2idx_fasttext.items():\n",
        "    fasttext_matrix[idx] = fasttext_model.wv[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BJlpZiQ7JIt"
      },
      "outputs": [],
      "source": [
        "# FastText Dataset\n",
        "train_dataset_fasttext = TextEmbeddingDataset(train_inputs, train_targets, word2idx_fasttext, max_len)\n",
        "test_dataset_fasttext = TextEmbeddingDataset(test_inputs, test_targets, word2idx_fasttext, max_len)\n",
        "\n",
        "# FastText DataLoader\n",
        "train_loader_fasttext = DataLoader(train_dataset_fasttext, batch_size=64, shuffle=True)\n",
        "test_loader_fasttext = DataLoader(test_dataset_fasttext, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm0TYDYQ7JOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa53d47-a86d-4243-8de6-c72184abf09d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.9575\n",
            "Epoch 2, Loss: 1.1569\n",
            "Epoch 3, Loss: 0.7266\n",
            "Epoch 4, Loss: 0.4900\n",
            "Epoch 5, Loss: 0.3428\n",
            "Epoch 6, Loss: 0.2583\n",
            "Epoch 7, Loss: 0.2018\n",
            "Epoch 8, Loss: 0.1638\n",
            "Epoch 9, Loss: 0.1387\n",
            "Epoch 10, Loss: 0.1514\n",
            "Test Accuracy (FastText with LSTM): 0.6645\n"
          ]
        }
      ],
      "source": [
        "model_fasttext = EmbeddingLSTM(fasttext_matrix, hidden_dim, output_dim).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_fasttext.parameters(), lr=0.005)\n",
        "\n",
        "for epoch in range(10):\n",
        "    loss = train(model_fasttext, train_loader_fasttext, loss_fn, optimizer)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "accuracy = evaluate(model_fasttext, test_loader_fasttext)\n",
        "print(f\"Test Accuracy (FastText with LSTM): {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g10W_P6o7hzG"
      },
      "source": [
        "# GloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXM0jrLs-S29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11986d5b-cce7-4c3e-a2e9-502462a2a0f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading GloVe embeddings...\n",
            "GloVe embeddings downloaded and extracted.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# GloVe ë‹¤ìš´ë¡œë“œ ë° ì„ë² ë”© ë¡œë“œ\n",
        "GLOVE_URL = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "GLOVE_ZIP = \"glove.6B.zip\"\n",
        "GLOVE_FILE = \"glove.6B.200d.txt\"\n",
        "\n",
        "if not os.path.exists(GLOVE_FILE):\n",
        "    print(\"Downloading GloVe embeddings...\")\n",
        "    urllib.request.urlretrieve(GLOVE_URL, GLOVE_ZIP)\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(GLOVE_ZIP, 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "    print(\"GloVe embeddings downloaded and extracted.\")\n",
        "else:\n",
        "    print(\"GloVe embeddings already available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtvQiGDi-lae"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 200  # GloVe ì„ë² ë”© ì°¨ì› ì„¤ì •\n",
        "glove_embeddings = {}\n",
        "\n",
        "with open(GLOVE_FILE, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coeffs = np.asarray(values[1:], dtype='float32')\n",
        "        glove_embeddings[word] = coeffs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9XGV1S3-qAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3729886-448f-4cb7-ce40-a39be28f0a12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "len(glove_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10fL_A-G_HMR"
      },
      "outputs": [],
      "source": [
        "# GloVe word2idx ìƒì„±\n",
        "word2idx_glove = {word: idx + 1 for idx, word in enumerate(glove_embeddings.keys())}\n",
        "glove_matrix = np.zeros((len(word2idx_glove) + 1, embedding_dim))\n",
        "\n",
        "for word, idx in word2idx_glove.items():\n",
        "    embedding_vector = glove_embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        glove_matrix[idx] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja_J8Hhu-A02"
      },
      "outputs": [],
      "source": [
        "# GloVe Dataset\n",
        "train_dataset_glove = TextEmbeddingDataset(train_inputs, train_targets, word2idx_glove, max_len)\n",
        "test_dataset_glove = TextEmbeddingDataset(test_inputs, test_targets, word2idx_glove, max_len)\n",
        "\n",
        "# GloVe DataLoader\n",
        "train_loader_glove = DataLoader(train_dataset_glove, batch_size=32, shuffle=True)\n",
        "test_loader_glove = DataLoader(test_dataset_glove, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwlmMKks_Dc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d654c0f4-0fc6-4b77-ff20-08143d7603d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.7563\n",
            "Epoch 2, Loss: 1.0265\n",
            "Epoch 3, Loss: 0.6944\n",
            "Epoch 4, Loss: 0.4894\n",
            "Epoch 5, Loss: 0.3504\n",
            "Epoch 6, Loss: 0.2661\n",
            "Epoch 7, Loss: 0.2399\n",
            "Epoch 8, Loss: 0.2442\n",
            "Epoch 9, Loss: 0.2471\n",
            "Epoch 10, Loss: 0.2013\n",
            "Test Accuracy (GloVe with LSTM): 0.6618\n"
          ]
        }
      ],
      "source": [
        "model_glove = EmbeddingLSTM(glove_matrix, hidden_dim, output_dim).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_glove.parameters(), lr=0.005)\n",
        "\n",
        "for epoch in range(10):\n",
        "    loss = train(model_glove, train_loader_glove, loss_fn, optimizer)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "accuracy = evaluate(model_glove, test_loader_glove)\n",
        "print(f\"Test Accuracy (GloVe with LSTM): {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ğŸ“Š í•™ìŠµ ê³¡ì„  í•´ì„\n",
        "\n",
        "### 1. **Word2Vec**\n",
        "- **Loss ê°ì†Œ**: 2.06 â†’ 0.12ê¹Œì§€ ê¾¸ì¤€íˆ ê°ì†Œ, ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´\n",
        "- **Test Accuracy**: **0.6430**\n",
        "- â†’ í•™ìŠµì€ ì˜ ë˜ì—ˆì§€ë§Œ, ìµœì¢… ì •í™•ë„ëŠ” ì„¸ ëª¨ë¸ ì¤‘ ê°€ì¥ ë‚®ìŒ\n",
        "\n",
        "        Epoch 1, Loss: 2.0690\n",
        "        Epoch 2, Loss: 1.2437\n",
        "        Epoch 3, Loss: 0.7738\n",
        "        Epoch 4, Loss: 0.4933\n",
        "        Epoch 5, Loss: 0.3334\n",
        "        Epoch 6, Loss: 0.2397\n",
        "        Epoch 7, Loss: 0.1821\n",
        "        Epoch 8, Loss: 0.1709\n",
        "        Epoch 9, Loss: 0.1524\n",
        "        Epoch 10, Loss: 0.1287\n",
        "        Test Accuracy (Word2Vec with LSTM): 0.6430\n",
        "---\n",
        "\n",
        "### 2. **FastText**\n",
        "- **Loss ê°ì†Œ**: 1.95 â†’ 0.15ê¹Œì§€ ê°ì†Œ, Word2Vecê³¼ ë¹„ìŠ·í•˜ê²Œ ì•ˆì •ì \n",
        "- **Test Accuracy**: **0.6645**\n",
        "- â†’ ì„¸ ëª¨ë¸ ì¤‘ **ê°€ì¥ ë†’ì€ ì •í™•ë„**  \n",
        "- ì´ìœ : FastTextëŠ” subword(ë¶€ë¶„ ë‹¨ì–´) ì •ë³´ë¥¼ í™œìš©í•˜ê¸° ë•Œë¬¸ì— í¬ê·€ ë‹¨ì–´, ì˜¤íƒˆì, í˜•íƒœ ë³€í™”ì— ê°•í•¨ â†’ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¡°ê¸ˆ ë” ì¢‹ìŒ\n",
        "\n",
        "        Epoch 1, Loss: 1.9575\n",
        "        Epoch 2, Loss: 1.1569\n",
        "        Epoch 3, Loss: 0.7266\n",
        "        Epoch 4, Loss: 0.4900\n",
        "        Epoch 5, Loss: 0.3428\n",
        "        Epoch 6, Loss: 0.2583\n",
        "        Epoch 7, Loss: 0.2018\n",
        "        Epoch 8, Loss: 0.1638\n",
        "        Epoch 9, Loss: 0.1387\n",
        "        Epoch 10, Loss: 0.1514\n",
        "        Test Accuracy (FastText with LSTM): 0.6645\n",
        "---\n",
        "\n",
        "### 3. **GloVe**\n",
        "- **Loss ê°ì†Œ**: 1.75 â†’ 0.20ê¹Œì§€ ê°ì†Œ, ì¤‘ê°„ì— ì•½ê°„ í”ë“¤ë¦¼(8~9 epochì—ì„œ lossê°€ ë‹¤ì‹œ ì˜¬ë¼ê°)\n",
        "- **Test Accuracy**: **0.6618**\n",
        "- â†’ FastTextë³´ë‹¤ ì‚´ì§ ë‚®ì§€ë§Œ Word2Vecë³´ë‹¤ëŠ” ë†’ìŒ  \n",
        "- ì´ìœ : GloVeëŠ” ì „ì—­ ë™ì‹œì¶œí˜„ í†µê³„ë¥¼ ë°˜ì˜í•´ ì˜ë¯¸ì  ì¼ê´€ì„±ì´ ì¢‹ìŒ. ë‹¤ë§Œ í•™ìŠµ ë°ì´í„°ì™€ ë„ë©”ì¸ ì°¨ì´ê°€ ìˆìœ¼ë©´ ì„±ëŠ¥ì´ í”ë“¤ë¦´ ìˆ˜ ìˆìŒ\n",
        "\n",
        "        Epoch 1, Loss: 1.7563\n",
        "        Epoch 2, Loss: 1.0265\n",
        "        Epoch 3, Loss: 0.6944\n",
        "        Epoch 4, Loss: 0.4894\n",
        "        Epoch 5, Loss: 0.3504\n",
        "        Epoch 6, Loss: 0.2661\n",
        "        Epoch 7, Loss: 0.2399\n",
        "        Epoch 8, Loss: 0.2442\n",
        "        Epoch 9, Loss: 0.2471\n",
        "        Epoch 10, Loss: 0.2013\n",
        "        Test Accuracy (GloVe with LSTM): 0.6618\n",
        "---\n",
        "\n",
        "## âœ… ì¢…í•© í•´ì„\n",
        "\n",
        "| ì„ë² ë”© | ìµœì¢… ì •í™•ë„ | íŠ¹ì§• |\n",
        "|--------|-------------|------|\n",
        "| Word2Vec | 0.6430 | ë‹¨ì–´ ë‹¨ìœ„, ë‹¨ìˆœí•˜ê³  ë¹ ë¥´ì§€ë§Œ OOVì— ì·¨ì•½ |\n",
        "| FastText | **0.6645 (ìµœê³ )** | subword ê¸°ë°˜, í¬ê·€ ë‹¨ì–´/í˜•íƒœ ë³€í™”ì— ê°•í•¨ |\n",
        "| GloVe | 0.6618 | ì „ì—­ í†µê³„ ê¸°ë°˜, ì˜ë¯¸ì  ì¼ê´€ì„± ì¢‹ìŒ |\n",
        "\n",
        "ğŸ‘‰ **FastTextê°€ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥**ì„ ë³´ì˜€ê³ , GloVeê°€ ê·¸ ë’¤ë¥¼ ë”°ë¥´ë©°, Word2Vecì€ ê°€ì¥ ë‚®ì€ ì •í™•ë„ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.  \n",
        "ì¦‰, **ë°ì´í„°ì— í¬ê·€ ë‹¨ì–´ë‚˜ ë‹¤ì–‘í•œ í˜•íƒœ ë³€í™”ê°€ ë§ì„ìˆ˜ë¡ FastTextê°€ ìœ ë¦¬**í•˜ë‹¤ëŠ” ê±¸ ë³´ì—¬ì£¼ëŠ” ê²°ê³¼ì˜ˆìš”.\n"
      ],
      "metadata": {
        "id": "7UaE6WaECHc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ë°ì´í„° í™•ì¸"
      ],
      "metadata": {
        "id": "6ABdMh_odW88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "\n",
        "!apt-get update -qq\n",
        "!apt-get install fonts-nanum* -qq\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf' # ë‚˜ëˆ” ê³ ë”•\n",
        "font_name = fm.FontProperties(fname=path, size=10).get_name() # ê¸°ë³¸ í°íŠ¸ ì‚¬ì´ì¦ˆ : 10\n",
        "plt.rc('font', family=font_name)\n",
        "\n",
        "fm.fontManager.addfont(path)"
      ],
      "metadata": {
        "id": "SkkM_SDn2m4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a79e6c8-8f84-40e2-b48b-725c66df838f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 10.3 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
            "Fetched 10.3 MB in 2s (5,937 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package fonts-nanum-coding.\n",
            "(Reading database ... 121737 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum-coding_2.5-3_all.deb ...\n",
            "Unpacking fonts-nanum-coding (2.5-3) ...\n",
            "Selecting previously unselected package fonts-nanum-eco.\n",
            "Preparing to unpack .../fonts-nanum-eco_1.000-7_all.deb ...\n",
            "Unpacking fonts-nanum-eco (1.000-7) ...\n",
            "Selecting previously unselected package fonts-nanum-extra.\n",
            "Preparing to unpack .../fonts-nanum-extra_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum-extra (20200506-1) ...\n",
            "Setting up fonts-nanum-extra (20200506-1) ...\n",
            "Setting up fonts-nanum-coding (2.5-3) ...\n",
            "Setting up fonts-nanum-eco (1.000-7) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omKtptx9cTl1",
        "outputId": "95e6ca28-1490-402e-edb7-18edf5ba111c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample document: \n",
            "\n",
            "\n",
            "I am sure some bashers of Pens fans are pretty confused about the lack\n",
            "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
            "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
            "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
            "are killing those Devils worse than I thought. Jagr just showed you why\n",
            "he is much better than his regular season stats. He is also a lot\n",
            "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
            "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
            "regular season game.          PENS RULE!!!\n",
            "\n",
            "\n",
            "10 rec.sport.hockey\n"
          ]
        }
      ],
      "source": [
        "# ìƒ˜í”Œ ë¬¸ì„œ í™•ì¸\n",
        "print(f\"Sample document: \\n{texts[0]}\")\n",
        "# ì •ë‹µ ë ˆì´ë¸” ì¶œë ¥\n",
        "print(news_data.target[0],news_data.target_names[news_data.target[0]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì „ì²´ ë¬¸ì„œ ìˆ˜\n",
        "print(\"ì „ì²´ ë¬¸ì„œ ìˆ˜:\", len(texts))\n",
        "\n",
        "# ê³ ìœ  ë¬¸ì„œ ìˆ˜ (ì¤‘ë³µ ì œê±°)\n",
        "# unique_texts = set(texts)\n",
        "print(\"ê³ ìœ  ë¬¸ì„œ ìˆ˜:\", len(set(texts)))\n",
        "\n",
        "# ì¤‘ë³µ ë¬¸ì„œ ìˆ˜\n",
        "print(\"ì¤‘ë³µ ë¬¸ì„œ ìˆ˜:\", len(texts) - len(set(texts)))\n",
        "\n",
        "# ì˜ˆì‹œ: ì¤‘ë³µëœ ë¬¸ì„œ ëª‡ ê°œ ì¶œë ¥í•´ë³´ê¸°\n",
        "\n",
        "duplicates = [doc for doc, count in Counter(texts).items() if count > 1]\n",
        "print(\"ì¤‘ë³µëœ ë¬¸ì„œ ì˜ˆì‹œ:\", duplicates[:5])  # ì•ì—ì„œ 5ê°œë§Œ í™•ì¸\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9kQHnggXtOw",
        "outputId": "1e88a83c-9fbd-4d41-9f4b-a6a75af6472e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì „ì²´ ë¬¸ì„œ ìˆ˜: 18846\n",
            "ê³ ìœ  ë¬¸ì„œ ìˆ˜: 18287\n",
            "ì¤‘ë³µ ë¬¸ì„œ ìˆ˜: 559\n",
            "ì¤‘ë³µëœ ë¬¸ì„œ ì˜ˆì‹œ: ['', 'please subscrive me.', 'Inguiry by address:er1@eridan.chuvashia.su\\n', '\\n\\n\\n\\n\\n', '\\n\\n\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def check_duplicates(texts_, step_name):\n",
        "    total = len(texts_)\n",
        "    unique = len(set(texts_))\n",
        "    duplicates = total - unique\n",
        "    print(f\"[{step_name}] ì „ì²´ ë¬¸ì„œ ìˆ˜: {total}, ê³ ìœ  ë¬¸ì„œ ìˆ˜: {unique}, ì¤‘ë³µ ë¬¸ì„œ ìˆ˜: {duplicates}\")\n",
        "    return duplicates\n",
        "\n",
        "# 1. ì›ë³¸ ë°ì´í„°\n",
        "print(\"\\n--- ì „ì²˜ë¦¬ ë‹¨ê³„ë³„ ì¤‘ë³µ ë¬¸ì„œ ë³€í™” ---\")\n",
        "check_duplicates(texts, \"ì›ë³¸\")\n",
        "\n",
        "# 2. ì•ë’¤ ê³µë°± ì œê±°\n",
        "step1 = [t.strip() for t in texts]\n",
        "check_duplicates(step1, \"ì•ë’¤ ê³µë°± ì œê±°\")\n",
        "\n",
        "# 3. ì†Œë¬¸ì ë³€í™˜\n",
        "step2 = [t.lower() for t in step1]\n",
        "check_duplicates(step2, \"ì†Œë¬¸ì ë³€í™˜\")\n",
        "\n",
        "# 4. ì¤‘ê°„ ê³µë°± ì •ê·œí™”\n",
        "step3 = [re.sub(r'\\s+', ' ', t) for t in step2]\n",
        "check_duplicates(step3, \"ì¤‘ê°„ ê³µë°± ì •ê·œí™”\")\n",
        "\n",
        "# 5. íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì˜ˆì‹œ)\n",
        "step4 = [re.sub(r'[^a-zA-Z\\s]', '', t) for t in step3]\n",
        "check_duplicates(step4, \"íŠ¹ìˆ˜ë¬¸ì ì œê±°\")\n",
        "\n",
        "# 6. ë¶ˆìš©ì–´ ì œê±° (ê°„ë‹¨ ì˜ˆì‹œ)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "step5 = [\" \".join([w for w in t.split() if w not in stop_words]) for t in step4]\n",
        "check_duplicates(step5, \"ë¶ˆìš©ì–´ ì œê±°\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDpvhrtEZKn6",
        "outputId": "911e038a-72f8-4e90-a1ca-77b2781642c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ì „ì²˜ë¦¬ ë‹¨ê³„ë³„ ì¤‘ë³µ ë¬¸ì„œ ë³€í™” ---\n",
            "[ì›ë³¸] ì „ì²´ ë¬¸ì„œ ìˆ˜: 18846, ê³ ìœ  ë¬¸ì„œ ìˆ˜: 18287, ì¤‘ë³µ ë¬¸ì„œ ìˆ˜: 559\n",
            "[ì•ë’¤ ê³µë°± ì œê±°] ì „ì²´ ë¬¸ì„œ ìˆ˜: 18846, ê³ ìœ  ë¬¸ì„œ ìˆ˜: 18263, ì¤‘ë³µ ë¬¸ì„œ ìˆ˜: 583\n",
            "[ì†Œë¬¸ì ë³€í™˜] ì „ì²´ ë¬¸ì„œ ìˆ˜: 18846, ê³ ìœ  ë¬¸ì„œ ìˆ˜: 18260, ì¤‘ë³µ ë¬¸ì„œ ìˆ˜: 586\n",
            "[ì¤‘ê°„ ê³µë°± ì •ê·œí™”] ì „ì²´ ë¬¸ì„œ ìˆ˜: 18846, ê³ ìœ  ë¬¸ì„œ ìˆ˜: 18254, ì¤‘ë³µ ë¬¸ì„œ ìˆ˜: 592\n",
            "[íŠ¹ìˆ˜ë¬¸ì ì œê±°] ì „ì²´ ë¬¸ì„œ ìˆ˜: 18846, ê³ ìœ  ë¬¸ì„œ ìˆ˜: 18232, ì¤‘ë³µ ë¬¸ì„œ ìˆ˜: 614\n",
            "[ë¶ˆìš©ì–´ ì œê±°] ì „ì²´ ë¬¸ì„œ ìˆ˜: 18846, ê³ ìœ  ë¬¸ì„œ ìˆ˜: 18205, ì¤‘ë³µ ë¬¸ì„œ ìˆ˜: 641\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "641"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.ë°ì´í„° ì „ì²˜ë¦¬"
      ],
      "metadata": {
        "id": "PuADcFgv7q7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2-1. í† í°í™” (ì‹œê°„ì´ ì¢€ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\n",
        "def preprocess_text(text):\n",
        "    # 1. ì†Œë¬¸ì ë³€í™˜\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì•ŒíŒŒë²³ê³¼ ê³µë°±ë§Œ ë‚¨ê¸°ê¸°)\n",
        "    # [^a-zA-Z\\s]: ì•ŒíŒŒë²³ê³¼ ê³µë°±(ìŠ¤í˜ì´ìŠ¤, ì—”í„°)ì´ 'ì•„ë‹Œ' ê²ƒì€ ëª¨ë‘ ì œê±°\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # 3. í† í°í™”\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 4. ë¶ˆìš©ì–´ ì œê±° (ì„ íƒì‚¬í•­ì´ì§€ë§Œ ì¶”ì²œ)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # 5. ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œê±° (ì˜ˆ: 1ê¸€ì)\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "\n",
        "    return tokens\n",
        "print(\"í† í°í™” ì§„í–‰ ì¤‘...\")\n",
        "clean_texts = []\n",
        "clean_labels = []\n",
        "\n",
        "# zipì„ ì¨ì„œ í…ìŠ¤íŠ¸ì™€ ì •ë‹µ(label)ì„ ê°™ì´ ì²˜ë¦¬í•´ì•¼ ì§ì´ ë§ìŠµë‹ˆë‹¤.\n",
        "for text, label in zip(texts, labels):\n",
        "    tokens = preprocess_text(text)\n",
        "\n",
        "    # **ì¤‘ìš”**: ì „ì²˜ë¦¬ í–ˆë”ë‹ˆ ë‹¨ì–´ê°€ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš°(ë¹ˆ ë¦¬ìŠ¤íŠ¸)ëŠ” ë°ì´í„°ì—ì„œ ì œì™¸\n",
        "    # ì „ì²˜ë¦¬ ê²°ê³¼ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì„ì‹œ ì €ì¥\n",
        "    if len(tokens) > 0:\n",
        "        clean_texts.append(tokens)\n",
        "        clean_labels.append(label)\n",
        "\n",
        "print(f\"ì›ë˜ ë°ì´í„° ê°œìˆ˜: {len(texts)}\")\n",
        "print(f\"ì „ì²˜ë¦¬ í›„ ë°ì´í„° ê°œìˆ˜: {len(clean_texts)}\")\n",
        "# (ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ë¹ˆ í…ìŠ¤íŠ¸ê°€ ì œê±°ë˜ì–´ ê°œìˆ˜ê°€ ì¡°ê¸ˆ ì¤„ì–´ë“¤ ê²ƒì…ë‹ˆë‹¤)\n",
        "\n",
        "# 2. ì¤‘ë³µ ì œê±° (ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ì¶”ì²œ)\n",
        "# ë¦¬ìŠ¤íŠ¸ëŠ” ë°”ë¡œ setìœ¼ë¡œ ëª» ë°”ê¾¸ë¯€ë¡œ, íŠœí”Œë¡œ ë³€í™˜í•´ì„œ ì¤‘ë³µ ì²´í¬\n",
        "unique_data = {}\n",
        "for tokens, label in zip(clean_texts, clean_labels):\n",
        "    # í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ íŠœí”Œë¡œ ë³€í™˜ (Keyë¡œ ì“°ê¸° ìœ„í•´)\n",
        "    tokens_tuple = tuple(tokens)\n",
        "\n",
        "    # ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•˜ë©´ ìë™ìœ¼ë¡œ ì¤‘ë³µëœ Key(ë‚´ìš©)ëŠ” ë®ì–´ì”Œì›Œì§\n",
        "    # (ë‚´ìš©ì´ ê°™ìœ¼ë©´ ë¼ë²¨ë„ ê°™ë‹¤ê³  ê°€ì •)\n",
        "    unique_data[tokens_tuple] = label\n",
        "\n",
        "# 3. ìµœì¢… ë°ì´í„° ë³µì›\n",
        "final_texts = [list(tokens) for tokens in unique_data.keys()]\n",
        "final_labels = list(unique_data.values())\n",
        "\n",
        "print(f\"ìµœì¢… í•™ìŠµ ë°ì´í„° ìˆ˜(ì¤‘ë³µ ì œê±° í›„): {len(final_texts)}\")\n",
        "\n",
        "# ë³€ìˆ˜ëª… ì—…ë°ì´íŠ¸ (ì´í›„ ì½”ë“œì™€ ì—°ê²°)\n",
        "tokenized_texts = final_texts\n",
        "labels = final_labels\n",
        "\n",
        "# ë°ì´í„° í™•ì¸\n",
        "print(\"\\n--- ìµœì¢… ë°ì´í„° ì˜ˆì‹œ ---\")\n",
        "print(f\"Sample: {tokenized_texts[0]}\")\n",
        "print(f\"Label: {labels[0]}\")\n",
        "\n",
        "# 2-2. ë‹¨ì–´ì¥(Vocab) ë§Œë“¤ê¸°\n",
        "word_counts = Counter()\n",
        "for text in tokenized_texts:\n",
        "    word_counts.update(text)\n",
        "\n",
        "# ë¹ˆë„ìˆ˜ê°€ ë‚®ì€ ë‹¨ì–´ëŠ” ì œê±°í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì— ì¢‹ìŠµë‹ˆë‹¤ (ì˜ˆ: 5ë²ˆ ë¯¸ë§Œ ë“±ì¥ ë‹¨ì–´ ë¬´ì‹œ)\n",
        "vocab = {'<PAD>': 0, '<UNK>': 1} # PAD: íŒ¨ë”©ìš©, UNK: ëª¨ë¥´ëŠ” ë‹¨ì–´ìš©\n",
        "idx = 2\n",
        "for word, count in word_counts.items():\n",
        "    if count >= 5:\n",
        "        vocab[word] = idx\n",
        "        idx += 1\n",
        "\n",
        "print(f\"ë‹¨ì–´ì¥ í¬ê¸°: {len(vocab)}\")\n",
        "\n",
        "# 2-3. í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜ (Integer Encoding)\n",
        "encoded_texts = []\n",
        "for text in tokenized_texts:\n",
        "    encoded = [vocab.get(word, vocab['<UNK>']) for word in text]\n",
        "    encoded_texts.append(encoded)\n",
        "\n",
        "# 2-4. í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    encoded_texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whoWI9xI7t_F",
        "outputId": "0e87f8ea-4ac0-4ca5-8b62-cca464dcf5ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "í† í°í™” ì§„í–‰ ì¤‘...\n",
            "ì›ë˜ ë°ì´í„° ê°œìˆ˜: 18846\n",
            "ì „ì²˜ë¦¬ í›„ ë°ì´í„° ê°œìˆ˜: 18299\n",
            "ìµœì¢… í•™ìŠµ ë°ì´í„° ìˆ˜(ì¤‘ë³µ ì œê±° í›„): 18200\n",
            "\n",
            "--- ìµœì¢… ë°ì´í„° ì˜ˆì‹œ ---\n",
            "Sample: ['sure', 'bashers', 'pens', 'fans', 'pretty', 'confused', 'lack', 'kind', 'posts', 'recent', 'pens', 'massacre', 'devils', 'actually', 'bit', 'puzzled', 'bit', 'relieved', 'however', 'going', 'put', 'end', 'nonpittsburghers', 'relief', 'bit', 'praise', 'pens', 'man', 'killing', 'devils', 'worse', 'thought', 'jagr', 'showed', 'much', 'better', 'regular', 'season', 'stats', 'also', 'lot', 'fo', 'fun', 'watch', 'playoffs', 'bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'couple', 'games', 'since', 'pens', 'going', 'beat', 'pulp', 'jersey', 'anyway', 'disappointed', 'see', 'islanders', 'lose', 'final', 'regular', 'season', 'game', 'pens', 'rule']\n",
            "Label: 10\n",
            "ë‹¨ì–´ì¥ í¬ê¸°: 25072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# tokenized_textsëŠ” ì´ì „ ì…€ì—ì„œ ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n",
        "# vocab ìƒì„± ì‹œ ì‚¬ìš©í–ˆë˜ ìµœì†Œ ë¹ˆë„ìˆ˜ (min_count=5)ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤.\n",
        "\n",
        "# 1. word_counts ë‹¤ì‹œ ê³„ì‚° (vocab ìƒì„± ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ 5íšŒ ì´ìƒ ë“±ì¥ ë‹¨ì–´ë§Œ ê³ ë ¤)\n",
        "all_words = []\n",
        "for text_list in tokenized_texts:\n",
        "    all_words.extend(text_list)\n",
        "\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "# vocabì— ì‹¤ì œë¡œ í¬í•¨ëœ ë‹¨ì–´ë“¤ë§Œ í•„í„°ë§\n",
        "filtered_word_counts = {word: count for word, count in word_counts.items() if count >= 5}\n",
        "\n",
        "print(\"--- vocab ë‹¨ì–´ ë¹ˆë„ìˆ˜ (ë§ì€ ìˆœ) ---\")\n",
        "# ë¹ˆë„ìˆ˜ê°€ ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬\n",
        "sorted_by_most_frequent = sorted(filtered_word_counts.items(), key=lambda item: item[1], reverse=True)\n",
        "for word, count in sorted_by_most_frequent[:10]:\n",
        "    print(f\"ë‹¨ì–´: '{word}', ë¹ˆë„ìˆ˜: {count}\")\n",
        "\n",
        "print(\"\\n--- vocab ë‹¨ì–´ ë¹ˆë„ìˆ˜ (ì ì€ ìˆœ) ---\")\n",
        "# ë¹ˆë„ìˆ˜ê°€ ì ì€ ìˆœìœ¼ë¡œ ì •ë ¬\n",
        "sorted_by_least_frequent = sorted(filtered_word_counts.items(), key=lambda item: item[1])\n",
        "for word, count in sorted_by_least_frequent[:10]:\n",
        "    print(f\"ë‹¨ì–´: '{word}', ë¹ˆë„ìˆ˜: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRhImK32NLcG",
        "outputId": "6188e4ad-0c97-44eb-fee8-6dc8e4f1750e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- vocab ë‹¨ì–´ ë¹ˆë„ìˆ˜ (ë§ì€ ìˆœ) ---\n",
            "ë‹¨ì–´: 'would', ë¹ˆë„ìˆ˜: 10118\n",
            "ë‹¨ì–´: 'one', ë¹ˆë„ìˆ˜: 9933\n",
            "ë‹¨ì–´: 'dont', ë¹ˆë„ìˆ˜: 6413\n",
            "ë‹¨ì–´: 'like', ë¹ˆë„ìˆ˜: 6365\n",
            "ë‹¨ì–´: 'people', ë¹ˆë„ìˆ˜: 6276\n",
            "ë‹¨ì–´: 'get', ë¹ˆë„ìˆ˜: 5768\n",
            "ë‹¨ì–´: 'know', ë¹ˆë„ìˆ˜: 5726\n",
            "ë‹¨ì–´: 'also', ë¹ˆë„ìˆ˜: 5513\n",
            "ë‹¨ì–´: 'think', ë¹ˆë„ìˆ˜: 4975\n",
            "ë‹¨ì–´: 'use', ë¹ˆë„ìˆ˜: 4956\n",
            "\n",
            "--- vocab ë‹¨ì–´ ë¹ˆë„ìˆ˜ (ì ì€ ìˆœ) ---\n",
            "ë‹¨ì–´: 'bashers', ë¹ˆë„ìˆ˜: 5\n",
            "ë‹¨ì–´: 'transfered', ë¹ˆë„ìˆ˜: 5\n",
            "ë‹¨ì–´: 'fuhrs', ë¹ˆë„ìˆ˜: 5\n",
            "ë‹¨ì–´: 'untrustworthy', ë¹ˆë„ìˆ˜: 5\n",
            "ë‹¨ì–´: 'laundry', ë¹ˆë„ìˆ˜: 5\n",
            "ë‹¨ì–´: 'augsburg', ë¹ˆë„ìˆ˜: 5\n",
            "ë‹¨ì–´: 'magick', ë¹ˆë„ìˆ˜: 5\n",
            "ë‹¨ì–´: 'apprehend', ë¹ˆë„ìˆ˜: 5\n",
            "ë‹¨ì–´: 'boasts', ë¹ˆë„ìˆ˜: 5\n",
            "ë‹¨ì–´: 'gyroscopes', ë¹ˆë„ìˆ˜: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.ì„ë² ë”© ì ìš©"
      ],
      "metadata": {
        "id": "RfwEP2J29dfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from gensim.models import Word2Vec\n",
        "\n",
        "# ì„ë² ë”© ì°¨ì› ì„¤ì • (ë³´í†µ 100, 200, 300 ë“±ì„ ì‚¬ìš©)\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "# 1. Word2Vec ëª¨ë¸ í•™ìŠµ (í˜„ì¬ ë°ì´í„°ì…‹ìœ¼ë¡œ)\n",
        "w2v_model = Word2Vec(sentences=tokenized_texts,\n",
        "                     vector_size=EMBEDDING_DIM,\n",
        "                     window=5,\n",
        "                     min_count=5,\n",
        "                     workers=4\n",
        "                     )\n",
        "# 2. ì„ë² ë”© í–‰ë ¬(Weight Matrix) ìƒì„±\n",
        "# ëª¨ë¸ì˜ ì„ë² ë”© ë ˆì´ì–´ì— ë„£ì–´ì¤„ ì´ˆê¸°ê°’ì…ë‹ˆë‹¤.\n",
        "embedding_matrix = np.zeros((len(vocab), EMBEDDING_DIM))\n",
        "\n",
        "for word, idx in vocab.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[idx] = w2v_model.wv[word]\n",
        "    else:\n",
        "        # Word2Vecì— ì—†ëŠ” ë‹¨ì–´(UNK ë“±)ëŠ” ëœë¤ ì´ˆê¸°í™”\n",
        "        embedding_matrix[idx] = np.random.normal(size=(EMBEDDING_DIM,))\n",
        "\n",
        "# PyTorch Tensorë¡œ ë³€í™˜\n",
        "embedding_weights = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "print(\"ì„ë² ë”© í–‰ë ¬ ìƒì„± ì™„ë£Œ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_pJmgZe9clE",
        "outputId": "cfad8956-887d-4dad-af73-73f16fea2dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì„ë² ë”© í–‰ë ¬ ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.PyTorch Dataset & DataLoader êµ¬í˜„"
      ],
      "metadata": {
        "id": "frC4TQND9iNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 200  # ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ ì„¤ì • (ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê³ , ì§§ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€)\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # íŒ¨ë”© ë° ìë¥´ê¸° (Padding & Truncating)\n",
        "        if len(text) > self.max_len:\n",
        "            text = text[:self.max_len]\n",
        "        else:\n",
        "            text = text + [0] * (self.max_len - len(text)) # 0ì€ <PAD>ì˜ ì¸ë±ìŠ¤\n",
        "\n",
        "        return torch.tensor(text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# ë°ì´í„° ë¡œë” ìƒì„±\n",
        "train_dataset = NewsDataset(X_train, y_train, MAX_LEN)\n",
        "test_dataset = NewsDataset(X_test, y_test, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "LZXb6EUj9lht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.LSTM ëª¨ë¸"
      ],
      "metadata": {
        "id": "Q9SNr4f79q-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pretrained_weights):\n",
        "        super(TextClassifier, self).__init__()\n",
        "\n",
        "        # 1. ì„ë² ë”© ë ˆì´ì–´ (Pre-trained weights ë¡œë“œ)\n",
        "        self.embedding = nn.Embedding.from_pretrained(pretrained_weights, freeze=False)\n",
        "        # freeze=Trueë©´ í•™ìŠµ ì¤‘ì— ì„ë² ë”©ì´ ë³€í•˜ì§€ ì•ŠìŒ, Falseë©´ ë¯¸ì„¸ì¡°ì •ë¨\n",
        "\n",
        "        # 2. LSTM ë ˆì´ì–´\n",
        "        # batch_first=True: ì…ë ¥ ë°ì´í„°ê°€ (Batch, Seq, Feature) ìˆœì„œì„\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        # 3. ë¶„ë¥˜ ë ˆì´ì–´ (20ê°œ ì¹´í…Œê³ ë¦¬)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, max_len]\n",
        "        embedded = self.embedding(x)  # -> [batch_size, max_len, embedding_dim]\n",
        "\n",
        "        # LSTM í†µê³¼\n",
        "        # output: ëª¨ë“  ì‹œì ì˜ ì€ë‹‰ ìƒíƒœ\n",
        "        # (hidden, cell): ë§ˆì§€ë§‰ ì‹œì ì˜ ì€ë‹‰ ìƒíƒœ\n",
        "        _, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        # ë§ˆì§€ë§‰ ì‹œì ì˜ hidden stateë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜\n",
        "        # hidden[-1] : [batch_size, hidden_dim]\n",
        "        out = self.fc(hidden[-1])\n",
        "        return out\n",
        "\n",
        "# ëª¨ë¸ ì´ˆê¸°í™”\n",
        "model = TextClassifier(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=128,\n",
        "    output_dim=20, # ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ìˆ˜\n",
        "    pretrained_weights=embedding_weights\n",
        ")\n",
        "\n",
        "# GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ ì„¤ì •\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "ZiU6hJ049vNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.í•™ìŠµ ë° í‰ê°€ (Loop)"
      ],
      "metadata": {
        "id": "G_MCXnAT912O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in iterator:\n",
        "        text, label = batch\n",
        "        text, label = text.to(device), label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, label = batch\n",
        "            text, label = text.to(device), label.to(device)\n",
        "\n",
        "            predictions = model(text)\n",
        "            loss = criterion(predictions, label)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # ì •í™•ë„ ê³„ì‚°\n",
        "            predicted_class = predictions.argmax(dim=1)\n",
        "            correct += (predicted_class == label).sum().item()\n",
        "            total += label.size(0)\n",
        "\n",
        "    return epoch_loss / len(iterator), correct / total\n",
        "\n",
        "# í•™ìŠµ ì‹¤í–‰\n",
        "N_EPOCHS = 5\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxbi82dk9-qT",
        "outputId": "daaf4ecd-d913-4e76-8878-8ed6e0e920db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Train Loss: 2.960 | Test Loss: 2.935 | Test Acc: 7.53%\n",
            "Epoch: 02 | Train Loss: 2.693 | Test Loss: 2.416 | Test Acc: 21.51%\n",
            "Epoch: 03 | Train Loss: 2.181 | Test Loss: 2.051 | Test Acc: 29.51%\n",
            "Epoch: 04 | Train Loss: 1.826 | Test Loss: 1.781 | Test Acc: 39.20%\n",
            "Epoch: 05 | Train Loss: 1.537 | Test Loss: 1.604 | Test Acc: 45.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(20):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh2bVq09QLKO",
        "outputId": "293f2427-07d9-4c18-c70f-5085329f5ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Train Loss: 1.261 | Test Loss: 1.380 | Test Acc: 53.43%\n",
            "Epoch: 02 | Train Loss: 0.989 | Test Loss: 1.300 | Test Acc: 58.54%\n",
            "Epoch: 03 | Train Loss: 0.747 | Test Loss: 1.273 | Test Acc: 61.81%\n",
            "Epoch: 04 | Train Loss: 0.579 | Test Loss: 1.266 | Test Acc: 63.52%\n",
            "Epoch: 05 | Train Loss: 0.454 | Test Loss: 1.289 | Test Acc: 64.51%\n",
            "Epoch: 06 | Train Loss: 0.351 | Test Loss: 1.318 | Test Acc: 65.82%\n",
            "Epoch: 07 | Train Loss: 0.278 | Test Loss: 1.390 | Test Acc: 65.44%\n",
            "Epoch: 08 | Train Loss: 0.224 | Test Loss: 1.467 | Test Acc: 65.25%\n",
            "Epoch: 09 | Train Loss: 0.170 | Test Loss: 1.535 | Test Acc: 65.30%\n",
            "Epoch: 10 | Train Loss: 0.146 | Test Loss: 1.542 | Test Acc: 66.81%\n",
            "Epoch: 11 | Train Loss: 0.111 | Test Loss: 1.682 | Test Acc: 65.27%\n",
            "Epoch: 12 | Train Loss: 0.088 | Test Loss: 1.706 | Test Acc: 65.22%\n",
            "Epoch: 13 | Train Loss: 0.072 | Test Loss: 1.849 | Test Acc: 64.59%\n",
            "Epoch: 14 | Train Loss: 0.075 | Test Loss: 1.807 | Test Acc: 65.66%\n",
            "Epoch: 15 | Train Loss: 0.064 | Test Loss: 1.878 | Test Acc: 65.60%\n",
            "Epoch: 16 | Train Loss: 0.058 | Test Loss: 1.833 | Test Acc: 65.27%\n",
            "Epoch: 17 | Train Loss: 0.049 | Test Loss: 1.933 | Test Acc: 65.88%\n",
            "Epoch: 18 | Train Loss: 0.036 | Test Loss: 2.005 | Test Acc: 65.19%\n",
            "Epoch: 19 | Train Loss: 0.054 | Test Loss: 2.011 | Test Acc: 60.41%\n",
            "Epoch: 20 | Train Loss: 0.056 | Test Loss: 2.006 | Test Acc: 65.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê³¼ì í•©-\n",
        "65í¼ì„¼íŠ¸ ìœ„ë¡œ ëª»ê°\n"
      ],
      "metadata": {
        "id": "CX-oeCsYRX0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ìˆ˜ì •1"
      ],
      "metadata": {
        "id": "89uhm18JUqAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassifier1(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pretrained_weights):\n",
        "        super(TextClassifier1, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(pretrained_weights, freeze=False)\n",
        "\n",
        "        # [ìˆ˜ì • 1] bidirectional=True ì¶”ê°€ (ì–‘ë°©í–¥ ì„¤ì •)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # [ìˆ˜ì • 2] ì…ë ¥ ì°¨ì›ì„ 2ë°°ë¡œ ëŠ˜ë¦¼ (hidden_dim * 2)\n",
        "        # ì™œëƒí•˜ë©´ ì™¼ìª½ì—ì„œ ì˜¨ ì •ë³´ + ì˜¤ë¥¸ìª½ì—ì„œ ì˜¨ ì •ë³´ê°€ í•©ì³ì§€ê¸° ë•Œë¬¸\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # ì¶œë ¥ê°’ ë°›ëŠ” ë¶€ë¶„ë„ ì‚´ì§ ìˆ˜ì •\n",
        "        # hidden state êµ¬ì¡°ê°€ ë°”ë€Œì§€ë§Œ, ë³´í†µ outputì˜ ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì„ ì“°ëŠ” ê²Œ í¸í•¨\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        # ì–‘ë°©í–¥ì¼ ë•ŒëŠ” outputì˜ ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì„ ê°€ì ¸ì˜¤ëŠ” ë°©ì‹ì´ ì¡°ê¸ˆ ë‹¤ë¦…ë‹ˆë‹¤.\n",
        "        # ê°€ì¥ ì‰¬ìš´ ë°©ë²•: ë§ˆì§€ë§‰ hidden state 2ê°œ(ì •ë°©í–¥, ì—­ë°©í–¥)ë¥¼ í•©ì³ì„œ ë„£ê¸°\n",
        "        hidden_cat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "\n",
        "        out = self.fc(hidden_cat)\n",
        "        return out\n",
        "# ëª¨ë¸ ì´ˆê¸°í™”\n",
        "model1 = TextClassifier1(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=128,\n",
        "    output_dim=20, # ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ìˆ˜\n",
        "    pretrained_weights=embedding_weights\n",
        ")\n",
        "\n",
        "# GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ ì„¤ì •\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model1 = model1.to(device)\n",
        "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
        "criterion1 = nn.CrossEntropyLoss()\n",
        "# í•™ìŠµ ì‹¤í–‰\n",
        "for epoch in range(20):\n",
        "    train_loss = train(model1, train_loader, optimizer1, criterion1)\n",
        "    test_loss, test_acc = evaluate(model1, test_loader, criterion1)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NVTMdFMRb3D",
        "outputId": "dc01e6d4-f1b0-4be0-96d8-625d37cab51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Train Loss: 2.202 | Test Loss: 1.944 | Test Acc: 35.08%\n",
            "Epoch: 02 | Train Loss: 1.663 | Test Loss: 1.733 | Test Acc: 41.70%\n",
            "Epoch: 03 | Train Loss: 1.396 | Test Loss: 1.380 | Test Acc: 55.22%\n",
            "Epoch: 04 | Train Loss: 1.152 | Test Loss: 1.267 | Test Acc: 58.32%\n",
            "Epoch: 05 | Train Loss: 0.916 | Test Loss: 1.223 | Test Acc: 60.49%\n",
            "Epoch: 06 | Train Loss: 0.715 | Test Loss: 1.120 | Test Acc: 64.62%\n",
            "Epoch: 07 | Train Loss: 0.530 | Test Loss: 1.107 | Test Acc: 66.40%\n",
            "Epoch: 08 | Train Loss: 0.380 | Test Loss: 1.159 | Test Acc: 65.91%\n",
            "Epoch: 09 | Train Loss: 0.272 | Test Loss: 1.176 | Test Acc: 68.82%\n",
            "Epoch: 10 | Train Loss: 0.199 | Test Loss: 1.243 | Test Acc: 68.54%\n",
            "Epoch: 11 | Train Loss: 0.135 | Test Loss: 1.336 | Test Acc: 67.80%\n",
            "Epoch: 12 | Train Loss: 0.105 | Test Loss: 1.360 | Test Acc: 68.68%\n",
            "Epoch: 13 | Train Loss: 0.067 | Test Loss: 1.489 | Test Acc: 66.92%\n",
            "Epoch: 14 | Train Loss: 0.059 | Test Loss: 1.546 | Test Acc: 68.08%\n",
            "Epoch: 15 | Train Loss: 0.043 | Test Loss: 1.586 | Test Acc: 67.09%\n",
            "Epoch: 16 | Train Loss: 0.035 | Test Loss: 1.686 | Test Acc: 67.53%\n",
            "Epoch: 17 | Train Loss: 0.047 | Test Loss: 1.584 | Test Acc: 67.94%\n",
            "Epoch: 18 | Train Loss: 0.036 | Test Loss: 1.596 | Test Acc: 67.97%\n",
            "Epoch: 19 | Train Loss: 0.029 | Test Loss: 1.616 | Test Acc: 67.86%\n",
            "Epoch: 20 | Train Loss: 0.017 | Test Loss: 1.743 | Test Acc: 68.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ìˆ˜ì •2"
      ],
      "metadata": {
        "id": "e3pzV05sUv9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, verbose=False, path='checkpoint.pt'):\n",
        "        \"\"\"\n",
        "        patience: ì„±ëŠ¥ì´ ì•ˆ ì¢‹ì•„ì ¸ë„ ì°¸ì•„ì£¼ëŠ” íšŸìˆ˜ (ë³´í†µ 3~5)\n",
        "        path: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        # [ìˆ˜ì •] np.Inf -> np.inf (ì†Œë¬¸ìë¡œ ë³€ê²½)\n",
        "        self.val_loss_min = np.inf\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        # ì²« ì‹¤í–‰ì´ê±°ë‚˜ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì¡Œì„ ë•Œ\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif val_loss > self.best_loss:\n",
        "            # ì„±ëŠ¥ì´ ì•ˆ ì¢‹ì•„ì§ (Lossê°€ ì»¤ì§) -> ì°¸ì„ì„±(Counter) ê°ì†Œ\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            # ì„±ëŠ¥ì´ ë‹¤ì‹œ ì¢‹ì•„ì§ -> ì¹´ìš´í„° ì´ˆê¸°í™” ë° ì €ì¥\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''ì„±ëŠ¥ì´ ì¢‹ì•„ì¡Œì„ ë•Œ ëª¨ë¸ ì €ì¥'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "5J0w0gLdTzLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ëª¨ë¸ ì •ì˜ (ì˜¤íƒ€ ìˆ˜ì •ë¨)\n",
        "class TextClassifier2(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pretrained_weights):\n",
        "        # [ìˆ˜ì •] í´ë˜ìŠ¤ ì´ë¦„ì— ë§ì¶°ì„œ super() í˜¸ì¶œ (í˜¹ì€ super().__init__() ì‚¬ìš©)\n",
        "        super(TextClassifier2, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(pretrained_weights, freeze=False)\n",
        "\n",
        "        # Dropout, ì–‘ë°©í–¥, 2ì¸µ ìŒ“ê¸° ì ìš©ë¨\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2,\n",
        "                            batch_first=True, bidirectional=True, dropout=0.5)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # ì–‘ë°©í–¥ì´ë¯€ë¡œ hidden_dim * 2\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        # ì–‘ë°©í–¥ì˜ ë§ˆì§€ë§‰ hidden state ê²°í•©\n",
        "        hidden_cat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "\n",
        "        hidden_cat = self.dropout(hidden_cat)\n",
        "        out = self.fc(hidden_cat)\n",
        "        return out\n",
        "\n",
        "# 2. ëª¨ë¸ ë° í•™ìŠµ ì„¤ì •\n",
        "model2 = TextClassifier2(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=128,    # í•„ìš”í•˜ë©´ 256ìœ¼ë¡œ ëŠ˜ë ¤ë„ ë¨\n",
        "    output_dim=20,\n",
        "    pretrained_weights=embedding_weights\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model2 = model2.to(device)\n",
        "\n",
        "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "# [ì¶”ê°€] ì¡°ê¸° ì¢…ë£Œ ê°ì²´ ìƒì„± (patience=3: ì„±ì ì´ 3ë²ˆ ì—°ì† ì•ˆ ì˜¤ë¥´ë©´ ë©ˆì¶¤)\n",
        "# ìœ„ì—ì„œ ë§Œë“  EarlyStopping í´ë˜ìŠ¤ê°€ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
        "early_stopping = EarlyStopping(patience=3, verbose=True, path='best_model.pt')\n",
        "\n",
        "# 3. í•™ìŠµ ì‹¤í–‰\n",
        "print(\"í•™ìŠµ ì‹œì‘! (ì„±ëŠ¥ í–¥ìƒ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œë©ë‹ˆë‹¤)\")\n",
        "N_EPOCHS = 50  # ë„‰ë„‰í•˜ê²Œ ì¡ì•„ë„ ì•Œì•„ì„œ ë©ˆì¶¤\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = train(model2, train_loader, optimizer2, criterion2)\n",
        "    test_loss, test_acc = evaluate(model2, test_loader, criterion2)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
        "\n",
        "    # ì¡°ê¸° ì¢…ë£Œ ì²´í¬\n",
        "    early_stopping(test_loss, model2)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"ì¡°ê¸° ì¢…ë£Œ ë°œë™! ê³¼ì í•© ì§ì „ì— ë©ˆì·„ìŠµë‹ˆë‹¤.\")\n",
        "        break\n",
        "\n",
        "# í•™ìŠµ ì¢…ë£Œ í›„, ê°€ì¥ ì¢‹ì•˜ë˜ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "model2.load_state_dict(torch.load('best_model.pt'))\n",
        "print(\"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2txOfLLgSx02",
        "outputId": "9316bb3e-b7f2-478f-b3ca-1d36666e5444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "í•™ìŠµ ì‹œì‘! (ì„±ëŠ¥ í–¥ìƒ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œë©ë‹ˆë‹¤)\n",
            "Epoch: 01 | Train Loss: 2.268 | Test Loss: 1.887 | Test Acc: 34.56%\n",
            "Validation loss decreased (inf --> 1.886813).  Saving model ...\n",
            "Epoch: 02 | Train Loss: 1.784 | Test Loss: 1.655 | Test Acc: 43.71%\n",
            "Validation loss decreased (1.886813 --> 1.654714).  Saving model ...\n",
            "Epoch: 03 | Train Loss: 1.548 | Test Loss: 1.479 | Test Acc: 49.34%\n",
            "Validation loss decreased (1.654714 --> 1.479002).  Saving model ...\n",
            "Epoch: 04 | Train Loss: 1.343 | Test Loss: 1.410 | Test Acc: 53.49%\n",
            "Validation loss decreased (1.479002 --> 1.410051).  Saving model ...\n",
            "Epoch: 05 | Train Loss: 1.124 | Test Loss: 1.280 | Test Acc: 57.45%\n",
            "Validation loss decreased (1.410051 --> 1.279745).  Saving model ...\n",
            "Epoch: 06 | Train Loss: 0.930 | Test Loss: 1.234 | Test Acc: 59.51%\n",
            "Validation loss decreased (1.279745 --> 1.234161).  Saving model ...\n",
            "Epoch: 07 | Train Loss: 0.751 | Test Loss: 1.188 | Test Acc: 62.66%\n",
            "Validation loss decreased (1.234161 --> 1.187638).  Saving model ...\n",
            "Epoch: 08 | Train Loss: 0.607 | Test Loss: 1.239 | Test Acc: 63.98%\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Epoch: 09 | Train Loss: 0.468 | Test Loss: 1.248 | Test Acc: 66.21%\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Epoch: 10 | Train Loss: 0.371 | Test Loss: 1.314 | Test Acc: 65.16%\n",
            "EarlyStopping counter: 3 out of 3\n",
            "ì¡°ê¸° ì¢…ë£Œ ë°œë™! ê³¼ì í•© ì§ì „ì— ë©ˆì·„ìŠµë‹ˆë‹¤.\n",
            "ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}