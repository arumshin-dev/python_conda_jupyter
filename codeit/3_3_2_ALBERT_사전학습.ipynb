{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumshin-dev/python_conda_jupyter/blob/main/codeit/3_3_2_ALBERT_%E1%84%89%E1%85%A1%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ALBERT ëª¨ë¸\n",
        "\n",
        "ê¸°ë³¸ ì ìœ¼ë¡œ BERT ëª¨ë¸ì€ ì¸ì½”ë”© ë¸”ë¡ì´ ê¹Šì–´ì§ˆ ìˆ˜ ë¡ íŒŒë¼ë¯¸í„°ì˜ ê·œëª¨ê°€ ë§¤ìš° ì»¤ì§ˆ ë¿ë§Œ ì•„ë‹ˆë¼, ë§¤ìš° í° ì–´íœ˜ ì‚¬ì „(vocab)ê³¼ ì„ë² ë”© ê¸¸ì´ë¥¼ ì‚¬ìš©í•  ë•Œ, ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ê°€ ì°¨ì§€í•˜ëŠ” íŒŒë¼ë¯¸í„°ê°€ ë„ˆë¬´ ì»¤ì ¸ ë²„ë¦¬ëŠ” ë¬¸ì œê°€ ìƒê¹ë‹ˆë‹¤.\n",
        "\n",
        "ALBERTëŠ” í¬ê²Œ 2ê°€ì§€ ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ì´ë¥¼ í¬ê²Œ ë‚®ì¶”ê³  ëª¨ë¸ íŒŒë¼ë¯¸í„° ê·œëª¨ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "- **ì„ë² ë”© íŒŒë¼ë¯¸í„°ì˜ ë¶„ë¦¬(Factorized Embedding Parameterization)**\n",
        "\n",
        "    * (ë‹¨ì–´ ì„ë² ë”© ì°¨ì›) â†’ (í”„ë¡œì ì…˜ ë ˆì´ì–´) â†’ (Transformer hidden ì°¨ì›)ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì„ë² ë”© í–‰ë ¬ì˜ íŒŒë¼ë¯¸í„°ë¥¼ í¬ê²Œ ê°ì†Œ\n",
        "\n",
        "    * ë‹¨ì–´ ì„ë² ë”©ì€ ì‘ì€ ì°¨ì›(ì˜ˆ: 128ì°¨ì›)ìœ¼ë¡œ ë‘ê³ , ì´í›„ ì„ í˜• ë ˆì´ì–´ë¥¼ í†µí•´ Transformer ë¸”ë¡ì—ì„œ ì‚¬ìš©í•˜ëŠ” ìˆ¨ê¹€(hidden) ì°¨ì›(ì˜ˆ: 256~768ì°¨ì› ë“±)ìœ¼ë¡œ ë§¤í•‘\n",
        "\n",
        "- **í¬ë¡œìŠ¤-ë ˆì´ì–´ íŒŒë¼ë¯¸í„° ê³µìœ (Cross-Layer Parameter Sharing)**\n",
        "\n",
        "    * ì—¬ëŸ¬ ê°œì˜ Transformer ë ˆì´ì–´ê°€ ëª¨ë‘ ê°™ì€ ê°€ì¤‘ì¹˜ë¥¼ ê³µìœ í•˜ì—¬, í•œ ê°œì˜ Transformer ë¸”ë¡ì„ Në²ˆ ë°˜ë³µí•´ì„œ í˜¸ì¶œ(ë ˆì´ì–´ë¥¼ ë³µì‚¬í•´ì„œ ì“°ëŠ” ê²ƒì´ ì•„ë‹Œ, â€˜ê°™ì€â€™ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ëŠ” êµ¬ì¡°)\n",
        "\n",
        "ë˜í•œ, ALBERTëŠ” BERTì˜ NSP ì‚¬ì „í•™ìŠµ ëŒ€ì‹  ë‘ ë¬¸ì¥ì˜ ìˆœì„œê°€ ì˜¬ë°”ë¥¸ì§€ë¥¼ ë§íˆëŠ” SOP ì‚¬ì „í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. SOPëŠ” NSPì˜ ëœë¤ ë°©ì‹ì— ë¹„í•´ ì–¸ì–´ì  ì¼ê´€ì„±ê³¼ ë¬¸ë§¥ ì—°ê²°ì„±ì„ ë” íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n"
      ],
      "metadata": {
        "id": "_LpTpKV_dz9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "A2lyv3KLd8mv",
        "outputId": "79b29369-33f0-4b47-c00f-b833bdb929a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6ce40e7c-bc1f-4716-a4ab-57799985e989\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6ce40e7c-bc1f-4716-a4ab-57799985e989\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.csv to train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import copy\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "OU5gYbQTeBdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(f'./train.csv')\n",
        "df[['HS01','SS01']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "rnZdXMc8eCB5",
        "outputId": "f7a79f28-95b1-4c2f-9087-2d96290e2c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    HS01  \\\n",
              "0                              ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.   \n",
              "1         ì´ë²ˆ ë‹¬ì— ë˜ ê¸‰ì—¬ê°€ ê¹ì˜€ì–´! ë¬¼ê°€ëŠ” ì˜¤ë¥´ëŠ”ë° ì›”ê¸‰ë§Œ ìê¾¸ ê¹ì´ë‹ˆê¹Œ ë„ˆë¬´ í™”ê°€ ë‚˜.   \n",
              "2      íšŒì‚¬ì— ì‹ ì…ì´ ë“¤ì–´ì™”ëŠ”ë° ë§íˆ¬ê°€ ê±°ìŠ¬ë ¤. ê·¸ëŸ° ì• ë¥¼ ë§¤ì¼ ë´ì•¼ í•œë‹¤ê³  ìƒê°í•˜ë‹ˆê¹Œ ìŠ¤...   \n",
              "3      ì§ì¥ì—ì„œ ë§‰ë‚´ë¼ëŠ” ì´ìœ ë¡œ ë‚˜ì—ê²Œë§Œ ì˜¨ê°– ì‹¬ë¶€ë¦„ì„ ì‹œì¼œ. ì¼ë„ ë§ì€ ë° ì •ë§ ë¶„í•˜ê³  ...   \n",
              "4                  ì–¼ë§ˆ ì „ ì…ì‚¬í•œ ì‹ ì…ì‚¬ì›ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ë„ˆë¬´ í™”ê°€ ë‚˜.   \n",
              "...                                                  ...   \n",
              "51623     ë‚˜ì´ê°€ ë¨¹ê³  ì´ì œ ëˆë„ ëª» ë²Œì–´ ì˜¤ë‹ˆê¹Œ ì–´ë–»ê²Œ ì‚´ì•„ê°€ì•¼ í• ì§€ ë§‰ë§‰í•´. ëŠ¥ë ¥ë„ ì—†ê³ .   \n",
              "51624        ëª¸ì´ ë§ì´ ì•½í•´ì¡Œë‚˜ ë´. ì´ì œ ì „ê³¼ ê°™ì´ ì¼í•˜ì§€ ëª»í•  ê²ƒ ê°™ì•„ ë„ˆë¬´ ì§œì¦ ë‚˜.   \n",
              "51625   ì´ì œ ì–´ë–»ê²Œ í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´. ë‚¨í¸ë„ ê·¸ë ‡ê³  ë…¸í›„ ì¤€ë¹„ë„ ì•ˆ ë˜ì–´ì„œ ë¯¸ë˜ê°€ ê±±ì •ë¼.   \n",
              "51626  ëª‡ì‹­ ë…„ì„ í•¨ê»˜ ì‚´ì•˜ë˜ ë‚¨í¸ê³¼ ì´í˜¼í–ˆì–´. ê·¸ë™ì•ˆì˜ ì„¸ì›”ì— ë°°ì‹ ê°ì„ ëŠë¼ê³  ë„ˆë¬´ í™”ê°€ ë‚˜.   \n",
              "51627  ë‚¨í¸ê³¼ ê²°í˜¼í•œ ì§€ ì‚¬ì‹­ ë…„ì´ì•¼. ì´ì œ ì‚¬ëŒ ë§Œë‚˜ëŠ” ê²ƒë„ ë²„ê²ê³  ì•Œë˜ ì‚¬ëŒë„ ì ì  ì‚¬ë¼ì ¸.   \n",
              "\n",
              "                                                    SS01  \n",
              "0                            ë§ì´ í˜ë“œì‹œê² ì–´ìš”. ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìˆë‚˜ìš”?  \n",
              "1               ê¸‰ì—¬ê°€ ì¤„ì–´ ì†ìƒí•˜ì‹œê² ì–´ìš”. ì›”ê¸‰ì´ ì¤„ì–´ë“  ê²ƒì„ ì–´ë–»ê²Œ ë³´ì™„í•˜ì‹¤ ê±´ê°€ìš”?  \n",
              "2      íšŒì‚¬ ë™ë£Œ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ëŠ” ê²ƒ ê°™ì•„ìš”. ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì–´ë–¤ ë…¸ë ¥ì„ ...  \n",
              "3      ê´€ë ¨ ì—†ëŠ” ì‹¬ë¶€ë¦„ì„ ëª¨ë‘ í•˜ê²Œ ë˜ì–´ì„œ ë…¸ì—¬ìš°ì‹œêµ°ìš”. ì–´ë–¤ ê²ƒì´ ìƒí™©ì„ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆ...  \n",
              "4      ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì€ íƒœë„ì— í™”ê°€ ë‚˜ì…¨êµ°ìš”. ìƒëŒ€ë°©ì˜ ì–´ë–¤ í–‰ë™ì´ ê·¸ëŸ° ê°ì •ì„ ìœ ë°œí•˜ëŠ”...  \n",
              "...                                                  ...  \n",
              "51623                 ê²½ì œì ì¸ ë¬¸ì œ ë•Œë¬¸ì— ë§‰ë§‰í•˜ì‹œêµ°ìš”. ë§ˆìŒì´ í¸ì¹˜ ì•Šìœ¼ì‹œê² ì–´ìš”.  \n",
              "51624              ê±´ê°•ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê¸°ë¶„ì´ ì¢‹ì§€ ì•Šìœ¼ì‹œêµ°ìš”. ì†ìƒí•˜ì‹œê² ì–´ìš”.  \n",
              "51625                      ë…¸í›„ ì¤€ë¹„ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê±±ì •ì´ ë§ìœ¼ì‹œê² ì–´ìš”.  \n",
              "51626                               ê°€ì¡±ê³¼ì˜ ë¬¸ì œ ë•Œë¬¸ì— ì†ìƒí•˜ì‹œê² ì–´ìš”.  \n",
              "51627                    ëŒ€ì¸ê´€ê³„ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê±±ì •ë˜ì‹œê³  ì†ìƒí•˜ì‹œê² ì–´ìš”.  \n",
              "\n",
              "[51628 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7db60d53-8920-467f-a527-dfcf5a11a24b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HS01</th>\n",
              "      <th>SS01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.</td>\n",
              "      <td>ë§ì´ í˜ë“œì‹œê² ì–´ìš”. ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìˆë‚˜ìš”?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ì´ë²ˆ ë‹¬ì— ë˜ ê¸‰ì—¬ê°€ ê¹ì˜€ì–´! ë¬¼ê°€ëŠ” ì˜¤ë¥´ëŠ”ë° ì›”ê¸‰ë§Œ ìê¾¸ ê¹ì´ë‹ˆê¹Œ ë„ˆë¬´ í™”ê°€ ë‚˜.</td>\n",
              "      <td>ê¸‰ì—¬ê°€ ì¤„ì–´ ì†ìƒí•˜ì‹œê² ì–´ìš”. ì›”ê¸‰ì´ ì¤„ì–´ë“  ê²ƒì„ ì–´ë–»ê²Œ ë³´ì™„í•˜ì‹¤ ê±´ê°€ìš”?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>íšŒì‚¬ì— ì‹ ì…ì´ ë“¤ì–´ì™”ëŠ”ë° ë§íˆ¬ê°€ ê±°ìŠ¬ë ¤. ê·¸ëŸ° ì• ë¥¼ ë§¤ì¼ ë´ì•¼ í•œë‹¤ê³  ìƒê°í•˜ë‹ˆê¹Œ ìŠ¤...</td>\n",
              "      <td>íšŒì‚¬ ë™ë£Œ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ëŠ” ê²ƒ ê°™ì•„ìš”. ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì–´ë–¤ ë…¸ë ¥ì„ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ì§ì¥ì—ì„œ ë§‰ë‚´ë¼ëŠ” ì´ìœ ë¡œ ë‚˜ì—ê²Œë§Œ ì˜¨ê°– ì‹¬ë¶€ë¦„ì„ ì‹œì¼œ. ì¼ë„ ë§ì€ ë° ì •ë§ ë¶„í•˜ê³  ...</td>\n",
              "      <td>ê´€ë ¨ ì—†ëŠ” ì‹¬ë¶€ë¦„ì„ ëª¨ë‘ í•˜ê²Œ ë˜ì–´ì„œ ë…¸ì—¬ìš°ì‹œêµ°ìš”. ì–´ë–¤ ê²ƒì´ ìƒí™©ì„ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ì–¼ë§ˆ ì „ ì…ì‚¬í•œ ì‹ ì…ì‚¬ì›ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ë„ˆë¬´ í™”ê°€ ë‚˜.</td>\n",
              "      <td>ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì€ íƒœë„ì— í™”ê°€ ë‚˜ì…¨êµ°ìš”. ìƒëŒ€ë°©ì˜ ì–´ë–¤ í–‰ë™ì´ ê·¸ëŸ° ê°ì •ì„ ìœ ë°œí•˜ëŠ”...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51623</th>\n",
              "      <td>ë‚˜ì´ê°€ ë¨¹ê³  ì´ì œ ëˆë„ ëª» ë²Œì–´ ì˜¤ë‹ˆê¹Œ ì–´ë–»ê²Œ ì‚´ì•„ê°€ì•¼ í• ì§€ ë§‰ë§‰í•´. ëŠ¥ë ¥ë„ ì—†ê³ .</td>\n",
              "      <td>ê²½ì œì ì¸ ë¬¸ì œ ë•Œë¬¸ì— ë§‰ë§‰í•˜ì‹œêµ°ìš”. ë§ˆìŒì´ í¸ì¹˜ ì•Šìœ¼ì‹œê² ì–´ìš”.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51624</th>\n",
              "      <td>ëª¸ì´ ë§ì´ ì•½í•´ì¡Œë‚˜ ë´. ì´ì œ ì „ê³¼ ê°™ì´ ì¼í•˜ì§€ ëª»í•  ê²ƒ ê°™ì•„ ë„ˆë¬´ ì§œì¦ ë‚˜.</td>\n",
              "      <td>ê±´ê°•ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê¸°ë¶„ì´ ì¢‹ì§€ ì•Šìœ¼ì‹œêµ°ìš”. ì†ìƒí•˜ì‹œê² ì–´ìš”.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51625</th>\n",
              "      <td>ì´ì œ ì–´ë–»ê²Œ í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´. ë‚¨í¸ë„ ê·¸ë ‡ê³  ë…¸í›„ ì¤€ë¹„ë„ ì•ˆ ë˜ì–´ì„œ ë¯¸ë˜ê°€ ê±±ì •ë¼.</td>\n",
              "      <td>ë…¸í›„ ì¤€ë¹„ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê±±ì •ì´ ë§ìœ¼ì‹œê² ì–´ìš”.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51626</th>\n",
              "      <td>ëª‡ì‹­ ë…„ì„ í•¨ê»˜ ì‚´ì•˜ë˜ ë‚¨í¸ê³¼ ì´í˜¼í–ˆì–´. ê·¸ë™ì•ˆì˜ ì„¸ì›”ì— ë°°ì‹ ê°ì„ ëŠë¼ê³  ë„ˆë¬´ í™”ê°€ ë‚˜.</td>\n",
              "      <td>ê°€ì¡±ê³¼ì˜ ë¬¸ì œ ë•Œë¬¸ì— ì†ìƒí•˜ì‹œê² ì–´ìš”.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51627</th>\n",
              "      <td>ë‚¨í¸ê³¼ ê²°í˜¼í•œ ì§€ ì‚¬ì‹­ ë…„ì´ì•¼. ì´ì œ ì‚¬ëŒ ë§Œë‚˜ëŠ” ê²ƒë„ ë²„ê²ê³  ì•Œë˜ ì‚¬ëŒë„ ì ì  ì‚¬ë¼ì ¸.</td>\n",
              "      <td>ëŒ€ì¸ê´€ê³„ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê±±ì •ë˜ì‹œê³  ì†ìƒí•˜ì‹œê² ì–´ìš”.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>51628 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7db60d53-8920-467f-a527-dfcf5a11a24b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7db60d53-8920-467f-a527-dfcf5a11a24b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7db60d53-8920-467f-a527-dfcf5a11a24b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c531fd22-b3bb-4876-931f-4998e423d243\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c531fd22-b3bb-4876-931f-4998e423d243')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c531fd22-b3bb-4876-931f-4998e423d243 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df[['HS01','SS01']]\",\n  \"rows\": 51628,\n  \"fields\": [\n    {\n      \"column\": \"HS01\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 51600,\n        \"samples\": [\n          \"\\ub0a8\\ud3b8\\uc774 \\ubc8c\\uc5b4\\uc624\\ub294 \\ub3c8\\ub9cc\\uc73c\\ub85c\\ub294 \\ub109\\ub109\\uc9c0\\uac00 \\uc54a\\uc544\\uc11c \\uc77c\\uc744 \\uc54c\\uc544\\ubcf4\\uace0 \\uc788\\ub294\\ub370 \\ud560 \\uc218 \\uc788\\ub294 \\uc77c\\uc774 \\uc5c6\\uc5b4.\",\n          \"\\ub098\\ub294 \\ud328\\ubc30\\uc790\\uc57c.\",\n          \"\\uc624\\ub298 \\uc624\\ub79c\\ub9cc\\uc5d0 \\ub300\\ud559 \\ub3d9\\uae30\\ub4e4\\uacfc \\uc800\\ub141 \\uba39\\uae30\\ub85c \\ud55c \\ub0a0\\uc774\\uc57c. \\uac04\\ub9cc\\uc5d0 \\uce5c\\uad6c\\ub4e4 \\ubcfc \\uc0dd\\uac01\\ud558\\ub2c8 \\ub108\\ubb34 \\uae30\\ubd84 \\uc88b\\uc544.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SS01\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50352,\n        \"samples\": [\n          \"\\uac70\\uc561\\uc744 \\uc190\\uc5d0 \\uc950\\uac8c \\ub418\\uc5b4 \\ubd88\\uc548\\ud558\\uba74\\uc11c\\ub3c4 \\ub5a8\\ub9ac\\uc2dc\\uaca0\\uc5b4\\uc694.\",\n          \"\\ub0a8\\ud3b8\\ubd84\\uc774 \\uc9d1\\uc548\\uc77c\\uc744 \\uc798\\ud55c\\ub2e4\\uace0 \\uc8fc\\uc7a5\\ud574\\uc11c \\ud669\\ub2f9\\ud558\\uc2dc\\uad70\\uc694.\",\n          \"\\uacb0\\ud63c \\uc900\\ube44\\uc758 \\uc5b4\\ub5a4 \\ubd80\\ubd84\\uc5d0 \\ub9c8\\uc74c\\uc774 \\ubd88\\ud3b8\\ud558\\uc2e0\\uac00\\uc694? \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### í•™ìŠµ ë°ì´í„°ì„¸íŠ¸ êµ¬ì„±"
      ],
      "metadata": {
        "id": "rgh2mlJqevu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BPE :ë°”ì´íŠ¸ í˜ì–´ ì¸ì½”ë”©(Byte Pair Encoding, BPE)\n",
        "import os, re\n",
        "\n",
        "# ì¶”ê°€ ì“°ê¸°ëª¨ë“œë¡œ í…ìŠ¤íŠ¸ íŒŒì¼ ì—´ê¸°\n",
        "with open('train.txt', 'w', encoding='utf-8') as f:\n",
        "  for text in df['HS01']:\n",
        "        text = str(text)\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)     # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
        "        text = re.sub(r'[\\n\\t]', ' ', text)     # ì¤„ë°”ê¿ˆ, íƒ­ ì œê±°\n",
        "        text = re.sub(r'\\s+', ' ', text)        # ì—°ì†ëœ ê³µë°± ì œê±°\n",
        "        text= text.strip()                      # ë¬¸ì¥ ì–‘ë ê³µë°± ì œê±°\n",
        "        try:\n",
        "            f.write(text+'\\n')\n",
        "        except:\n",
        "                pass\n",
        "\n",
        "# ì €ì¥ ê²½ë¡œ ìƒì„±\n",
        "os.makedirs('./bpe', exist_ok=True)\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input='train.txt',                      # í…ìŠ¤íŠ¸ ë­‰ì¹˜ íŒŒì¼\n",
        "    model_prefix='./bpe/spm_krsent',        # ì¶œë ¥ ëª¨ë¸ íŒŒì¼ ì´ë¦„\n",
        "    vocab_size=2000                         # í† í° ê°œìˆ˜\n",
        ")\n",
        "\n",
        "spm.SentencePieceTrainer.train(input='train.txt',               # í…ìŠ¤íŠ¸ ë­‰ì¹˜ íŒŒì¼\n",
        "                            model_prefix='./bpe/spm_krsent',    # ì¶œë ¥ ëª¨ë¸ íŒŒì¼ ì´ë¦„\n",
        "                            vocab_size=4000,                    # í† í° ê°œìˆ˜\n",
        "                            bos_id=1,\n",
        "                            eos_id=2,\n",
        "                            unk_id=3,\n",
        "                            pad_id=0\n",
        "                            )"
      ],
      "metadata": {
        "id": "4ZvxhI8FeJv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SPDataSet(Dataset):\n",
        "    def __init__(self, sp, max_len):\n",
        "        self.max_len = max_len\n",
        "        self.df = pd.read_csv(f'./train.csv')\n",
        "        self.sp = sp\n",
        "\n",
        "        # SOP íƒœìŠ¤í¬ë¥¼ ìœ„í•œ (sent1, sent2, label) ë¦¬ìŠ¤íŠ¸\n",
        "        self.pairs = []\n",
        "\n",
        "        # \"ì˜¬ë°”ë¥¸ ìˆœì„œ\" => label = 0\n",
        "        # \"ë’¤ì§‘íŒ ìˆœì„œ\" => label = 1\n",
        "        for _, item in self.df.iterrows():\n",
        "            sent1 = item['HS01']\n",
        "            sent2 = item['SS01']\n",
        "\n",
        "            # ì›ë˜ ìˆœì„œ\n",
        "            self.pairs.append((sent1, sent2, 0))\n",
        "            # ë’¤ì§‘íŒ ìˆœì„œ\n",
        "            self.pairs.append((sent2, sent1, 1))\n",
        "\n",
        "        # 10ê°œì˜ ë¬¸ì¥ìŒë§Œ í™•ì¸ (ë””ë²„ê·¸ ìš©ë„)\n",
        "        print(self.pairs[:10])\n",
        "\n",
        "    def zero_pad(self, tok):\n",
        "        \"\"\"í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ max_len ê¸¸ì´ì— ë§ì¶° ì œë¡œ íŒ¨ë”©\"\"\"\n",
        "        if len(tok) >= self.max_len:\n",
        "            return tok[:self.max_len]\n",
        "        else:\n",
        "            padding = np.zeros(self.max_len, dtype=np.int64)\n",
        "            padding[:len(tok)] = tok\n",
        "            return padding\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent1, sent2, label = self.pairs[idx]\n",
        "\n",
        "        # SentencePieceë¡œ ë¬¸ì¥ â†’ ID ì‹œí€€ìŠ¤\n",
        "        sent1_ids = self.sp.encode_as_ids(sent1)\n",
        "        sent2_ids = self.sp.encode_as_ids(sent2)\n",
        "\n",
        "        # ë‘ ë¬¸ì¥ì„ [BOS] + sent1 + [EOS] + sent2 + [EOS]ë¡œ ì´ì–´ ë¶™ì„\n",
        "        inp = [self.sp.bos_id()] + sent1_ids + [self.sp.eos_id()] \\\n",
        "              + sent2_ids + [self.sp.eos_id()]\n",
        "\n",
        "        inp = self.zero_pad(inp)  # (max_len,) numpy array\n",
        "        inp_tensor = torch.tensor(inp, dtype=torch.long)\n",
        "\n",
        "        # íŒ¨ë”© ë§ˆìŠ¤í¬\n",
        "        mask = inp_tensor.eq(0)   # True/False í…ì„œ\n",
        "\n",
        "        return inp_tensor, torch.tensor(label, dtype=torch.long), mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "\n",
        "# ì˜ˆì‹œ ì‚¬ìš©\n",
        "sp = spm.SentencePieceProcessor(model_file=f'./bpe/spm_krsent.model')\n",
        "dataset = SPDataSet(sp, max_len=60)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "for inp, tar, mask in dataloader:\n",
        "    print(f'ì…ë ¥ í† í° : {inp}')\n",
        "    print(f'SOP ë¼ë²¨ : {tar}')\n",
        "    print(f'íŒ¨ë”© ë§ˆìŠ¤í¬ : {mask}')\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBAxqPrmeIu0",
        "outputId": "ee32100e-5363-4729-b253-bee40474ea4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.', 'ë§ì´ í˜ë“œì‹œê² ì–´ìš”. ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìˆë‚˜ìš”?', 0), ('ë§ì´ í˜ë“œì‹œê² ì–´ìš”. ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìˆë‚˜ìš”?', 'ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.', 1), ('ì´ë²ˆ ë‹¬ì— ë˜ ê¸‰ì—¬ê°€ ê¹ì˜€ì–´! ë¬¼ê°€ëŠ” ì˜¤ë¥´ëŠ”ë° ì›”ê¸‰ë§Œ ìê¾¸ ê¹ì´ë‹ˆê¹Œ ë„ˆë¬´ í™”ê°€ ë‚˜.', 'ê¸‰ì—¬ê°€ ì¤„ì–´ ì†ìƒí•˜ì‹œê² ì–´ìš”. ì›”ê¸‰ì´ ì¤„ì–´ë“  ê²ƒì„ ì–´ë–»ê²Œ ë³´ì™„í•˜ì‹¤ ê±´ê°€ìš”?', 0), ('ê¸‰ì—¬ê°€ ì¤„ì–´ ì†ìƒí•˜ì‹œê² ì–´ìš”. ì›”ê¸‰ì´ ì¤„ì–´ë“  ê²ƒì„ ì–´ë–»ê²Œ ë³´ì™„í•˜ì‹¤ ê±´ê°€ìš”?', 'ì´ë²ˆ ë‹¬ì— ë˜ ê¸‰ì—¬ê°€ ê¹ì˜€ì–´! ë¬¼ê°€ëŠ” ì˜¤ë¥´ëŠ”ë° ì›”ê¸‰ë§Œ ìê¾¸ ê¹ì´ë‹ˆê¹Œ ë„ˆë¬´ í™”ê°€ ë‚˜.', 1), ('íšŒì‚¬ì— ì‹ ì…ì´ ë“¤ì–´ì™”ëŠ”ë° ë§íˆ¬ê°€ ê±°ìŠ¬ë ¤. ê·¸ëŸ° ì• ë¥¼ ë§¤ì¼ ë´ì•¼ í•œë‹¤ê³  ìƒê°í•˜ë‹ˆê¹Œ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„. ', 'íšŒì‚¬ ë™ë£Œ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ëŠ” ê²ƒ ê°™ì•„ìš”. ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì–´ë–¤ ë…¸ë ¥ì„ í•˜ë©´ ì¢‹ì„ê¹Œìš”?', 0), ('íšŒì‚¬ ë™ë£Œ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ëŠ” ê²ƒ ê°™ì•„ìš”. ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì–´ë–¤ ë…¸ë ¥ì„ í•˜ë©´ ì¢‹ì„ê¹Œìš”?', 'íšŒì‚¬ì— ì‹ ì…ì´ ë“¤ì–´ì™”ëŠ”ë° ë§íˆ¬ê°€ ê±°ìŠ¬ë ¤. ê·¸ëŸ° ì• ë¥¼ ë§¤ì¼ ë´ì•¼ í•œë‹¤ê³  ìƒê°í•˜ë‹ˆê¹Œ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„. ', 1), ('ì§ì¥ì—ì„œ ë§‰ë‚´ë¼ëŠ” ì´ìœ ë¡œ ë‚˜ì—ê²Œë§Œ ì˜¨ê°– ì‹¬ë¶€ë¦„ì„ ì‹œì¼œ. ì¼ë„ ë§ì€ ë° ì •ë§ ë¶„í•˜ê³  ì„­ì„­í•´.', 'ê´€ë ¨ ì—†ëŠ” ì‹¬ë¶€ë¦„ì„ ëª¨ë‘ í•˜ê²Œ ë˜ì–´ì„œ ë…¸ì—¬ìš°ì‹œêµ°ìš”. ì–´ë–¤ ê²ƒì´ ìƒí™©ì„ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆê²Œ ë„ì›€ì„ ì¤„ê¹Œìš”?', 0), ('ê´€ë ¨ ì—†ëŠ” ì‹¬ë¶€ë¦„ì„ ëª¨ë‘ í•˜ê²Œ ë˜ì–´ì„œ ë…¸ì—¬ìš°ì‹œêµ°ìš”. ì–´ë–¤ ê²ƒì´ ìƒí™©ì„ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆê²Œ ë„ì›€ì„ ì¤„ê¹Œìš”?', 'ì§ì¥ì—ì„œ ë§‰ë‚´ë¼ëŠ” ì´ìœ ë¡œ ë‚˜ì—ê²Œë§Œ ì˜¨ê°– ì‹¬ë¶€ë¦„ì„ ì‹œì¼œ. ì¼ë„ ë§ì€ ë° ì •ë§ ë¶„í•˜ê³  ì„­ì„­í•´.', 1), ('ì–¼ë§ˆ ì „ ì…ì‚¬í•œ ì‹ ì…ì‚¬ì›ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ë„ˆë¬´ í™”ê°€ ë‚˜.', 'ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì€ íƒœë„ì— í™”ê°€ ë‚˜ì…¨êµ°ìš”. ìƒëŒ€ë°©ì˜ ì–´ë–¤ í–‰ë™ì´ ê·¸ëŸ° ê°ì •ì„ ìœ ë°œí•˜ëŠ” ê²ƒì¼ê¹Œìš”?', 0), ('ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì€ íƒœë„ì— í™”ê°€ ë‚˜ì…¨êµ°ìš”. ìƒëŒ€ë°©ì˜ ì–´ë–¤ í–‰ë™ì´ ê·¸ëŸ° ê°ì •ì„ ìœ ë°œí•˜ëŠ” ê²ƒì¼ê¹Œìš”?', 'ì–¼ë§ˆ ì „ ì…ì‚¬í•œ ì‹ ì…ì‚¬ì›ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ë„ˆë¬´ í™”ê°€ ë‚˜.', 1)]\n",
            "ì…ë ¥ í† í° : tensor([[   1,  340,   25,  919,  665,  259,   64,  117,   53, 1024,  143,  427,\n",
            "          783,    3,    2,  334,  340,  919,  665,   53,   19,  140,    5,   72,\n",
            "            3,  690,   84,  155,    6,  580,    9,  117,  344,    3,    2,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [   1,  120,    4,   24,   24,  114,  623,   18,   22,  339,  143,  760,\n",
            "          783,    3,    2,   31,  137,  184,    4, 1386,    5, 3105,   12,  240,\n",
            "          122,   11,    7,    4,   24,   24,  114,  623,   18,   22,    3,    2,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])\n",
            "SOP ë¼ë²¨ : tensor([1, 1])\n",
            "íŒ¨ë”© ë§ˆìŠ¤í¬ : tensor([[False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_tokens_for_mlm(input_ids, special_tokens_ids, mask_token_id, vocab_size, mlm_prob=0.15):\n",
        "    device = input_ids.device\n",
        "    B, L = input_ids.shape\n",
        "\n",
        "    # ì…ë ¥ í† í°ì„ ë³µì‚¬í•˜ì—¬ ì‚¬ìš©\n",
        "    masked_input_ids = input_ids.clone()\n",
        "    # ë¼ë²¨ í† í°ì„ ìœ„í•´ ë™ì¼ í˜•ìƒì˜ -100 í…ì„œ ìƒì„±\n",
        "    mlm_labels = torch.full_like(input_ids, -100)  # -100ì€ ë¼ë²¨ì—ì„œ ë¬´ì‹œë˜ëŠ” í† í°ìœ¼ë¡œ ì‚¬ìš©\n",
        "\n",
        "    # ìŠ¤í˜ì…œ í† í° ìœ„ì¹˜ë¥¼ True\n",
        "    special_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
        "    for sp_id in special_tokens_ids:\n",
        "        special_mask |= (input_ids == sp_id)\n",
        "\n",
        "    # 0~1 ì‚¬ì´ì˜ ë¬´ì‘ìœ„ í™•ë¥  ë¶„í¬ ìƒì„±\n",
        "    rand_vals = torch.rand_like(input_ids.float())\n",
        "\n",
        "    # mlm_prob(0.15) ë³´ë‹¤ ì‘ê³  ìŠ¤í˜ì…œ í† í°ì¸ ì•„ë‹Œê²½ìš°ë§Œ\n",
        "    to_mask = (rand_vals < mlm_prob) & (~special_mask)\n",
        "\n",
        "    # ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì˜ ì •ë‹µ ë ˆì´ë¸” = ì›ë³¸ í† í°\n",
        "    mlm_labels[to_mask] = input_ids[to_mask]\n",
        "\n",
        "    # BERT ë…¼ë¬¸ ë¹„ìœ¨: 80% -> [MASK], 10% -> random, 10% -> ì›ë³¸\n",
        "    mask_choices = torch.rand_like(input_ids.float())\n",
        "\n",
        "    # mask_choicesì—ì„œ 80%ì— í•´ë‹¹í•˜ê³  ë§ˆìŠ¤í¬ ì¸ë±ìŠ¤ì¸ ê²½ìš° ë§ˆìŠ¤í¬ í† í°ìœ¼ë¡œ\n",
        "    mask_1 = (mask_choices < 0.8) & to_mask\n",
        "    masked_input_ids[mask_1] = mask_token_id\n",
        "\n",
        "    # mask_choicesì—ì„œ 10%ì— í•´ë‹¹í•˜ê³  ë§ˆìŠ¤í¬ ì¸ë±ìŠ¤ì¸ ê²½ìš° ëœë¤ í† í° ë³€í™˜\n",
        "    mask_2 = (mask_choices >= 0.8) & (mask_choices < 0.9) & to_mask\n",
        "    random_tokens = torch.randint(0, vocab_size, size=(), device=device)\n",
        "    masked_input_ids[mask_2] = random_tokens\n",
        "\n",
        "    return masked_input_ids, mlm_labels"
      ],
      "metadata": {
        "id": "qinN7Tmae-hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ëª¨ë¸ë§"
      ],
      "metadata": {
        "id": "LW3Pr9CKfGQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ğŸ—ï¸ Multi-Head Transformer Block\n",
        "    - Transformerì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œ\n",
        "    - Self-Attention + Feed Forward Network\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, nhead, feed_dim=512, gelu=False, dropout=0.):\n",
        "        super(MTBlock, self).__init__()\n",
        "\n",
        "        self.mha = nn.MultiheadAttention(hidden_dim, nhead, dropout=dropout, batch_first=True)\n",
        "        if gelu:\n",
        "            self.active = nn.GELU()\n",
        "        else:\n",
        "            self.active = nn.ReLU()\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, feed_dim),\n",
        "            self.active,\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(feed_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # ì…€í”„ ì–´í…ì…˜\n",
        "        attn_output, _ = self.mha(x, x, x, key_padding_mask=mask, need_weights=False)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.norm1(x + attn_output)\n",
        "\n",
        "        # í”¼ë“œí¬ì›Œë“œ\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.norm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ],
      "metadata": {
        "id": "RmsyYWdWfIdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, max_pos=10000):\n",
        "        super().__init__()\n",
        "        # ìœ„ì¹˜ ì¸ë±ìŠ¤\n",
        "        position = torch.arange(max_pos).unsqueeze(1)\n",
        "        # ìœ„ì¹˜ì— ë”°ë¥¸ ê°ë„ ì¶”ì¶œ\n",
        "        # ì§€ìˆ˜(exp)ì™€ ë¡œê·¸(log)ë¥¼ í•¨ê»˜ì¨ ì§€ìˆ˜ìŠ¹ í˜•íƒœ(**)ë¥¼ ë§Œë“¤ì–´ ì‚¬ìš©\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
        "\n",
        "        # ì œë¡œ í…ì„œë¥¼ ë§Œë“¤ê³  sin,cos ê²°ê³¼ í• ë‹¹\n",
        "        pe = torch.zeros(1, max_pos, embed_dim)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe) # í•™ìŠµë˜ì§€ ì•ŠëŠ” ëª¨ë“ˆë¡œ ì €ì¥\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ì„ë² ë”©ì´ ì…ë ¥ë˜ë©´ í¬ì§€ì…˜ê³¼ ë”í•˜ì—¬ ë°˜í™˜\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x"
      ],
      "metadata": {
        "id": "equ0dKmRfKg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleALBERT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dim=64,\n",
        "        hidden_dim=256,\n",
        "        num_heads=4,\n",
        "        feed_dim=512,\n",
        "        num_layers=4,\n",
        "        num_classes=2,   # SOP ë˜í•œ ì´ì§„ ë¶„ë¥˜\n",
        "        dropout=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # ì‘ì€ ì°¨ì›ì˜ ë‹¨ì–´ ì„ë² ë”©\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # ì„ë² ë”© í”„ë¡œì ì…˜ â†’ hidden_dim\n",
        "        self.embedding_proj = nn.Linear(embedding_dim, hidden_dim)\n",
        "\n",
        "        # ìœ„ì¹˜ ì¸ì½”ë”©\n",
        "        self.pos_encoding = PositionalEncoding(hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # ALBERT: ëª¨ë“  ë ˆì´ì–´ ê°€ì¤‘ì¹˜ ê³µìœ \n",
        "        self.transformer_block = MTBlock(hidden_dim, num_heads, feed_dim, dropout=dropout)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # MLM í—¤ë“œ\n",
        "        self.mlm_head = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "        # SOP í—¤ë“œ\n",
        "        self.sop_head = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        # ì„ë² ë”© + í”„ë¡œì ì…˜\n",
        "        emb = self.word_embeddings(x)            # (B, L, embedding_dim)\n",
        "        emb = self.embedding_proj(emb)           # (B, L, hidden_dim)\n",
        "\n",
        "        # ìœ„ì¹˜ ì¸ì½”ë”© & ë“œë¡­ì•„ì›ƒ\n",
        "        emb = self.pos_encoding(emb)\n",
        "        emb = self.dropout(emb)\n",
        "\n",
        "        # ë™ì¼í•œ Transformer ë¸”ë¡ì„ num_layersë²ˆ ë°˜ë³µ(ê°€ì¤‘ì¹˜ ê³µìœ )\n",
        "        x_out = emb\n",
        "        for _ in range(self.num_layers):\n",
        "            x_out = self.transformer_block(x_out, mask)\n",
        "\n",
        "        # MLM ë¡œì§“\n",
        "        mlm_logits = self.mlm_head(x_out)        # (B, L, vocab_size)\n",
        "\n",
        "        # CLS í† í°(ì²« ë²ˆì§¸ í† í°)ìœ¼ë¡œ SOP ë¡œì§“\n",
        "        cls_emb = x_out[:, 0]                    # (B, hidden_dim)\n",
        "        sop_logits = self.sop_head(cls_emb)      # (B, 2)\n",
        "\n",
        "        return mlm_logits, sop_logits\n",
        "\n",
        "vocab_size = sp.get_piece_size()\n",
        "model = SimpleALBERT(vocab_size=vocab_size)\n",
        "with torch.no_grad():\n",
        "    for inp, tar, mask in dataloader:\n",
        "        mlm_logits, nsp_logits = model(inp.long(), mask)\n",
        "        print(mlm_logits.shape)\n",
        "        print(nsp_logits)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKwxCjugfNEY",
        "outputId": "608764e4-5f2c-477e-c7a8-5341b09c8295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 60, 4000])\n",
            "tensor([[-0.3710, -0.0163],\n",
            "        [-0.0922,  0.0738]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ëª¨ë¸ í•™ìŠµ"
      ],
      "metadata": {
        "id": "QRjS5VZIfP7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜\n",
        "seq_len = 100\n",
        "embed_dim = 64\n",
        "hidden_dim=128\n",
        "num_heads = 4\n",
        "feed_dim = 256\n",
        "num_layers = 4\n",
        "num_classes = 2\n",
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "lr = 1e-4\n",
        "\n",
        "# MLM ë³€í™˜ í•¨ìˆ˜ì— ë„£ì–´ì¤„ ìŠ¤í˜ì…œ í† í° ì •ì˜\n",
        "mask_id = sp.get_piece_size()\n",
        "special_tokens_ids = [sp.bos_id(), sp.eos_id(), 0]\n",
        "\n",
        "# ë§ˆìŠ¤í¬ í† í°ì´ ì¶”ê°€ë˜ë¯€ë¡œ í† í°ê°œìˆ˜ì— +1\n",
        "sp = spm.SentencePieceProcessor(model_file=f'./bpe/spm_krsent.model')\n",
        "vocab_size = sp.get_piece_size() + 1\n",
        "\n",
        "# ë°ì´í„°ì„¸íŠ¸ ë¶„í• \n",
        "dataset = SPDataSet(sp, seq_len)\n",
        "generator1 = torch.Generator().manual_seed(42)\n",
        "test_dataset, train_dataset = random_split(dataset, [0.2, 0.8], generator=generator1)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f'train dataset size: {len(train_dataset)}')\n",
        "print(f'test dataset size: {len(test_dataset)}')\n",
        "\n",
        "# ëª¨ë¸ ìƒì„±\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleALBERT(vocab_size, embed_dim, hidden_dim, num_heads, feed_dim, num_layers, num_classes).to(device)\n",
        "\n",
        "# 2ê°œì˜ ì†ì‹¤ ìƒì„±\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "mlm_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)  # MLM\n",
        "sop_loss_fn = nn.CrossEntropyLoss() # SOP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM3XPQkkfRN-",
        "outputId": "d0c9fb2d-7190-40a2-c470-aa906c207660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.', 'ë§ì´ í˜ë“œì‹œê² ì–´ìš”. ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìˆë‚˜ìš”?', 0), ('ë§ì´ í˜ë“œì‹œê² ì–´ìš”. ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìˆë‚˜ìš”?', 'ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.', 1), ('ì´ë²ˆ ë‹¬ì— ë˜ ê¸‰ì—¬ê°€ ê¹ì˜€ì–´! ë¬¼ê°€ëŠ” ì˜¤ë¥´ëŠ”ë° ì›”ê¸‰ë§Œ ìê¾¸ ê¹ì´ë‹ˆê¹Œ ë„ˆë¬´ í™”ê°€ ë‚˜.', 'ê¸‰ì—¬ê°€ ì¤„ì–´ ì†ìƒí•˜ì‹œê² ì–´ìš”. ì›”ê¸‰ì´ ì¤„ì–´ë“  ê²ƒì„ ì–´ë–»ê²Œ ë³´ì™„í•˜ì‹¤ ê±´ê°€ìš”?', 0), ('ê¸‰ì—¬ê°€ ì¤„ì–´ ì†ìƒí•˜ì‹œê² ì–´ìš”. ì›”ê¸‰ì´ ì¤„ì–´ë“  ê²ƒì„ ì–´ë–»ê²Œ ë³´ì™„í•˜ì‹¤ ê±´ê°€ìš”?', 'ì´ë²ˆ ë‹¬ì— ë˜ ê¸‰ì—¬ê°€ ê¹ì˜€ì–´! ë¬¼ê°€ëŠ” ì˜¤ë¥´ëŠ”ë° ì›”ê¸‰ë§Œ ìê¾¸ ê¹ì´ë‹ˆê¹Œ ë„ˆë¬´ í™”ê°€ ë‚˜.', 1), ('íšŒì‚¬ì— ì‹ ì…ì´ ë“¤ì–´ì™”ëŠ”ë° ë§íˆ¬ê°€ ê±°ìŠ¬ë ¤. ê·¸ëŸ° ì• ë¥¼ ë§¤ì¼ ë´ì•¼ í•œë‹¤ê³  ìƒê°í•˜ë‹ˆê¹Œ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„. ', 'íšŒì‚¬ ë™ë£Œ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ëŠ” ê²ƒ ê°™ì•„ìš”. ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì–´ë–¤ ë…¸ë ¥ì„ í•˜ë©´ ì¢‹ì„ê¹Œìš”?', 0), ('íšŒì‚¬ ë™ë£Œ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ëŠ” ê²ƒ ê°™ì•„ìš”. ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì–´ë–¤ ë…¸ë ¥ì„ í•˜ë©´ ì¢‹ì„ê¹Œìš”?', 'íšŒì‚¬ì— ì‹ ì…ì´ ë“¤ì–´ì™”ëŠ”ë° ë§íˆ¬ê°€ ê±°ìŠ¬ë ¤. ê·¸ëŸ° ì• ë¥¼ ë§¤ì¼ ë´ì•¼ í•œë‹¤ê³  ìƒê°í•˜ë‹ˆê¹Œ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„. ', 1), ('ì§ì¥ì—ì„œ ë§‰ë‚´ë¼ëŠ” ì´ìœ ë¡œ ë‚˜ì—ê²Œë§Œ ì˜¨ê°– ì‹¬ë¶€ë¦„ì„ ì‹œì¼œ. ì¼ë„ ë§ì€ ë° ì •ë§ ë¶„í•˜ê³  ì„­ì„­í•´.', 'ê´€ë ¨ ì—†ëŠ” ì‹¬ë¶€ë¦„ì„ ëª¨ë‘ í•˜ê²Œ ë˜ì–´ì„œ ë…¸ì—¬ìš°ì‹œêµ°ìš”. ì–´ë–¤ ê²ƒì´ ìƒí™©ì„ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆê²Œ ë„ì›€ì„ ì¤„ê¹Œìš”?', 0), ('ê´€ë ¨ ì—†ëŠ” ì‹¬ë¶€ë¦„ì„ ëª¨ë‘ í•˜ê²Œ ë˜ì–´ì„œ ë…¸ì—¬ìš°ì‹œêµ°ìš”. ì–´ë–¤ ê²ƒì´ ìƒí™©ì„ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆê²Œ ë„ì›€ì„ ì¤„ê¹Œìš”?', 'ì§ì¥ì—ì„œ ë§‰ë‚´ë¼ëŠ” ì´ìœ ë¡œ ë‚˜ì—ê²Œë§Œ ì˜¨ê°– ì‹¬ë¶€ë¦„ì„ ì‹œì¼œ. ì¼ë„ ë§ì€ ë° ì •ë§ ë¶„í•˜ê³  ì„­ì„­í•´.', 1), ('ì–¼ë§ˆ ì „ ì…ì‚¬í•œ ì‹ ì…ì‚¬ì›ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ë„ˆë¬´ í™”ê°€ ë‚˜.', 'ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì€ íƒœë„ì— í™”ê°€ ë‚˜ì…¨êµ°ìš”. ìƒëŒ€ë°©ì˜ ì–´ë–¤ í–‰ë™ì´ ê·¸ëŸ° ê°ì •ì„ ìœ ë°œí•˜ëŠ” ê²ƒì¼ê¹Œìš”?', 0), ('ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì€ íƒœë„ì— í™”ê°€ ë‚˜ì…¨êµ°ìš”. ìƒëŒ€ë°©ì˜ ì–´ë–¤ í–‰ë™ì´ ê·¸ëŸ° ê°ì •ì„ ìœ ë°œí•˜ëŠ” ê²ƒì¼ê¹Œìš”?', 'ì–¼ë§ˆ ì „ ì…ì‚¬í•œ ì‹ ì…ì‚¬ì›ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ë„ˆë¬´ í™”ê°€ ë‚˜.', 1)]\n",
            "train dataset size: 82604\n",
            "test dataset size: 20652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('./checkpoint'):\n",
        "    os.mkdir('./checkpoint')\n",
        "\n",
        "train_loss = []\n",
        "test_acc = []\n",
        "# í•™ìŠµ ë£¨í”„\n",
        "for epoch in range(num_epochs):\n",
        "    t_mlm_loss = 0.0\n",
        "    t_sop_loss = 0.0\n",
        "    model.train()\n",
        "    for i, (inp, labels, mask) in enumerate(train_loader):\n",
        "        inp = inp.long().to(device)\n",
        "        nsp_label = labels.to(device)\n",
        "        mask = mask.to(device)\n",
        "        mlm_inp, mlm_labels = mask_tokens_for_mlm(inp.long(), special_tokens_ids, mask_id, vocab_size)\n",
        "        mlm_logits, sop_logits = model(mlm_inp, mask)\n",
        "\n",
        "        # MLMì€ batch*seq í˜•ìƒ ë³€í™˜ í•˜ì—¬ ì†ì‹¤ ê³„ì‚°\n",
        "        mlm_loss = mlm_loss_fn(mlm_logits.view(-1, vocab_size), mlm_labels.view(-1))\n",
        "        # SOP ì†ì‹¤ ê³„ì‚°\n",
        "        sop_loss = sop_loss_fn(sop_logits, nsp_label)\n",
        "\n",
        "        # ë‘ ì†ì‹¤ì„ ë”í•˜ì—¬ ì—­ì „íŒŒ\n",
        "        loss = mlm_loss + sop_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        t_mlm_loss += mlm_loss.item()\n",
        "        t_sop_loss += sop_loss.item()\n",
        "\n",
        "        if (i+1) % 200 == 0:\n",
        "            print(f\"Epoch: {epoch}, Batch: {i+1}, MLM Loss: {t_mlm_loss/(i+1)}, SOP Loss:{t_sop_loss/(i+1)}\")\n",
        "\n",
        "    avg_loss = (t_mlm_loss + t_sop_loss) / len(train_loader)\n",
        "    print(f\"Train ===> Epoch {epoch+1} Loss: {avg_loss}\")\n",
        "    train_loss.append(avg_loss)\n",
        "    checkpoint_path = f\"checkpoint/ckpt{epoch}.pt\"\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "    # í‰ê°€ ê³¼ì •(NSP ì •í™•ë„ë§Œ í™•ì¸)\n",
        "    v_acc = 0.0\n",
        "    v_loss = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, labels, mask) in enumerate(val_loader):\n",
        "            inp = inp.long().to(device)\n",
        "            labels = labels.to(device)\n",
        "            mask = mask.to(device)\n",
        "            mlm_logits, sop_logits = model(inp, mask)\n",
        "\n",
        "            # SOP ì •í™•ë„ ì—°ì‚°\n",
        "            acc = (sop_logits.argmax(dim=1) == labels).sum() / len(labels)\n",
        "            v_acc += acc\n",
        "\n",
        "        avg_acc = v_acc / len(val_loader)\n",
        "        print(f\"Val ===> Epoch {epoch+1}, Val_SOP_Accuracy: {avg_acc}\")\n",
        "        test_acc.append(avg_acc.cpu().numpy())\n",
        "\n",
        "    print('-'*30)\n",
        "torch.save(model, \"model1.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYyJ_VG0xsFE",
        "outputId": "bdfa75d7-dbd9-4042-9229-aabb5790acbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Batch: 200, MLM Loss: 7.252193458080292, SOP Loss:0.6714782665669918\n",
            "Epoch: 0, Batch: 400, MLM Loss: 6.885182340145111, SOP Loss:0.419650372969918\n",
            "Epoch: 0, Batch: 600, MLM Loss: 6.716233363946279, SOP Loss:0.3068597737917056\n",
            "Epoch: 0, Batch: 800, MLM Loss: 6.620505163669586, SOP Loss:0.24641353070852345\n",
            "Epoch: 0, Batch: 1000, MLM Loss: 6.559554091930389, SOP Loss:0.20783814616780727\n",
            "Epoch: 0, Batch: 1200, MLM Loss: 6.511069982846578, SOP Loss:0.18100157124844068\n",
            "Train ===> Epoch 1 Loss: 6.6639475409997635\n",
            "Val ===> Epoch 1, Val_SOP_Accuracy: 0.99651700258255\n",
            "------------------------------\n",
            "Epoch: 1, Batch: 200, MLM Loss: 6.227266519069672, SOP Loss:0.046208606362342836\n",
            "Epoch: 1, Batch: 400, MLM Loss: 6.213429446220398, SOP Loss:0.042574848461372314\n",
            "Epoch: 1, Batch: 600, MLM Loss: 6.190332304636637, SOP Loss:0.04284097826671011\n",
            "Epoch: 1, Batch: 800, MLM Loss: 6.17110288143158, SOP Loss:0.04103899515830563\n",
            "Epoch: 1, Batch: 1000, MLM Loss: 6.152517322063446, SOP Loss:0.04038077578053344\n",
            "Epoch: 1, Batch: 1200, MLM Loss: 6.133127236366272, SOP Loss:0.03924406772159273\n",
            "Train ===> Epoch 2 Loss: 6.1644433720034995\n",
            "Val ===> Epoch 2, Val_SOP_Accuracy: 0.9974845051765442\n",
            "------------------------------\n",
            "Epoch: 2, Batch: 200, MLM Loss: 5.964857568740845, SOP Loss:0.03197353695752099\n",
            "Epoch: 2, Batch: 400, MLM Loss: 5.945886467695236, SOP Loss:0.03140571404423099\n",
            "Epoch: 2, Batch: 600, MLM Loss: 5.925715165932973, SOP Loss:0.031089797024615107\n",
            "Epoch: 2, Batch: 800, MLM Loss: 5.90876651763916, SOP Loss:0.03077739385436871\n",
            "Epoch: 2, Batch: 1000, MLM Loss: 5.891813533782959, SOP Loss:0.030784315905417317\n",
            "Epoch: 2, Batch: 1200, MLM Loss: 5.873669328689576, SOP Loss:0.030209321935253684\n",
            "Train ===> Epoch 3 Loss: 5.896049457240603\n",
            "Val ===> Epoch 3, Val_SOP_Accuracy: 0.9985003471374512\n",
            "------------------------------\n",
            "Epoch: 3, Batch: 200, MLM Loss: 5.7463460898399354, SOP Loss:0.021353226330247707\n",
            "Epoch: 3, Batch: 400, MLM Loss: 5.71656561255455, SOP Loss:0.023163938314537516\n",
            "Epoch: 3, Batch: 600, MLM Loss: 5.702371315161387, SOP Loss:0.022596737721857305\n",
            "Epoch: 3, Batch: 800, MLM Loss: 5.6881388401985165, SOP Loss:0.021754231112718115\n",
            "Epoch: 3, Batch: 1000, MLM Loss: 5.67338221502304, SOP Loss:0.021283695224905388\n",
            "Epoch: 3, Batch: 1200, MLM Loss: 5.660845367511113, SOP Loss:0.021157127903037084\n",
            "Train ===> Epoch 4 Loss: 5.6781878394637335\n",
            "Val ===> Epoch 4, Val_SOP_Accuracy: 0.9989840984344482\n",
            "------------------------------\n",
            "Epoch: 4, Batch: 200, MLM Loss: 5.554738049507141, SOP Loss:0.01669447780412156\n",
            "Epoch: 4, Batch: 400, MLM Loss: 5.542493425607681, SOP Loss:0.016431234493502415\n",
            "Epoch: 4, Batch: 600, MLM Loss: 5.523535227775573, SOP Loss:0.01612428211403312\n",
            "Epoch: 4, Batch: 800, MLM Loss: 5.512579069733619, SOP Loss:0.015899944998673164\n",
            "Epoch: 4, Batch: 1000, MLM Loss: 5.501937847614288, SOP Loss:0.01596314066031482\n",
            "Epoch: 4, Batch: 1200, MLM Loss: 5.488972872495651, SOP Loss:0.01556976798325195\n",
            "Train ===> Epoch 5 Loss: 5.500978057457274\n",
            "Val ===> Epoch 5, Val_SOP_Accuracy: 0.9989357590675354\n",
            "------------------------------\n",
            "Epoch: 5, Batch: 200, MLM Loss: 5.40794988155365, SOP Loss:0.013537720790773165\n",
            "Epoch: 5, Batch: 400, MLM Loss: 5.405414870977402, SOP Loss:0.013789128848147812\n",
            "Epoch: 5, Batch: 600, MLM Loss: 5.390132208665212, SOP Loss:0.012573359430098208\n",
            "Epoch: 5, Batch: 800, MLM Loss: 5.386795587539673, SOP Loss:0.012165696012634725\n",
            "Epoch: 5, Batch: 1000, MLM Loss: 5.373730247020721, SOP Loss:0.012154793368157698\n",
            "Epoch: 5, Batch: 1200, MLM Loss: 5.367004247903824, SOP Loss:0.01207649287202609\n",
            "Train ===> Epoch 6 Loss: 5.372244422662923\n",
            "Val ===> Epoch 6, Val_SOP_Accuracy: 0.9989840984344482\n",
            "------------------------------\n",
            "Epoch: 6, Batch: 200, MLM Loss: 5.2926404929161075, SOP Loss:0.010474863711860962\n",
            "Epoch: 6, Batch: 400, MLM Loss: 5.267450034618378, SOP Loss:0.011220979440113297\n",
            "Epoch: 6, Batch: 600, MLM Loss: 5.25629896402359, SOP Loss:0.010596590529215367\n",
            "Epoch: 6, Batch: 800, MLM Loss: 5.253202741742134, SOP Loss:0.010797182100250212\n",
            "Epoch: 6, Batch: 1000, MLM Loss: 5.240073510169983, SOP Loss:0.010822825752955395\n",
            "Epoch: 6, Batch: 1200, MLM Loss: 5.233695851961771, SOP Loss:0.01092996192171995\n",
            "Train ===> Epoch 7 Loss: 5.236292144519932\n",
            "Val ===> Epoch 7, Val_SOP_Accuracy: 0.9989840984344482\n",
            "------------------------------\n",
            "Epoch: 7, Batch: 200, MLM Loss: 5.145768799781799, SOP Loss:0.014898120114812627\n",
            "Epoch: 7, Batch: 400, MLM Loss: 5.1301833546161655, SOP Loss:0.012031370219410746\n",
            "Epoch: 7, Batch: 600, MLM Loss: 5.122385381062825, SOP Loss:0.011177716758053673\n",
            "Epoch: 7, Batch: 800, MLM Loss: 5.1114273124933245, SOP Loss:0.01061617420771654\n",
            "Epoch: 7, Batch: 1000, MLM Loss: 5.100940482616425, SOP Loss:0.010444419834529982\n",
            "Epoch: 7, Batch: 1200, MLM Loss: 5.090348179737727, SOP Loss:0.01038874994116971\n",
            "Train ===> Epoch 8 Loss: 5.096011299489168\n",
            "Val ===> Epoch 8, Val_SOP_Accuracy: 0.9990808367729187\n",
            "------------------------------\n",
            "Epoch: 8, Batch: 200, MLM Loss: 5.011676342487335, SOP Loss:0.008844857984659029\n",
            "Epoch: 8, Batch: 400, MLM Loss: 5.012302671670914, SOP Loss:0.009409205123884021\n",
            "Epoch: 8, Batch: 600, MLM Loss: 5.002108178138733, SOP Loss:0.009103953238663962\n",
            "Epoch: 8, Batch: 800, MLM Loss: 5.001057924032211, SOP Loss:0.008876495515833084\n",
            "Epoch: 8, Batch: 1000, MLM Loss: 4.990055821418762, SOP Loss:0.00884413229036727\n",
            "Epoch: 8, Batch: 1200, MLM Loss: 4.984263734817505, SOP Loss:0.008736046739795712\n",
            "Train ===> Epoch 9 Loss: 4.990527712323758\n",
            "Val ===> Epoch 9, Val_SOP_Accuracy: 0.9990808367729187\n",
            "------------------------------\n",
            "Epoch: 9, Batch: 200, MLM Loss: 4.9129532289505, SOP Loss:0.009521137990814168\n",
            "Epoch: 9, Batch: 400, MLM Loss: 4.90893091917038, SOP Loss:0.009526658023642085\n",
            "Epoch: 9, Batch: 600, MLM Loss: 4.897058697541555, SOP Loss:0.008702937205162015\n",
            "Epoch: 9, Batch: 800, MLM Loss: 4.894850785136223, SOP Loss:0.008822038018752209\n",
            "Epoch: 9, Batch: 1000, MLM Loss: 4.88676178741455, SOP Loss:0.009040598000501632\n",
            "Epoch: 9, Batch: 1200, MLM Loss: 4.880467409292857, SOP Loss:0.008326760663561193\n",
            "Train ===> Epoch 10 Loss: 4.886362286743656\n",
            "Val ===> Epoch 10, Val_SOP_Accuracy: 0.9991292357444763\n",
            "------------------------------\n",
            "Epoch: 10, Batch: 200, MLM Loss: 4.806403169631958, SOP Loss:0.008142856537233456\n",
            "Epoch: 10, Batch: 400, MLM Loss: 4.802851432561875, SOP Loss:0.00807180793788575\n",
            "Epoch: 10, Batch: 600, MLM Loss: 4.789405997594198, SOP Loss:0.0077723967250737285\n",
            "Epoch: 10, Batch: 800, MLM Loss: 4.790247708559036, SOP Loss:0.008263266688445584\n",
            "Epoch: 10, Batch: 1000, MLM Loss: 4.783085967540741, SOP Loss:0.008266779532568762\n",
            "Epoch: 10, Batch: 1200, MLM Loss: 4.7752465740839645, SOP Loss:0.00796569529181094\n",
            "Train ===> Epoch 11 Loss: 4.779569315810169\n",
            "Val ===> Epoch 11, Val_SOP_Accuracy: 0.9991292357444763\n",
            "------------------------------\n",
            "Epoch: 11, Batch: 200, MLM Loss: 4.6961453557014465, SOP Loss:0.008365923559758813\n",
            "Epoch: 11, Batch: 400, MLM Loss: 4.700183659791946, SOP Loss:0.008215802431332122\n",
            "Epoch: 11, Batch: 600, MLM Loss: 4.698183658917745, SOP Loss:0.00922597122376222\n",
            "Epoch: 11, Batch: 800, MLM Loss: 4.6966122841835025, SOP Loss:0.008450113966264326\n",
            "Epoch: 11, Batch: 1000, MLM Loss: 4.6893063406944275, SOP Loss:0.008471217287951732\n",
            "Epoch: 11, Batch: 1200, MLM Loss: 4.680809551080068, SOP Loss:0.008414136535514748\n",
            "Train ===> Epoch 12 Loss: 4.686646363605326\n",
            "Val ===> Epoch 12, Val_SOP_Accuracy: 0.9991775751113892\n",
            "------------------------------\n",
            "Epoch: 12, Batch: 200, MLM Loss: 4.60105235338211, SOP Loss:0.008876460411338485\n",
            "Epoch: 12, Batch: 400, MLM Loss: 4.604260816574096, SOP Loss:0.009028693849650153\n",
            "Epoch: 12, Batch: 600, MLM Loss: 4.6002089913686115, SOP Loss:0.00885711974966398\n",
            "Epoch: 12, Batch: 800, MLM Loss: 4.594967293143273, SOP Loss:0.008513934592210717\n",
            "Epoch: 12, Batch: 1000, MLM Loss: 4.587788785934448, SOP Loss:0.007980566819300292\n",
            "Epoch: 12, Batch: 1200, MLM Loss: 4.586433953444163, SOP Loss:0.007629843404368633\n",
            "Train ===> Epoch 13 Loss: 4.59562503151265\n",
            "Val ===> Epoch 13, Val_SOP_Accuracy: 0.9992743730545044\n",
            "------------------------------\n",
            "Epoch: 13, Batch: 200, MLM Loss: 4.547409174442291, SOP Loss:0.006553374612994958\n",
            "Epoch: 13, Batch: 400, MLM Loss: 4.543010904788971, SOP Loss:0.00619563861800998\n",
            "Epoch: 13, Batch: 600, MLM Loss: 4.527070267597835, SOP Loss:0.006693769207801476\n",
            "Epoch: 13, Batch: 800, MLM Loss: 4.52555334419012, SOP Loss:0.007290769281444227\n",
            "Epoch: 13, Batch: 1000, MLM Loss: 4.519082590579987, SOP Loss:0.007844055930458126\n",
            "Epoch: 13, Batch: 1200, MLM Loss: 4.512797975937525, SOP Loss:0.007817864616360264\n",
            "Train ===> Epoch 14 Loss: 4.518463131959348\n",
            "Val ===> Epoch 14, Val_SOP_Accuracy: 0.9991775751113892\n",
            "------------------------------\n",
            "Epoch: 14, Batch: 200, MLM Loss: 4.4582970535755155, SOP Loss:0.005331999960981193\n",
            "Epoch: 14, Batch: 400, MLM Loss: 4.466139370799064, SOP Loss:0.006053634701675037\n",
            "Epoch: 14, Batch: 600, MLM Loss: 4.458251439730327, SOP Loss:0.005968318222536861\n",
            "Epoch: 14, Batch: 800, MLM Loss: 4.4476993089914325, SOP Loss:0.006587914505689696\n",
            "Epoch: 14, Batch: 1000, MLM Loss: 4.436587573051453, SOP Loss:0.006936432457383489\n",
            "Epoch: 14, Batch: 1200, MLM Loss: 4.4345243966579435, SOP Loss:0.006983913406123368\n",
            "Train ===> Epoch 15 Loss: 4.4394145496731765\n",
            "Val ===> Epoch 15, Val_SOP_Accuracy: 0.9992259740829468\n",
            "------------------------------\n",
            "Epoch: 15, Batch: 200, MLM Loss: 4.379668120145798, SOP Loss:0.006388458885558066\n",
            "Epoch: 15, Batch: 400, MLM Loss: 4.377796823382377, SOP Loss:0.0066897223946216396\n",
            "Epoch: 15, Batch: 600, MLM Loss: 4.3706442133585615, SOP Loss:0.006307927211213003\n",
            "Epoch: 15, Batch: 800, MLM Loss: 4.370806183815002, SOP Loss:0.006430375434574671\n",
            "Epoch: 15, Batch: 1000, MLM Loss: 4.369442171096802, SOP Loss:0.006414488230358984\n",
            "Epoch: 15, Batch: 1200, MLM Loss: 4.365004235506058, SOP Loss:0.006804454333984419\n",
            "Train ===> Epoch 16 Loss: 4.370277866489928\n",
            "Val ===> Epoch 16, Val_SOP_Accuracy: 0.9994194507598877\n",
            "------------------------------\n",
            "Epoch: 16, Batch: 200, MLM Loss: 4.314183415174484, SOP Loss:0.007957817501446697\n",
            "Epoch: 16, Batch: 400, MLM Loss: 4.315329890847206, SOP Loss:0.007547978565598896\n",
            "Epoch: 16, Batch: 600, MLM Loss: 4.31652800599734, SOP Loss:0.0071995601656332535\n",
            "Epoch: 16, Batch: 800, MLM Loss: 4.30567397505045, SOP Loss:0.007173231371161819\n",
            "Epoch: 16, Batch: 1000, MLM Loss: 4.294418661117554, SOP Loss:0.007400794734800002\n",
            "Epoch: 16, Batch: 1200, MLM Loss: 4.290178634723028, SOP Loss:0.007116321759579781\n",
            "Train ===> Epoch 17 Loss: 4.295495165708749\n",
            "Val ===> Epoch 17, Val_SOP_Accuracy: 0.9993227124214172\n",
            "------------------------------\n",
            "Epoch: 17, Batch: 200, MLM Loss: 4.2599760437011716, SOP Loss:0.006445903769199504\n",
            "Epoch: 17, Batch: 400, MLM Loss: 4.255235893130302, SOP Loss:0.006468345699031488\n",
            "Epoch: 17, Batch: 600, MLM Loss: 4.25468225757281, SOP Loss:0.00628746791461405\n",
            "Epoch: 17, Batch: 800, MLM Loss: 4.241579812765122, SOP Loss:0.006134045391063409\n",
            "Epoch: 17, Batch: 1000, MLM Loss: 4.236035156726837, SOP Loss:0.006381777595321182\n",
            "Epoch: 17, Batch: 1200, MLM Loss: 4.227774660587311, SOP Loss:0.006634556316239468\n",
            "Train ===> Epoch 18 Loss: 4.230955847993837\n",
            "Val ===> Epoch 18, Val_SOP_Accuracy: 0.9994194507598877\n",
            "------------------------------\n",
            "Epoch: 18, Batch: 200, MLM Loss: 4.169450122117996, SOP Loss:0.008463046905671946\n",
            "Epoch: 18, Batch: 400, MLM Loss: 4.169575633406639, SOP Loss:0.00788641353523417\n",
            "Epoch: 18, Batch: 600, MLM Loss: 4.163013668457667, SOP Loss:0.007885692285951033\n",
            "Epoch: 18, Batch: 800, MLM Loss: 4.160856872797012, SOP Loss:0.0073143053714738926\n",
            "Epoch: 18, Batch: 1000, MLM Loss: 4.1589273438453676, SOP Loss:0.006962822441244498\n",
            "Epoch: 18, Batch: 1200, MLM Loss: 4.158374743262927, SOP Loss:0.007016476412100019\n",
            "Train ===> Epoch 19 Loss: 4.162793704487227\n",
            "Val ===> Epoch 19, Val_SOP_Accuracy: 0.9993711113929749\n",
            "------------------------------\n",
            "Epoch: 19, Batch: 200, MLM Loss: 4.127173649072647, SOP Loss:0.006031762881175382\n",
            "Epoch: 19, Batch: 400, MLM Loss: 4.119759411811828, SOP Loss:0.006999601268034894\n",
            "Epoch: 19, Batch: 600, MLM Loss: 4.120606014728546, SOP Loss:0.007195183958707882\n",
            "Epoch: 19, Batch: 800, MLM Loss: 4.11556386500597, SOP Loss:0.006917181271055597\n",
            "Epoch: 19, Batch: 1000, MLM Loss: 4.109357976436615, SOP Loss:0.0068865083628770665\n",
            "Epoch: 19, Batch: 1200, MLM Loss: 4.102945608695348, SOP Loss:0.007065492533571766\n",
            "Train ===> Epoch 20 Loss: 4.107212714540443\n",
            "Val ===> Epoch 20, Val_SOP_Accuracy: 0.9994678497314453\n",
            "------------------------------\n",
            "Epoch: 20, Batch: 200, MLM Loss: 4.039913368225098, SOP Loss:0.006094265448482474\n",
            "Epoch: 20, Batch: 400, MLM Loss: 4.046339935064315, SOP Loss:0.005892274549078138\n",
            "Epoch: 20, Batch: 600, MLM Loss: 4.043464479446411, SOP Loss:0.005779012457630112\n",
            "Epoch: 20, Batch: 800, MLM Loss: 4.04235439568758, SOP Loss:0.006491323403797651\n",
            "Epoch: 20, Batch: 1000, MLM Loss: 4.039278623342514, SOP Loss:0.007065564364027523\n",
            "Epoch: 20, Batch: 1200, MLM Loss: 4.038414477705955, SOP Loss:0.00669293246654585\n",
            "Train ===> Epoch 21 Loss: 4.042110830667218\n",
            "Val ===> Epoch 21, Val_SOP_Accuracy: 0.9994194507598877\n",
            "------------------------------\n",
            "Epoch: 21, Batch: 200, MLM Loss: 4.012314640283584, SOP Loss:0.0069340615731562135\n",
            "Epoch: 21, Batch: 400, MLM Loss: 4.020449360609055, SOP Loss:0.006394858006769937\n",
            "Epoch: 21, Batch: 600, MLM Loss: 4.016547044118245, SOP Loss:0.0064765357586899575\n",
            "Epoch: 21, Batch: 800, MLM Loss: 4.011891328692436, SOP Loss:0.0065610561378798594\n",
            "Epoch: 21, Batch: 1000, MLM Loss: 4.002759619235992, SOP Loss:0.006977896324613539\n",
            "Epoch: 21, Batch: 1200, MLM Loss: 3.9945228364070258, SOP Loss:0.006514790161327255\n",
            "Train ===> Epoch 22 Loss: 3.998119732453896\n",
            "Val ===> Epoch 22, Val_SOP_Accuracy: 0.9993711113929749\n",
            "------------------------------\n",
            "Epoch: 22, Batch: 200, MLM Loss: 3.965270549058914, SOP Loss:0.006638260125473607\n",
            "Epoch: 22, Batch: 400, MLM Loss: 3.950360538959503, SOP Loss:0.006590959989262046\n",
            "Epoch: 22, Batch: 600, MLM Loss: 3.9503716095288595, SOP Loss:0.0061269467621968944\n",
            "Epoch: 22, Batch: 800, MLM Loss: 3.9454730945825576, SOP Loss:0.006050932823400217\n",
            "Epoch: 22, Batch: 1000, MLM Loss: 3.9433874311447146, SOP Loss:0.006111466992646456\n",
            "Epoch: 22, Batch: 1200, MLM Loss: 3.940998373826345, SOP Loss:0.0060538584968405\n",
            "Train ===> Epoch 23 Loss: 3.944997625910518\n",
            "Val ===> Epoch 23, Val_SOP_Accuracy: 0.9993711113929749\n",
            "------------------------------\n",
            "Epoch: 23, Batch: 200, MLM Loss: 3.89814205288887, SOP Loss:0.006095478442875901\n",
            "Epoch: 23, Batch: 400, MLM Loss: 3.894756451845169, SOP Loss:0.004934787090496684\n",
            "Epoch: 23, Batch: 600, MLM Loss: 3.891285847822825, SOP Loss:0.0051083393214867105\n",
            "Epoch: 23, Batch: 800, MLM Loss: 3.890697762966156, SOP Loss:0.005940905261786611\n",
            "Epoch: 23, Batch: 1000, MLM Loss: 3.8884782648086547, SOP Loss:0.006032653019115969\n",
            "Epoch: 23, Batch: 1200, MLM Loss: 3.88571702837944, SOP Loss:0.006162832032769075\n",
            "Train ===> Epoch 24 Loss: 3.8887861078325527\n",
            "Val ===> Epoch 24, Val_SOP_Accuracy: 0.9993711113929749\n",
            "------------------------------\n",
            "Epoch: 24, Batch: 200, MLM Loss: 3.8332322001457215, SOP Loss:0.0064169584205956195\n",
            "Epoch: 24, Batch: 400, MLM Loss: 3.843003230690956, SOP Loss:0.005913746277383325\n",
            "Epoch: 24, Batch: 600, MLM Loss: 3.8445256567001342, SOP Loss:0.0063197210299767905\n",
            "Epoch: 24, Batch: 800, MLM Loss: 3.842465554475784, SOP Loss:0.006003911175803296\n",
            "Epoch: 24, Batch: 1000, MLM Loss: 3.840808263540268, SOP Loss:0.005669066534879676\n",
            "Epoch: 24, Batch: 1200, MLM Loss: 3.8345606670777004, SOP Loss:0.00591422497908449\n",
            "Train ===> Epoch 25 Loss: 3.8393734974556715\n",
            "Val ===> Epoch 25, Val_SOP_Accuracy: 0.9993711113929749\n",
            "------------------------------\n",
            "Epoch: 25, Batch: 200, MLM Loss: 3.8092810237407684, SOP Loss:0.006471984401941882\n",
            "Epoch: 25, Batch: 400, MLM Loss: 3.79857742190361, SOP Loss:0.004894322863729031\n",
            "Epoch: 25, Batch: 600, MLM Loss: 3.7932359810670215, SOP Loss:0.005357189461816839\n",
            "Epoch: 25, Batch: 800, MLM Loss: 3.7910389718413353, SOP Loss:0.006057574344540626\n",
            "Epoch: 25, Batch: 1000, MLM Loss: 3.789107637643814, SOP Loss:0.0058120895817264685\n",
            "Epoch: 25, Batch: 1200, MLM Loss: 3.7850875993569693, SOP Loss:0.005847405687175827\n",
            "Train ===> Epoch 26 Loss: 3.7898491955051656\n",
            "Val ===> Epoch 26, Val_SOP_Accuracy: 0.9994194507598877\n",
            "------------------------------\n",
            "Epoch: 26, Batch: 200, MLM Loss: 3.7578087663650512, SOP Loss:0.008803664311562898\n",
            "Epoch: 26, Batch: 400, MLM Loss: 3.7525155609846115, SOP Loss:0.0073115008308013785\n",
            "Epoch: 26, Batch: 600, MLM Loss: 3.7474746072292326, SOP Loss:0.00686766471194763\n",
            "Epoch: 26, Batch: 800, MLM Loss: 3.749894596338272, SOP Loss:0.006052126988879536\n",
            "Epoch: 26, Batch: 1000, MLM Loss: 3.7469653589725493, SOP Loss:0.005531477902761253\n",
            "Epoch: 26, Batch: 1200, MLM Loss: 3.7431587036450704, SOP Loss:0.005833768336669891\n",
            "Train ===> Epoch 27 Loss: 3.7478522514374073\n",
            "Val ===> Epoch 27, Val_SOP_Accuracy: 0.9994678497314453\n",
            "------------------------------\n",
            "Epoch: 27, Batch: 200, MLM Loss: 3.7110309588909147, SOP Loss:0.007064625145867467\n",
            "Epoch: 27, Batch: 400, MLM Loss: 3.72700787961483, SOP Loss:0.006780209923090297\n",
            "Epoch: 27, Batch: 600, MLM Loss: 3.7253229363759357, SOP Loss:0.006834756100700664\n",
            "Epoch: 27, Batch: 800, MLM Loss: 3.723896183669567, SOP Loss:0.0060298742233135276\n",
            "Epoch: 27, Batch: 1000, MLM Loss: 3.717689391851425, SOP Loss:0.005832369045499945\n",
            "Epoch: 27, Batch: 1200, MLM Loss: 3.7136761053403218, SOP Loss:0.005609385373051433\n",
            "Train ===> Epoch 28 Loss: 3.7169820770122377\n",
            "Val ===> Epoch 28, Val_SOP_Accuracy: 0.9995162487030029\n",
            "------------------------------\n",
            "Epoch: 28, Batch: 200, MLM Loss: 3.645462282896042, SOP Loss:0.0068268967444601\n",
            "Epoch: 28, Batch: 400, MLM Loss: 3.6740059876441955, SOP Loss:0.005453794262248266\n",
            "Epoch: 28, Batch: 600, MLM Loss: 3.6745336643854776, SOP Loss:0.005859104684944516\n",
            "Epoch: 28, Batch: 800, MLM Loss: 3.6731669336557387, SOP Loss:0.005543847794460817\n",
            "Epoch: 28, Batch: 1000, MLM Loss: 3.6731116771698, SOP Loss:0.00534456359865726\n",
            "Epoch: 28, Batch: 1200, MLM Loss: 3.670102125207583, SOP Loss:0.005357354814132123\n",
            "Train ===> Epoch 29 Loss: 3.6733770836548385\n",
            "Val ===> Epoch 29, Val_SOP_Accuracy: 0.9994678497314453\n",
            "------------------------------\n",
            "Epoch: 29, Batch: 200, MLM Loss: 3.6417850255966187, SOP Loss:0.005329263359308243\n",
            "Epoch: 29, Batch: 400, MLM Loss: 3.646102619767189, SOP Loss:0.005113175040460192\n",
            "Epoch: 29, Batch: 600, MLM Loss: 3.6401549804210664, SOP Loss:0.005272425697658036\n",
            "Epoch: 29, Batch: 800, MLM Loss: 3.6375078892707826, SOP Loss:0.005630323367686287\n",
            "Epoch: 29, Batch: 1000, MLM Loss: 3.630168185710907, SOP Loss:0.006002758740643912\n",
            "Epoch: 29, Batch: 1200, MLM Loss: 3.627611306508382, SOP Loss:0.005663482099450145\n",
            "Train ===> Epoch 30 Loss: 3.6326717514828424\n",
            "Val ===> Epoch 30, Val_SOP_Accuracy: 0.9995162487030029\n",
            "------------------------------\n",
            "Epoch: 30, Batch: 200, MLM Loss: 3.6121257448196413, SOP Loss:0.006321344251191476\n",
            "Epoch: 30, Batch: 400, MLM Loss: 3.613931865096092, SOP Loss:0.005960064353730559\n",
            "Epoch: 30, Batch: 600, MLM Loss: 3.6076908322175343, SOP Loss:0.005969662169336516\n",
            "Epoch: 30, Batch: 800, MLM Loss: 3.601264171898365, SOP Loss:0.005107355527952677\n",
            "Epoch: 30, Batch: 1000, MLM Loss: 3.597402809858322, SOP Loss:0.005440318236738676\n",
            "Epoch: 30, Batch: 1200, MLM Loss: 3.5927383301655453, SOP Loss:0.005618054957582596\n",
            "Train ===> Epoch 31 Loss: 3.5976506150453305\n",
            "Val ===> Epoch 31, Val_SOP_Accuracy: 0.9993711113929749\n",
            "------------------------------\n",
            "Epoch: 31, Batch: 200, MLM Loss: 3.594906506538391, SOP Loss:0.0058151357630049465\n",
            "Epoch: 31, Batch: 400, MLM Loss: 3.572741775512695, SOP Loss:0.007573066870045295\n",
            "Epoch: 31, Batch: 600, MLM Loss: 3.572753006219864, SOP Loss:0.006962668834130454\n",
            "Epoch: 31, Batch: 800, MLM Loss: 3.5712835863232613, SOP Loss:0.006431259695555127\n",
            "Epoch: 31, Batch: 1000, MLM Loss: 3.570508653640747, SOP Loss:0.0061202053419328875\n",
            "Epoch: 31, Batch: 1200, MLM Loss: 3.5652438525358834, SOP Loss:0.005879135207766619\n",
            "Train ===> Epoch 32 Loss: 3.5702809279376893\n",
            "Val ===> Epoch 32, Val_SOP_Accuracy: 0.9994678497314453\n",
            "------------------------------\n",
            "Epoch: 32, Batch: 200, MLM Loss: 3.527052271366119, SOP Loss:0.006361989890319819\n",
            "Epoch: 32, Batch: 400, MLM Loss: 3.536739559173584, SOP Loss:0.0063876361338225255\n",
            "Epoch: 32, Batch: 600, MLM Loss: 3.5379954644044243, SOP Loss:0.005394963702844204\n",
            "Epoch: 32, Batch: 800, MLM Loss: 3.5313536483049393, SOP Loss:0.005483067551658678\n",
            "Epoch: 32, Batch: 1000, MLM Loss: 3.5292921950817107, SOP Loss:0.005447164802892076\n",
            "Epoch: 32, Batch: 1200, MLM Loss: 3.5254851353168486, SOP Loss:0.005221531170330612\n",
            "Train ===> Epoch 33 Loss: 3.5293838699586737\n",
            "Val ===> Epoch 33, Val_SOP_Accuracy: 0.9995162487030029\n",
            "------------------------------\n",
            "Epoch: 33, Batch: 200, MLM Loss: 3.5143305826187134, SOP Loss:0.004207434800482588\n",
            "Epoch: 33, Batch: 400, MLM Loss: 3.495386391878128, SOP Loss:0.004354243554589629\n",
            "Epoch: 33, Batch: 600, MLM Loss: 3.4966725730895996, SOP Loss:0.0048773375279415635\n",
            "Epoch: 33, Batch: 800, MLM Loss: 3.4962799561023714, SOP Loss:0.004759412945977601\n",
            "Epoch: 33, Batch: 1000, MLM Loss: 3.4959726123809816, SOP Loss:0.005304721642576624\n",
            "Epoch: 33, Batch: 1200, MLM Loss: 3.492111353079478, SOP Loss:0.00518008560424884\n",
            "Train ===> Epoch 34 Loss: 3.4972931977491877\n",
            "Val ===> Epoch 34, Val_SOP_Accuracy: 0.9994678497314453\n",
            "------------------------------\n",
            "Epoch: 34, Batch: 200, MLM Loss: 3.466826169490814, SOP Loss:0.007661797801920329\n",
            "Epoch: 34, Batch: 400, MLM Loss: 3.4770048630237578, SOP Loss:0.006462739306443837\n",
            "Epoch: 34, Batch: 600, MLM Loss: 3.4688842324415843, SOP Loss:0.005475087448685372\n",
            "Epoch: 34, Batch: 800, MLM Loss: 3.468021064698696, SOP Loss:0.00568990112584288\n",
            "Epoch: 34, Batch: 1000, MLM Loss: 3.470719093322754, SOP Loss:0.0056850513288300136\n",
            "Epoch: 34, Batch: 1200, MLM Loss: 3.4696237816413245, SOP Loss:0.005364850443547766\n",
            "Train ===> Epoch 35 Loss: 3.4742644384228467\n",
            "Val ===> Epoch 35, Val_SOP_Accuracy: 0.9995162487030029\n",
            "------------------------------\n",
            "Epoch: 35, Batch: 200, MLM Loss: 3.438260952234268, SOP Loss:0.0064103617401269734\n",
            "Epoch: 35, Batch: 400, MLM Loss: 3.446388365030289, SOP Loss:0.0069453083935877655\n",
            "Epoch: 35, Batch: 600, MLM Loss: 3.4503644545873007, SOP Loss:0.006470789281035346\n",
            "Epoch: 35, Batch: 800, MLM Loss: 3.4450366789102556, SOP Loss:0.005999094443141075\n",
            "Epoch: 35, Batch: 1000, MLM Loss: 3.441610848426819, SOP Loss:0.005807085201769951\n",
            "Epoch: 35, Batch: 1200, MLM Loss: 3.4418817885716755, SOP Loss:0.005390581758444265\n",
            "Train ===> Epoch 36 Loss: 3.447072576132724\n",
            "Val ===> Epoch 36, Val_SOP_Accuracy: 0.9994194507598877\n",
            "------------------------------\n",
            "Epoch: 36, Batch: 200, MLM Loss: 3.413238364458084, SOP Loss:0.0060698730555304795\n",
            "Epoch: 36, Batch: 400, MLM Loss: 3.417059096097946, SOP Loss:0.005903053051442839\n",
            "Epoch: 36, Batch: 600, MLM Loss: 3.4190138550599416, SOP Loss:0.005492288709756394\n",
            "Epoch: 36, Batch: 800, MLM Loss: 3.4187163105607032, SOP Loss:0.005411929714309736\n",
            "Epoch: 36, Batch: 1000, MLM Loss: 3.412196582555771, SOP Loss:0.005311424112464011\n",
            "Epoch: 36, Batch: 1200, MLM Loss: 3.4131335008144377, SOP Loss:0.005013746559585949\n",
            "Train ===> Epoch 37 Loss: 3.416834696572752\n",
            "Val ===> Epoch 37, Val_SOP_Accuracy: 0.9995162487030029\n",
            "------------------------------\n",
            "Epoch: 37, Batch: 200, MLM Loss: 3.4024046540260313, SOP Loss:0.0036660060931899352\n",
            "Epoch: 37, Batch: 400, MLM Loss: 3.394172033071518, SOP Loss:0.0055340467039604845\n",
            "Epoch: 37, Batch: 600, MLM Loss: 3.3874516836802164, SOP Loss:0.005088499604050109\n",
            "Epoch: 37, Batch: 800, MLM Loss: 3.3876432093977926, SOP Loss:0.004800939104070494\n",
            "Epoch: 37, Batch: 1000, MLM Loss: 3.389221595287323, SOP Loss:0.00493824875838618\n",
            "Epoch: 37, Batch: 1200, MLM Loss: 3.385644739071528, SOP Loss:0.0053847479986507095\n",
            "Train ===> Epoch 38 Loss: 3.3909924064089987\n",
            "Val ===> Epoch 38, Val_SOP_Accuracy: 0.9994194507598877\n",
            "------------------------------\n",
            "Epoch: 38, Batch: 200, MLM Loss: 3.352164397239685, SOP Loss:0.0056634003053477495\n",
            "Epoch: 38, Batch: 400, MLM Loss: 3.350365645289421, SOP Loss:0.00602648961761588\n",
            "Epoch: 38, Batch: 600, MLM Loss: 3.361353578567505, SOP Loss:0.005398771623088881\n",
            "Epoch: 38, Batch: 800, MLM Loss: 3.3619978818297387, SOP Loss:0.005474219316483868\n",
            "Epoch: 38, Batch: 1000, MLM Loss: 3.3635161128044126, SOP Loss:0.005181910686202173\n",
            "Epoch: 38, Batch: 1200, MLM Loss: 3.3648385363817215, SOP Loss:0.0053219346798565915\n",
            "Train ===> Epoch 39 Loss: 3.369326729409009\n",
            "Val ===> Epoch 39, Val_SOP_Accuracy: 0.9995162487030029\n",
            "------------------------------\n",
            "Epoch: 39, Batch: 200, MLM Loss: 3.331389000415802, SOP Loss:0.003463031456740282\n",
            "Epoch: 39, Batch: 400, MLM Loss: 3.327538259625435, SOP Loss:0.005025619984462537\n",
            "Epoch: 39, Batch: 600, MLM Loss: 3.324545104900996, SOP Loss:0.005128375934315651\n",
            "Epoch: 39, Batch: 800, MLM Loss: 3.323928029537201, SOP Loss:0.005279078659768856\n",
            "Epoch: 39, Batch: 1000, MLM Loss: 3.3225798609256745, SOP Loss:0.004910546743034502\n",
            "Epoch: 39, Batch: 1200, MLM Loss: 3.326532567739487, SOP Loss:0.004668178240359945\n",
            "Train ===> Epoch 40 Loss: 3.3307976821333845\n",
            "Val ===> Epoch 40, Val_SOP_Accuracy: 0.9995162487030029\n",
            "------------------------------\n",
            "Epoch: 40, Batch: 200, MLM Loss: 3.3199170458316805, SOP Loss:0.007436220935342135\n",
            "Epoch: 40, Batch: 400, MLM Loss: 3.3125369995832443, SOP Loss:0.005972856842818146\n",
            "Epoch: 40, Batch: 600, MLM Loss: 3.302756910721461, SOP Loss:0.005746628610225646\n",
            "Epoch: 40, Batch: 800, MLM Loss: 3.3057265132665634, SOP Loss:0.005450032059370642\n",
            "Epoch: 40, Batch: 1000, MLM Loss: 3.301837033987045, SOP Loss:0.005390110562380869\n",
            "Epoch: 40, Batch: 1200, MLM Loss: 3.3063977330923082, SOP Loss:0.005270689239593291\n",
            "Train ===> Epoch 41 Loss: 3.310026732487125\n",
            "Val ===> Epoch 41, Val_SOP_Accuracy: 0.9995162487030029\n",
            "------------------------------\n",
            "Epoch: 41, Batch: 200, MLM Loss: 3.30251092672348, SOP Loss:0.005130840106503456\n",
            "Epoch: 41, Batch: 400, MLM Loss: 3.301107029914856, SOP Loss:0.0037633673340315\n",
            "Epoch: 41, Batch: 600, MLM Loss: 3.2960408214728036, SOP Loss:0.004978457666256872\n",
            "Epoch: 41, Batch: 800, MLM Loss: 3.2982694920897484, SOP Loss:0.005139328303430375\n",
            "Epoch: 41, Batch: 1000, MLM Loss: 3.2934093182086945, SOP Loss:0.005120589367274078\n",
            "Epoch: 41, Batch: 1200, MLM Loss: 3.2928193215529125, SOP Loss:0.005133176747555505\n",
            "Train ===> Epoch 42 Loss: 3.296197709856076\n",
            "Val ===> Epoch 42, Val_SOP_Accuracy: 0.9994678497314453\n",
            "------------------------------\n",
            "Epoch: 42, Batch: 200, MLM Loss: 3.270493738651276, SOP Loss:0.004948429454925645\n",
            "Epoch: 42, Batch: 400, MLM Loss: 3.2605131667852403, SOP Loss:0.004807161486078257\n",
            "Epoch: 42, Batch: 600, MLM Loss: 3.2587828505039216, SOP Loss:0.004123341189285081\n",
            "Epoch: 42, Batch: 800, MLM Loss: 3.260039128959179, SOP Loss:0.004074396126479769\n",
            "Epoch: 42, Batch: 1000, MLM Loss: 3.2611436133384704, SOP Loss:0.004144624199521786\n",
            "Epoch: 42, Batch: 1200, MLM Loss: 3.2638465650876363, SOP Loss:0.004333829272733662\n",
            "Train ===> Epoch 43 Loss: 3.2670108233812787\n",
            "Val ===> Epoch 43, Val_SOP_Accuracy: 0.9995162487030029\n",
            "------------------------------\n",
            "Epoch: 43, Batch: 200, MLM Loss: 3.245562436580658, SOP Loss:0.003579315144052089\n",
            "Epoch: 43, Batch: 400, MLM Loss: 3.244683340191841, SOP Loss:0.004971644869419834\n",
            "Epoch: 43, Batch: 600, MLM Loss: 3.2480068838596345, SOP Loss:0.004596173334430204\n",
            "Epoch: 43, Batch: 800, MLM Loss: 3.244059872329235, SOP Loss:0.004452261581436687\n",
            "Epoch: 43, Batch: 1000, MLM Loss: 3.247583726167679, SOP Loss:0.004718623613960517\n",
            "Epoch: 43, Batch: 1200, MLM Loss: 3.244145536025365, SOP Loss:0.004648183015463777\n",
            "Train ===> Epoch 44 Loss: 3.2480764651157643\n",
            "Val ===> Epoch 44, Val_SOP_Accuracy: 0.9994678497314453\n",
            "------------------------------\n",
            "Epoch: 44, Batch: 200, MLM Loss: 3.2301494944095612, SOP Loss:0.00543785451362055\n",
            "Epoch: 44, Batch: 400, MLM Loss: 3.237356289029121, SOP Loss:0.005417882560777798\n",
            "Epoch: 44, Batch: 600, MLM Loss: 3.230636384487152, SOP Loss:0.004860740400120752\n",
            "Epoch: 44, Batch: 800, MLM Loss: 3.2234556394815446, SOP Loss:0.004927404955924431\n",
            "Epoch: 44, Batch: 1000, MLM Loss: 3.2206586508750914, SOP Loss:0.004619334088056348\n",
            "Epoch: 44, Batch: 1200, MLM Loss: 3.2188435067733128, SOP Loss:0.004602443977485867\n",
            "Train ===> Epoch 45 Loss: 3.225736568643453\n",
            "Val ===> Epoch 45, Val_SOP_Accuracy: 0.9994194507598877\n",
            "------------------------------\n",
            "Epoch: 45, Batch: 200, MLM Loss: 3.207500512599945, SOP Loss:0.0057586173729578145\n",
            "Epoch: 45, Batch: 400, MLM Loss: 3.1940220826864243, SOP Loss:0.005902209232372116\n",
            "Epoch: 45, Batch: 600, MLM Loss: 3.2057695412635803, SOP Loss:0.005806750021523233\n",
            "Epoch: 45, Batch: 800, MLM Loss: 3.1992795890569687, SOP Loss:0.0058065156762313564\n",
            "Epoch: 45, Batch: 1000, MLM Loss: 3.1977713408470154, SOP Loss:0.005317386498420092\n",
            "Epoch: 45, Batch: 1200, MLM Loss: 3.195036140878995, SOP Loss:0.00539424631536652\n",
            "Train ===> Epoch 46 Loss: 3.200076498125549\n",
            "Val ===> Epoch 46, Val_SOP_Accuracy: 0.9994678497314453\n",
            "------------------------------\n",
            "Epoch: 46, Batch: 200, MLM Loss: 3.179726060628891, SOP Loss:0.005587969130356214\n",
            "Epoch: 46, Batch: 400, MLM Loss: 3.188705242872238, SOP Loss:0.004995514876736706\n",
            "Epoch: 46, Batch: 600, MLM Loss: 3.192136219739914, SOP Loss:0.005262687104477664\n",
            "Epoch: 46, Batch: 800, MLM Loss: 3.1871258583664894, SOP Loss:0.005328995772051712\n",
            "Epoch: 46, Batch: 1000, MLM Loss: 3.1870081622600557, SOP Loss:0.005242790226388024\n",
            "Epoch: 46, Batch: 1200, MLM Loss: 3.1805675770839055, SOP Loss:0.005244086376023915\n",
            "Train ===> Epoch 47 Loss: 3.1854825829769142\n",
            "Val ===> Epoch 47, Val_SOP_Accuracy: 0.9994678497314453\n",
            "------------------------------\n",
            "Epoch: 47, Batch: 200, MLM Loss: 3.1596761763095857, SOP Loss:0.005662960401932651\n",
            "Epoch: 47, Batch: 400, MLM Loss: 3.1585504704713823, SOP Loss:0.00520551700168653\n",
            "Epoch: 47, Batch: 600, MLM Loss: 3.157867888609568, SOP Loss:0.005203592809327044\n",
            "Epoch: 47, Batch: 800, MLM Loss: 3.1617822298407554, SOP Loss:0.004601778325322812\n",
            "Epoch: 47, Batch: 1000, MLM Loss: 3.1563899812698364, SOP Loss:0.0048980074140417855\n",
            "Epoch: 47, Batch: 1200, MLM Loss: 3.153653166095416, SOP Loss:0.0048267528527382334\n",
            "Train ===> Epoch 48 Loss: 3.1587810358518627\n",
            "Val ===> Epoch 48, Val_SOP_Accuracy: 0.9994194507598877\n",
            "------------------------------\n",
            "Epoch: 48, Batch: 200, MLM Loss: 3.153539478778839, SOP Loss:0.005218092174327466\n",
            "Epoch: 48, Batch: 400, MLM Loss: 3.1542271107435225, SOP Loss:0.005885912205048954\n",
            "Epoch: 48, Batch: 600, MLM Loss: 3.147127493619919, SOP Loss:0.005958592013121234\n",
            "Epoch: 48, Batch: 800, MLM Loss: 3.1428618851304053, SOP Loss:0.00582598241906453\n",
            "Epoch: 48, Batch: 1000, MLM Loss: 3.1414131495952606, SOP Loss:0.0055428239193715855\n",
            "Epoch: 48, Batch: 1200, MLM Loss: 3.1416739336649577, SOP Loss:0.00526731184751649\n",
            "Train ===> Epoch 49 Loss: 3.1467121295902762\n",
            "Val ===> Epoch 49, Val_SOP_Accuracy: 0.9994678497314453\n",
            "------------------------------\n",
            "Epoch: 49, Batch: 200, MLM Loss: 3.1469301807880403, SOP Loss:0.004519249741206295\n",
            "Epoch: 49, Batch: 400, MLM Loss: 3.125353336930275, SOP Loss:0.004998715921574331\n",
            "Epoch: 49, Batch: 600, MLM Loss: 3.116445063749949, SOP Loss:0.005474884232280601\n",
            "Epoch: 49, Batch: 800, MLM Loss: 3.113577825129032, SOP Loss:0.004927071053525651\n",
            "Epoch: 49, Batch: 1000, MLM Loss: 3.115727527618408, SOP Loss:0.0045556997249514095\n",
            "Epoch: 49, Batch: 1200, MLM Loss: 3.113119224111239, SOP Loss:0.004348097062435651\n",
            "Train ===> Epoch 50 Loss: 3.1164850259814076\n",
            "Val ===> Epoch 50, Val_SOP_Accuracy: 0.9994678497314453\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT vs ALBERT\n",
        "\n",
        "| êµ¬ë¶„          | **BERT (ì‹¤ìŠµ)**                            | **ALBERT (ì‹¤ìŠµ)**                                      |\n",
        "| ----------- | ---------------------------------------- | ---------------------------------------------------- |\n",
        "| **ì„ë² ë”© êµ¬ì¡°**  | `Embedding(vocab, 128)` â†’ ë°”ë¡œ Transformer | `Embedding(vocab, 64)` â†’ `Linear(64â†’128)` projection |\n",
        "| **ë ˆì´ì–´ êµ¬ì¡°**  | `MTBlock(128)` Ã— **4ê°œ (ê°ì ë‹¤ë¥¸ ê°€ì¤‘ì¹˜)**      | `MTBlock(128)` Ã— **1ê°œë¥¼ 4ë²ˆ ê³µìœ **                       |\n",
        "| **íŒŒë¼ë¯¸í„° ê·œëª¨** | í¼                                        | ì‘ìŒ (ì„ë² ë”© ì ˆë°˜ + block ê³µìœ )                               |\n",
        "| **ì†ë„**      | ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼                                 | ë” ë¹ ë¥´ê³  ê°€ë²¼ì›€                                            |\n",
        "\n"
      ],
      "metadata": {
        "id": "8iGZJ4AIiKws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- ê°€ì¤‘ì¹˜/ë©”ëª¨ë¦¬/ì„±ëŠ¥ ë¹„êµ\n",
        "\n",
        "| í•­ëª©        | BERT          | ALBERT           |\n",
        "| --------- | ------------- | ---------------- |\n",
        "| layer ê°€ì¤‘ì¹˜ | ëª¨ë“  blockì´ ë…ë¦½ì  | í•˜ë‚˜ì˜ blockì„ 4ë²ˆ ë°˜ë³µ |\n",
        "| ë©”ëª¨ë¦¬       | í¼             | ë§¤ìš° ì‘ìŒ            |\n",
        "| ì„±ëŠ¥        | ì•ˆì •ì            | ê°€ë²¼ìš°ë©° ë¹ ë¦„          |\n"
      ],
      "metadata": {
        "id": "d6Ln-s-MjjUU"
      }
    }
  ]
}