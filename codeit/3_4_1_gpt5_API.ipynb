{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumshin-dev/python_conda_jupyter/blob/main/codeit/3_4_1_gpt5_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-5.1 와 API 사용 예\n",
        "\n",
        "| 모드 (Mode)      | 용도 (Use Case)   | 속도 (Speed, 1~5) | 정확도 (Accuracy, 1~5) | 비용 (Cost, 1~5) |\n",
        "|------------------|--------------------|---------------------|--------------------------|-------------------|\n",
        "| **Instant**       | 일상 작업          | 3                   | 4                        | 2                 |\n",
        "| **Thinking**      | 복잡한 추론        | 2                   | 5                        | 3                 |\n",
        "| **No Reasoning**  | 빠른 응답          | 4                   | 3                        | 1                 |\n"
      ],
      "metadata": {
        "id": "H8zJQRm0I-4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ GPT-5.1 API 모델명 (Instant / Thinking)\n",
        "\n",
        "**Instant 모드**\n",
        "- 모델명: \"gpt-5.1-chat-latest\"\n",
        "- 특징: 빠른 응답, 일상적인 작업·대화형 작업에 적합\n",
        "- reasoning_effort=\"high\" 옵션을 주면 Instant 모델에서도 일정 수준의 Thinking 스타일 추론을 유도할 수 있음\n",
        "\n",
        "**Thinking 모드**\n",
        "- 모델명: \"gpt-5.1\"\n",
        "- 특징: 복잡한 추론, 논리적 문제 해결, 코드 분석에 적합\n",
        "\n",
        "https://platform.openai.com/docs/pricing\n",
        "\n",
        "https://platform.openai.com/docs/models\n"
      ],
      "metadata": {
        "id": "-BRYPYLGPf1_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cM1MaFwmvmeb",
        "outputId": "0dc112c5-14fb-46d2-f445-b0fe50fd972a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Name: openai\n",
            "Version: 1.109.1\n",
            "Summary: The official Python library for the openai API\n",
            "Home-page: https://github.com/openai/openai-python\n",
            "Author: \n",
            "Author-email: OpenAI <support@openai.com>\n",
            "License: Apache-2.0\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "# 1. OpenAI Python 라이브러리 설치\n",
        "!pip install openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# OpenAI API key\n",
        "openai_api_key = getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "# API키 설정\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSCTlKKowV7E",
        "outputId": "8e2d8f6c-43cb-44c6-b507-3beed1ad59cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **API Key 발급**\n",
        "\n",
        "1. https://platform.openai.com 접속\n",
        "2. API Keys 메뉴에서 새 키 생성"
      ],
      "metadata": {
        "id": "glRImUoNw_8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
        "\n",
        "# 간단한 테스트\n",
        "response = client.chat.completions.create(\n",
        "    model=\"pt-5.1-chat-latest\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"안녕!\"}]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts_St21Cw_CT",
        "outputId": "b1569f39-7520-468b-c738-f068e13ee355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕! 반가워요. 무엇을 도와드릴까요? 몇 가지 예시를 드리면:\n",
            "\n",
            "- 한국어 공부 도움: 표현, 문법, 어휘 공부\n",
            "- 번역이나 문장 다듬기\n",
            "- 정보 찾기: 과학, 기술, 역사 등\n",
            "- 레시피나 여행 정보\n",
            "- 코드나 수학 문제 풀이\n",
            "\n",
            "먼저 하고 싶은 주제나 궁금한 점이 있나요? 오늘의 기분이나 관심사도 들려주시면 같이 얘기해 볼게요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OpenAI Key Pricing**\n",
        "https://platform.openai.com/docs/pricing\n"
      ],
      "metadata": {
        "id": "e3qDQh_sxTDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Instant 모드"
      ],
      "metadata": {
        "id": "WLU8EMHmJS1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Adaptive Reasoning - 질문 난이도 자동 판단\n",
        "- GPT-5.1 Instant는 질문의 난이도를 자동으로 판단\n",
        "- 쉬운 질문: 빠른 응답 (2초)\n",
        "- 어려운 질문: 깊은 사고 (10초)\n",
        "\n"
      ],
      "metadata": {
        "id": "Un5ZXCa0JfIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 쉬운 질문 - 빠른 응답\n",
        "easy_question = \"npm 전역 패키지 목록 명령어는?\"\n",
        "# 예상 응답 시간: ~2초\n",
        "\n",
        "# API call\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-5.1-chat-latest\",\n",
        "    messages=[{\"role\": \"user\", \"content\": easy_question}]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW8UOjVvJlXk",
        "outputId": "a96c481b-0dff-43f3-ce08-a1027518a873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "npm에서 전역(global)으로 설치된 패키지 목록을 보려면 다음 명령어를 사용하면 됩니다.\n",
            "\n",
            "npm list -g --depth=0\n",
            "\n",
            "설명:\n",
            "• -g : 전역(global) 패키지를 의미\n",
            "• --depth=0 : 최상위 패키지만 보여줘서 보기 쉽게 함\n",
            "\n",
            "추가로 전역 설치 경로를 확인하려면:\n",
            "\n",
            "npm root -g\n",
            "\n",
            "npm이 전역 패키지를 설치하는 위치를 확인할 수 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 어려운 질문 - 깊은 사고\n",
        "hard_question = \"\"\"\n",
        "다음 시나리오에서 최적의 마이크로서비스 아키텍처를 설계하세요:\n",
        "- 일일 1억 건의 트랜잭션\n",
        "- 99.99% 가용성 요구\n",
        "- 실시간 데이터 동기화\n",
        "- 글로벌 배포\n",
        "\"\"\"\n",
        "# 예상 응답 시간: ~10초\n",
        "\n",
        "# API call\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-5.1-chat-latest\",\n",
        "    messages=[{\"role\": \"user\", \"content\": hard_question}]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOMlqsAhKKpC",
        "outputId": "39da77c1-ccf5-4e91-c1b6-ba17fca5fa89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "아래는 제시된 조건(일일 1억 트랜잭션, 99.99% 가용성, 실시간 동기화, 글로벌 배포)에 최적화된 **마이크로서비스 아키텍처 설계**입니다.  \n",
            "특정 기술 이름은 예시이며, 전체 구조 설계를 중심으로 설명합니다.\n",
            "\n",
            "---\n",
            "\n",
            "최적 아키텍처 개요\n",
            "- 글로벌 분산 환경을 전제로 한 **이벤트 기반 마이크로서비스 아키텍처**\n",
            "- 지역별로 독립 운영하면서 데이터는 실시간으로 전 세계에 **Eventually Consistent** 또는 **Strong-consistency가 필요한 부분만 선택적 처리**\n",
            "- 지역 장애에도 운영 가능한 **Active‑Active 배포**\n",
            "- 99.99% 가용성을 위한 **자동 복구·무중단 배포·수평확장 구조**\n",
            "\n",
            "---\n",
            "\n",
            "핵심 구성 요소\n",
            "\n",
            "1. API Gateway / Edge Layer  \n",
            "   - 지역별(Region-local) API Gateway 운영  \n",
            "   - 글로벌 트래픽 라우팅: Anycast DNS 또는 GSLB  \n",
            "   - 기능  \n",
            "     * Rate limiting  \n",
            "     * Auth  \n",
            "     * Request routing  \n",
            "     * Zero-downtime failover\n",
            "\n",
            "2. 마이크로서비스 구조  \n",
            "   - 각 서비스는 독립 배포·스케일링 가능  \n",
            "   - Stateless 우선 설계  \n",
            "   - 필요 시 CQRS 패턴 적용(읽기/쓰기 분리)\n",
            "\n",
            "3. 데이터 저장 계층  \n",
            "   - 글로벌 분산 가능 DB 선택  \n",
            "       * 예: DynamoDB Global Table, CockroachDB, Spanner 등  \n",
            "   - 쓰기 강도가 높은 서비스는 지역-local write → 메시지 스트림 기반 글로벌 동기화  \n",
            "   - 장점  \n",
            "       * Latency 감소  \n",
            "       * 장애 지역 독립성  \n",
            "       * 글로벌 데이터 일관성 보장\n",
            "\n",
            "4. 이벤트 스트리밍 & 실시간 동기화  \n",
            "   - 핵심 메시지 버스: Kafka, Pulsar, 또는 Managed streaming (MSK, PubSub 등)  \n",
            "   - 이벤트 기반 구조(Event-driven Architecture)  \n",
            "   - 실시간 동기화를 위한 Change Data Capture(CDC)  \n",
            "   - Outbox 패턴으로 서비스-DB-Stream간 데이터 유실 방지\n",
            "\n",
            "5. 캐시·서치 계층  \n",
            "   - 글로벌 캐싱: CDN + Regional cache (Redis Cluster 또는 Memory store)  \n",
            "   - 검색이 필요한 경우: 글로벌 동기화가 가능한 Search cluster (OpenSearch 등)\n",
            "\n",
            "6. 트랜잭션 처리 구조  \n",
            "   - 일일 1억 트랜잭션 기준 TPS ≈ 1157/sec  \n",
            "   - Burst 대비 10배 수준으로 설계  \n",
            "   - 트랜잭션은 다음 방식으로 처리  \n",
            "       * Idempotency key 적용  \n",
            "       * 분산 트랜잭션 회피  \n",
            "       * Saga 패턴으로 보상 처리\n",
            "\n",
            "7. 운영/내결함성  \n",
            "   - Multi-Region Active‑Active 구성  \n",
            "   - 서비스 실패 시 자동 트래픽 전환  \n",
            "   - 각 서비스는 다음 기능 필수  \n",
            "       * Circuit Breaker  \n",
            "       * Retry with backoff  \n",
            "       * Health check & auto-healing  \n",
            "       * Blue/Green 또는 Canary Deployment\n",
            "\n",
            "8. 모니터링·관측성  \n",
            "   - 글로벌 통합 관측 플랫폼  \n",
            "       * Tracing: OpenTelemetry  \n",
            "       * Logs: 중앙 로그 수집  \n",
            "       * Metrics: Prometheus + Thanos 또는 Managed solution  \n",
            "   - SLA · SLO 기반 자동 Scale-out\n",
            "\n",
            "---\n",
            "\n",
            "아키텍처 동작 흐름(요약)\n",
            "\n",
            "1. 사용자가 전 세계 어느 지역에서든 접근  \n",
            "2. Anycast DNS/GSLB가 가장 가까운 Region의 API Gateway로 라우팅  \n",
            "3. API Gateway → 해당 Region microservice 처리  \n",
            "4. 서비스는 지역 DB에 우선 저장 (Strong or Local-first)  \n",
            "5. CDC 또는 이벤트 스트림을 통해 글로벌 지역으로 데이터 전파  \n",
            "6. 각 Region은 실시간으로 동기화된 데이터 기반으로 동작  \n",
            "7. 장애 발생 시 글로벌 라우팅 레이어가 자동으로 정상 Region으로 트래픽 전환\n",
            "\n",
            "---\n",
            "\n",
            "기술 스택 예시\n",
            "\n",
            "- Gateway: API Gateway, Nginx, Cloudflare  \n",
            "- Service: Kubernetes 기반 마이크로서비스  \n",
            "- DB: Spanner / DynamoDB Global Table / CockroachDB  \n",
            "- Streaming: Kafka / PubSub / MSK  \n",
            "- Cache: Redis, CDN  \n",
            "- Monitoring: OpenTelemetry + Prometheus + Loki  \n",
            "\n",
            "---\n",
            "\n",
            "결론  \n",
            "본 아키텍처는  \n",
            "- 글로벌 Active‑Active 운영  \n",
            "- 서비스 간 독립성 확보  \n",
            "- 실시간 데이터 복제  \n",
            "- 초당 수천 건 이상의 안정적 처리  \n",
            "- 99.99% 가용성을 만족  \n",
            "하는 것을 목표로 설계되었습니다.\n",
            "\n",
            "원하시면 **아키텍처 다이어그램 형태**나 **특정 도메인(금융, 커머스 등)에 맞춘 세부 설계**도 추가 가능합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZB2W266DKJI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 실전 활용\n",
        "\n",
        "**Use Case 1: 이메일 자동 작성**\n"
      ],
      "metadata": {
        "id": "QiAcghfPxzpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "다음 정보로 전문적인 이메일을 작성하세요:\n",
        "- 받는 사람: 김철수 부장님\n",
        "- 목적: 프로젝트 일정 연기 요청\n",
        "- 이유: 외부 API 통합 지연\n",
        "- 새로운 일정: 2주 연장\n",
        "\"\"\"\n",
        "\n",
        "# API call\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-5-nano\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lZxfJZWjyCDU",
        "outputId": "3794006c-9b97-4826-ae33-c57df3209f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: 프로젝트 일정 연기 요청의 건\n",
            "\n",
            "수신: 김철수 부장님께\n",
            "\n",
            "안녕하세요. 프로젝트 관리팀의 [발신인 이름]입니다.\n",
            "\n",
            "현재 진행 중인 [프로젝트명]의 일정에 대해 보고 드리며, 일정 연기를 공식적으로 요청드립니다.\n",
            "\n",
            "사유: 외부 API 통합 지연으로 인해 원래 계획대로 개발 및 검증을 완료하기 어려운 상황입니다. 이로 인해 전체 일정에 영향이 예상되므로 부득이한 일정 연장이 필요합니다.\n",
            "\n",
            "새로운 일정: 기존 마감일로부터 2주 연장된 일정으로 조정하는 것을 제안드립니다. 구체적 마일스톤 및 예상 완료일은 부장님의 확인 후 확정해 드리겠습니다.\n",
            "\n",
            "대책 및 관리: 리스크 관리 계획을 보강하고, 외부 API 상태를 주간으로 모니터링하며 필요 시 대체 시나리오를 마련하겠습니다. 또한, 진행 상황은 주간 업데이트를 통해 공유드리겠습니다.\n",
            "\n",
            "부장님의 확인 및 승인을 부탁드립니다. 승인이 되면 즉시 상세 일정표를 공유하고, 필요한 경우 짧은 미팅을 통해 추가 설명드리겠습니다. 시간을 알려주시면 조정하겠습니다.\n",
            "\n",
            "감사합니다.\n",
            "\n",
            "[발신인 이름] [직책] [연락처] [소속/부서]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hJR1eRuCB_xi",
        "outputId": "9e286948-a1bb-42e2-f1ca-b68e001d46e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-CfIFXXT6Xoy0FOJZbGJYr8GsOIclQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Subject: 프로젝트 일정 연기 요청의 건\\n\\n수신: 김철수 부장님께\\n\\n안녕하세요. 프로젝트 관리팀의 [발신인 이름]입니다.\\n\\n현재 진행 중인 [프로젝트명]의 일정에 대해 보고 드리며, 일정 연기를 공식적으로 요청드립니다.\\n\\n사유: 외부 API 통합 지연으로 인해 원래 계획대로 개발 및 검증을 완료하기 어려운 상황입니다. 이로 인해 전체 일정에 영향이 예상되므로 부득이한 일정 연장이 필요합니다.\\n\\n새로운 일정: 기존 마감일로부터 2주 연장된 일정으로 조정하는 것을 제안드립니다. 구체적 마일스톤 및 예상 완료일은 부장님의 확인 후 확정해 드리겠습니다.\\n\\n대책 및 관리: 리스크 관리 계획을 보강하고, 외부 API 상태를 주간으로 모니터링하며 필요 시 대체 시나리오를 마련하겠습니다. 또한, 진행 상황은 주간 업데이트를 통해 공유드리겠습니다.\\n\\n부장님의 확인 및 승인을 부탁드립니다. 승인이 되면 즉시 상세 일정표를 공유하고, 필요한 경우 짧은 미팅을 통해 추가 설명드리겠습니다. 시간을 알려주시면 조정하겠습니다.\\n\\n감사합니다.\\n\\n[발신인 이름] [직책] [연락처] [소속/부서]', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1763957635, model='gpt-5-nano-2025-08-07', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=3435, prompt_tokens=60, total_tokens=3495, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=3136, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Case 2: 코드 리뷰**"
      ],
      "metadata": {
        "id": "PVJpXWLtyDtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "다음 Python 코드를 리뷰하고 개선점을 제시하세요:\n",
        "\n",
        "def process_data(data):\n",
        "    result = []\n",
        "    for i in range(len(data)):\n",
        "        if data[i] > 0:\n",
        "            result.append(data[i] * 2)\n",
        "    return result\n",
        "\"\"\"\n",
        "\n",
        "# API call\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-5-nano\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9CN4-TgCxSnB",
        "outputId": "e755076f-c9d9-42c6-84ee-1ad12e3dae44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "다음 코드의 의도는 데이터에서 양수인 값들만 남기고 각 값을 2배로 만드는 것입니다. 현재 구현은 파이썬답지 않게 인덱스 접근(range(len(data)))을 사용하고 있어 가독성과 성능 측면에서 개선 여지가 있습니다. 아래에 개선점과 간단한 대안을 제시합니다.\n",
            "\n",
            "개선점 요약\n",
            "- 파이썬ic하게 작성: 인덱스 순회 대신 직접 원소를 순회하기\n",
            "- 간결한 표현: 리스트 컴프리헨션으로 한 줄로 표현 가능\n",
            "- 타입 힌트 추가: 코드의 의도를 명확히 하고 IDE 도움을 받기 쉬움\n",
            "- 문서화: 간단한 docstring 추가\n",
            "- 입력 검증: data가 iterable인지 확인하거나 None 처리 등을 고려 가능\n",
            "- 보완 옵션: 비례 계수(factor) 같은 파라미터화, 제네레이터 버전으로 메모리 절감 가능\n",
            "- 부정적 예외 관리: 불리언(True/False)도 포함될 수 있는 점 주의(필요 시 제외 가능)\n",
            "\n",
            "권장 개선 코드 예시\n",
            "\n",
            "1) 가장 간단하고 파이썬답게\n",
            "- 의도: 양수인 값 x에 대해 x*2를 결과로 만듦\n",
            "- 구현: 리스트 컴프리헨션 사용\n",
            "\n",
            "def process_data(data):\n",
            "    \"\"\"Return a list of doubled positive numbers from the input data.\"\"\"\n",
            "    return [x * 2 for x in data if x > 0]\n",
            "\n",
            "2) 타입 힌트와 함께 더 명확하게\n",
            "- Iterable을 받아서 List로 반환하는 형태로 명확히 표시\n",
            "\n",
            "from typing import Iterable, List\n",
            "\n",
            "def process_data(data: Iterable[float]) -> List[float]:\n",
            "    \"\"\"Return a list of doubled positive numbers from the input data.\"\"\"\n",
            "    return [x * 2 for x in data if x > 0]\n",
            "\n",
            "참고: input에 정수도 들어올 수 있으므로 float로 표기해도 되지만, 파이썬 타입 시스템은 런타임에서 강제되지는 않으니 필요에 따라 int도 처리됩니다.\n",
            "\n",
            "3) 계수(factor) 파라미터를 추가해 일반화하기\n",
            "- 2배가 아니라 임의의 양의 스케일링을 원할 때 유용\n",
            "\n",
            "def process_data(data: Iterable[float], factor: float = 2.0) -> List[float]:\n",
            "    \"\"\"Return a list of (x * factor) for each positive x in data.\"\"\"\n",
            "    return [x * factor for x in data if x > 0]\n",
            "\n",
            "4) 메모리 절감이 필요하다면 제네레이터로 반환하기\n",
            "- 호출 측에서 리스트가 필요하지 않고 스트림으로 처리 가능하다면\n",
            "\n",
            "def process_data_generator(data: Iterable[float], factor: float = 2.0):\n",
            "    \"\"\"Yield doubled positives one by one (memory efficient).\"\"\"\n",
            "    for x in data:\n",
            "        if x > 0:\n",
            "            yield x * factor\n",
            "\n",
            "5) 부가적인 주의점\n",
            "- 불리언 값(True/False)을 원치 않는다면 필터에서 제외 가능:\n",
            "  - 예: if isinstance(x, (int, float)) and not isinstance(x, bool) and x > 0\n",
            "  - 다만 일반적으로 수치형 데이터의 경우 bool도 0/1로 취급되어 의도치 않게 포함될 수 있습니다. 필요 시 명시적으로 처리합니다.\n",
            "- 입력이 비정렬(None 등)인 경우에 대한 검증이 필요하면 초기에 체크를 추가할 수 있습니다.\n",
            "- 테스트를 추가하면 유지보수에 도움이 됩니다.\n",
            "\n",
            "간단한 사용 예\n",
            "- 원래 의도대로 작동하는지 확인:\n",
            "\n",
            "print(process_data([-1, 0, 1, 2.5]))  # 예: [2.0, 5.0] (또는 [2, 4] 등 입력 타입에 따라 다름)\n",
            "\n",
            "- 제네레이터 버전 사용 예:\n",
            "\n",
            "for v in process_data_generator([ -1, 0, 1, 2.5 ]):\n",
            "    print(v)  # 2.0, 5.0\n",
            "\n",
            "요청하신 개선 방향에 맞춰 간단히 포맷만 유지하면서도 파이썬답고 확장 가능한 코드로 바꾸는 것이 좋습니다. 원하시면 사용 맥락(입력 크기, 데이터 타입, 메모리 제약 등)에 맞춰 가장 적합한 버전(예: 매개변수 추가 여부, 제네레이터 사용 여부)을 더 구체적으로 제안해 드리겠습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용량 확인\n",
        "print(f\"입력 토큰: {response.usage.prompt_tokens}\")\n",
        "print(f\"출력 토큰: {response.usage.completion_tokens}\")\n",
        "print(f\"총 토큰: {response.usage.total_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVVrnL8SEMxh",
        "outputId": "1f6e9d0b-5418-47f4-98a9-a53d8ccfab4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 토큰: 60\n",
            "출력 토큰: 3879\n",
            "총 토큰: 3939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 성능 최적화\n",
        "\n",
        "**프롬프트 캐싱 활용:**"
      ],
      "metadata": {
        "id": "kFmhI2q8yQlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "# ============================================\n",
        "# 1. 매우 긴 system 프롬프트 준비 (약 2000+ 토큰)\n",
        "# ============================================\n",
        "\n",
        "base_text = \"\"\"\n",
        "You are an expert AI engineer. Provide concise, correct answers.\n",
        "This is prefix text for prompt caching demonstration.\n",
        "Repeat this section many times to increase token length.\n",
        "\"\"\"\n",
        "\n",
        "# 동일한 문장을 여러 번 반복 → 길고 반복되는 prefix 생성\n",
        "long_system_prompt = base_text * 150   # 약 2000~2500 토큰\n",
        "\n",
        "# 유저 메시지\n",
        "user_prompt = \"이 프롬프트 캐싱 예제에서 system 프롬프트는 몇 글자인가요?\"\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 2. 함수: API 호출 + usage 출력\n",
        "# ============================================\n",
        "\n",
        "def call_api(tag):\n",
        "    print(f\"\\n=== {tag} 호출 ===\")\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-5.1-chat-latest\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": long_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    print(\"입력 토큰:\", response.usage.prompt_tokens)\n",
        "    print(\"출력 토큰:\", response.usage.completion_tokens)\n",
        "    print(\"총 토큰:\", response.usage.total_tokens)\n",
        "\n",
        "    # 캐싱 정보\n",
        "    details = getattr(response.usage, \"prompt_tokens_details\", None)\n",
        "    print(\"prompt_tokens_details:\", details)\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "w3T-7VkpGr7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 3. 첫 번째 호출 (Warm-up: 캐시 생성)\n",
        "# ============================================\n",
        "first = call_api(\"1차 (캐시 생성)\")\n",
        "\n",
        "# ============================================\n",
        "# 4. 두 번째 호출 (Cache Hit: 프롬프트 캐싱 발생)\n",
        "# ============================================\n",
        "second = call_api(\"2차 (캐시 재사용 기대)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YdtEzDjGxQy",
        "outputId": "f1f2dc73-a23b-4fbf-9b80-b8b4bf9c2a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 1차 (캐시 생성) 호출 ===\n",
            "입력 토큰: 4833\n",
            "출력 토큰: 493\n",
            "총 토큰: 5326\n",
            "prompt_tokens_details: PromptTokensDetails(audio_tokens=0, cached_tokens=0)\n",
            "\n",
            "=== 2차 (캐시 재사용 기대) 호출 ===\n",
            "입력 토큰: 4833\n",
            "출력 토큰: 188\n",
            "총 토큰: 5021\n",
            "prompt_tokens_details: PromptTokensDetails(audio_tokens=0, cached_tokens=4736)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- OpenAI 플랫폼 문서: [프롬프트 캐싱 가이드](https://platform.openai.com/docs/guides/prompt-caching/how-it-works?utm_source=chatgpt.com)\n",
        "- OpenAI 블로그/제품 페이지 [API 프롬프트 캐싱](https://openai.com/index/api-prompt-caching/?utm_source=chatgpt.com)"
      ],
      "metadata": {
        "id": "KRLn1dW-HZDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenAI Chat Completion API에서 사용하는 3가지 role 설명\n",
        "1) system    : 모델의 행동 규칙, 톤, 역할을 설정하는 메시지\n",
        "2) user      : 사용자 입력(질문, 요구사항 등)을 전달하는 메시지\n",
        "3) assistant : 모델이 이전에 했던 답변을 다시 포함할 때 사용하는 메시지\n",
        "---\n",
        "\n",
        "- 일반적으로 프롬프트를 보낼 때는 \"user\"를 사용하고,\n",
        "- 대화 규칙을 설정하려면 \"system\",\n",
        "- 이전 모델 답변을 컨텍스트로 넣을 때는 \"assistant\"를 사용합니다."
      ],
      "metadata": {
        "id": "4XwHLLCIE_pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thinking 모드"
      ],
      "metadata": {
        "id": "UdvsdFA1MGqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 동적 사고 시간 이해\n",
        "\n",
        "Thinking 모드의 특징:\n",
        "\n",
        "- 쉬운 작업: 2배 빠름\n",
        "- 어려운 작업: 2배 느림 (더 정확)\n",
        "- reasoning_effort 파라미터로 제어"
      ],
      "metadata": {
        "id": "Azvkuho9NSRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "100만 개의 로그 데이터에서 특정 사용자 행동 패턴을 효율적으로 탐지하기 위한 알고리즘을 설계해 주세요.\n",
        "시간 복잡도, 공간 복잡도, 그리고 왜 이 방법이 최적인지 단계별로 설명해 주세요.\n",
        "\"\"\"\n",
        "\n",
        "# reasoning_effort 설정\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-5.1\",\n",
        "    messages=[{\"role\": \"user\", \"content\": question}],\n",
        "    reasoning_effort=\"high\"  # low, medium, high\n",
        ")"
      ],
      "metadata": {
        "id": "s1kR5VdWMMlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfQB0EYNNDTO",
        "outputId": "8644ba3e-646b-4064-81b4-f19c92272033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "다음은 “100만 개 로그에서 특정 사용자 행동 패턴(정해진 순서의 액션들)을 효율적으로 찾는” 알고리즘 설계입니다.  \n",
            "\n",
            "---\n",
            "\n",
            "## 0. 문제 가정(모델링)\n",
            "\n",
            "명확한 알고리즘 설명을 위해 아래와 같이 가정하겠습니다.\n",
            "\n",
            "- 로그 포맷:  \n",
            "  `(timestamp, user_id, action_type, ...)`\n",
            "- 로그 수: `N = 1,000,000` (100만)\n",
            "- 패턴: 길이 `m`인 사용자 행동 시퀀스  \n",
            "  예: `P = [VIEW_PRODUCT, ADD_TO_CART, PURCHASE]` (m = 3)\n",
            "- 목표:\n",
            "  - 각 사용자별로, 시간 순서대로 발생한 `action_type` 시퀀스 안에 `P`가 부분 시퀀스로 등장하는지 탐지\n",
            "  - 로그는 이미 `timestamp` 기준으로 정렬되어 있다고 가정(일반적인 로그 수집 시스템)\n",
            "\n",
            "---\n",
            "\n",
            "## 1. 직관적인(비효율적인) 방법과 한계\n",
            "\n",
            "1. **사용자별로 로그를 모으고 정렬**\n",
            "   - `user_id` 기준으로 그룹핑 → 각 사용자별로 시퀀스 생성\n",
            "   - 각 시퀀스에서 패턴 문자열 탐색(예: 단순 탐색)\n",
            "2. 문제점\n",
            "   - 그룹핑/정렬: `O(N log N)` (정렬 필요)\n",
            "   - 각 시퀀스 내 탐색을 단순하게 하면 최악 `O(N·m)`  \n",
            "   - 전체적으로 비효율: 최대 `O(N log N + N·m)`\n",
            "\n",
            "목표는 **정렬 없이 한 번의 스트리밍으로, 전체를 거의 선형 시간**에 처리하는 것입니다.\n",
            "\n",
            "---\n",
            "\n",
            "## 2. 핵심 아이디어: “사용자별 KMP 상태를 유지하는 1-pass 알고리즘”\n",
            "\n",
            "문자열 패턴 매칭 알고리즘인 **KMP (Knuth-Morris-Pratt)** 를 응용해,  \n",
            "각 사용자마다 현재 패턴이 얼마나 일치했는지 “부분 일치 길이”를 상태로 갖도록 합니다.\n",
            "\n",
            "### 2.1 패턴 전처리(KMP prefix function)\n",
            "\n",
            "패턴 `P` (길이 m)에 대해, KMP의 **prefix function(π 배열)** 을 미리 계산합니다.\n",
            "\n",
            "- `π[i]` = `P[0..i]`의 접두사이면서 접미사이기도 한 부분 문자열 중 최대 길이\n",
            "- 전처리 시간: `O(m)`\n",
            "- 이 배열 덕분에, 매 로그 처리 시 **최대 상수 시간**으로 상태 전이를 할 수 있습니다.\n",
            "\n",
            "### 2.2 사용자별 상태 저장 구조\n",
            "\n",
            "- 해시 맵(또는 배열)을 사용: `state[user_id] = 현재까지 패턴이 몇 글자 일치했는가 (0~m)`\n",
            "  - 예:  \n",
            "    - `state[U] = k` 의미: 사용자 U의 최근 행동 시퀀스에서 `P[0..k-1]`까지는 맞게 일치한 상태\n",
            "- 로그는 타임라인 전체에서 사용자들이 섞여 있지만,  \n",
            "  각 로그를 읽을 때 해당 user의 상태만 업데이트하면 되므로 **정렬이 필요 없음**.\n",
            "\n",
            "### 2.3 로그 스트리밍 처리 알고리즘 (KMP per user)\n",
            "\n",
            "로그를 시간 순서대로 한 줄씩 읽으면서 다음을 수행합니다.\n",
            "\n",
            "```pseudo\n",
            "입력: 패턴 P[0..m-1], 로그들 L[0..N-1]\n",
            "\n",
            "1. π = build_prefix_function(P)   // O(m)\n",
            "2. state = 빈 해시맵  // user_id -> 0 ~ m\n",
            "\n",
            "3. for i in 0..N-1:\n",
            "       (t, u, a, ...) = L[i]   // timestamp, user_id, action_type\n",
            "\n",
            "       s = state.get(u, 0)     // 사용자 u의 현재 일치 길이 (기본값 0)\n",
            "\n",
            "       // KMP 상태 전이\n",
            "       while s > 0 and P[s] != a:\n",
            "           s = π[s - 1]\n",
            "\n",
            "       if P[s] == a:\n",
            "           s = s + 1\n",
            "\n",
            "       if s == m:\n",
            "           // 패턴 완전히 매칭됨\n",
            "           패턴 발견 처리(u, t, i 등 기록)\n",
            "           s = π[m - 1]  // 겹치는 패턴도 잡기 위해 KMP 규칙대로 다음 상태로 이동\n",
            "\n",
            "       state[u] = s\n",
            "```\n",
            "\n",
            "- 각 로그 레코드에 대해:\n",
            "  - 현재 유저의 상태 `s`를 가져오고,\n",
            "  - 새 액션 `a`를 넣어 KMP 전이 규칙을 적용\n",
            "  - 상태가 m에 도달하면 패턴 감지\n",
            "\n",
            "**핵심:** 모든 사용자가 뒤섞인 상태 그대로 한 번만 스캔하면서,  \n",
            "각 사용자에 대해 독립적으로 KMP 매칭을 수행하는 것과 동일한 효과.\n",
            "\n",
            "---\n",
            "\n",
            "## 3. 시간 복잡도 분석\n",
            "\n",
            "### 3.1 패턴 전처리\n",
            "\n",
            "- π 배열 계산: `O(m)`\n",
            "\n",
            "### 3.2 로그 처리(loop)\n",
            "\n",
            "- 각 로그 레코드마다 수행하는 작업:\n",
            "  - 현재 상태 조회: 해시맵 조회 `O(1)` 평균\n",
            "  - KMP 전이: while-loop + if 체크  \n",
            "    KMP 특성상, 전체 N개의 입력에 대해 이 while 루프는 **총합 O(N)** 까지만 돈다는 것이 유명한 성질입니다.  \n",
            "    → 각 로그당 amortized `O(1)`  \n",
            "- 따라서 전체 로그 처리 시간: **`O(N)`**\n",
            "\n",
            "### 3.3 전체 시간\n",
            "\n",
            "- 총 시간 복잡도:\n",
            "  - `O(m) + O(N)` = **`O(N + m)`**\n",
            "\n",
            "100만 개 로그(`N=10^6`), 패턴 길이 `m`이 수십~수백 정도라면 완전히 선형 시간으로 처리됩니다.\n",
            "\n",
            "---\n",
            "\n",
            "## 4. 공간 복잡도 분석\n",
            "\n",
            "- `π` 배열: 크기 `m` → `O(m)`\n",
            "- `state` 맵:\n",
            "  - 사용자 수 `U`만큼의 정수 상태(`0~m`) 저장 → `O(U)`\n",
            "  - 최악의 경우, 각 사용자당 최소 1개의 로그가 있으면 `U ≤ N`\n",
            "- 기타 상수 크기 변수들: 무시 가능\n",
            "\n",
            "따라서 전체 공간 복잡도:\n",
            "\n",
            "- **`O(U + m)`**,  \n",
            "- 최악의 경우 `O(N + m)` 이지만,  \n",
            "  보통 `U << N` 인 경우가 많아 훨씬 작게 나옵니다.\n",
            "\n",
            "---\n",
            "\n",
            "## 5. 왜 이 방법이 “(점근적으로) 최적”인가?\n",
            "\n",
            "### 5.1 시간 측면: 최소 하한(Lower Bound)에 근접\n",
            "\n",
            "1. **모든 로그를 적어도 한 번은 읽어야 한다**\n",
            "   - 어떤 알고리즘이든, 패턴이 특정 로그 안에 숨어있는지 판단하려면  \n",
            "     그 로그를 최소 한 번은 읽어야 하므로, **시간 하한은 Ω(N)** 입니다.\n",
            "2. **패턴 자체도 한 번은 읽어야 한다**\n",
            "   - 패턴 길이 m에 대해, 전혀 읽지 않고 매칭을 할 수는 없으므로 **Ω(m)**.\n",
            "\n",
            "→ 이 문제의 이론적 시간 하한은 `Ω(N + m)` 입니다.  \n",
            "\n",
            "우리가 설계한 알고리즘은 **`O(N + m)`**에 동작하므로,  \n",
            "점근적으로 **최적(Optimal up to constant factor)** 입니다.\n",
            "\n",
            "### 5.2 공간 측면: 사용자별 상태 정보의 필요성\n",
            "\n",
            "1. **각 사용자별로 최소한의 진행 상태는 필요**\n",
            "   - “사용자 U가 패턴의 어디까지 일치했는가” 를 모를 경우,\n",
            "   - 다음 액션이 들어왔을 때 이게 패턴의 어느 부분과 대응되는지 매번 처음부터 다시 비교해야 함 → 시간 폭증\n",
            "2. KMP 방식은 이 상태를 단일 정수(`0~m`)로 축약\n",
            "   - 이는 “현재까지의 히스토리”를 요약한 것\n",
            "   - 이보다 적은 정보를 사용하면, 같은 상태지만 실제로는 다른 히스토리를 구분 못 해 잘못된 매칭/누락이 발생 가능\n",
            "3. 따라서 **사용자당 최소 O(1)** 크기의 상태는 필요,\n",
            "   - 전체적으로 **O(U)** 공간은 사실상 필수적인 수준의 하한에 가깝습니다.\n",
            "\n",
            "→ 우리의 알고리즘은 per-user **O(1)** 상태만 유지하므로,  \n",
            "공간 측면에서도 불필요하게 크지 않고 **합리적인(사실상 최소 수준의)** 사용량입니다.\n",
            "\n",
            "---\n",
            "\n",
            "## 6. 패턴이 여러 개이거나, 시간 제약이 있는 경우 확장\n",
            "\n",
            "질문에는 단일 패턴만 언급되어 있지만, 실무에서 자주 필요한 확장입니다.\n",
            "\n",
            "### 6.1 여러 개의 패턴 동시 탐지\n",
            "\n",
            "- 여러 행동 패턴 `P1, P2, ..., Pk`를 동시에 찾고 싶다면:\n",
            "  - 문자열 패턴 매칭의 **Aho–Corasick 자동자**를 사용\n",
            "  - 전처리: 모든 패턴을 Trie + 실패 링크 구성 → `O(패턴 총 길이)`  \n",
            "  - 로그 처리: 여전히 각 로그당 **O(1)** 평균 시간  \n",
            "  - 사용자별로 “자동자 상태”만 저장하면 됩니다. (KMP의 일반화 버전)\n",
            "\n",
            "시간/공간 복잡도는 유사하게 **선형 + 사용자 수** 수준입니다.\n",
            "\n",
            "### 6.2 “X 이후 10분 안에 Y, 그 후 5분 안에 Z” 같은 시간 제약\n",
            "\n",
            "- 상태에 **최근 이벤트의 타임스탬프**도 함께 저장:\n",
            "  - `state[u].k` : 패턴에서 몇 번째까지 맞았는지\n",
            "  - `state[u].last_time` : 마지막 일치 이벤트의 시간\n",
            "- 새 이벤트가 들어올 때:\n",
            "  - KMP 전이 + “해당 단계가 시간 제한을 만족하는가?” 검사\n",
            "  - 시간 제한 위반 시, 상태를 적절히 롤백 or 초기화\n",
            "\n",
            "복잡도는 여전히 `O(N)` 에 가깝게 유지할 수 있습니다.\n",
            "\n",
            "---\n",
            "\n",
            "## 7. 요약\n",
            "\n",
            "- **아이디어**:  \n",
            "  - 패턴은 KMP(또는 Aho–Corasick)로 자동자화  \n",
            "  - 각 사용자별로 “현재 패턴 일치 정도”를 상태로 저장  \n",
            "  - 로그를 시간 순으로 **한 번만** 스캔하며, 각 로그에서 해당 user의 상태만 업데이트\n",
            "- **시간 복잡도**:  \n",
            "  - 패턴 전처리: `O(m)`  \n",
            "  - 로그 처리: `O(N)` (로그당 평균 O(1))  \n",
            "  - 전체: **`O(N + m)`** → 이론적 하한과 동일, 점근적으로 최적\n",
            "- **공간 복잡도**:  \n",
            "  - `O(U + m)` (U: 사용자 수, 최악 시 `≤ N`)  \n",
            "  - 사용자당 O(1) 상태만 사용 → 사실상 최소 수준\n",
            "- **장점**:\n",
            "  - 로그 정렬이 필요 없음\n",
            "  - 대규모 로그(100만 건 이상)에서도 선형 시간 처리\n",
            "  - 여러 패턴, 시간 제약 등으로 자연스럽게 확장 가능\n",
            "\n",
            "원하시면,  \n",
            "- 특정한 패턴 예시(예: 장바구니 이탈, 결제 전 이탈 등)에 맞춘 구체적인 상태 정의,  \n",
            "- 또는 실제 코드(Python, Java 등) 수준으로 구현 예시  \n",
            "도 이어서 설명해 드리겠습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 복잡한 문제 해결\n",
        "- 전략:"
      ],
      "metadata": {
        "id": "s3-N-3UANCk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strategy_problem = \"\"\"\n",
        "스타트업 초기 단계에서 다음 상황입니다:\n",
        "- 예산: 5억원\n",
        "- 팀: 개발자 3명, 디자이너 1명\n",
        "- 목표: 6개월 내 MVP 출시\n",
        "- 경쟁사: 이미 2개 존재\n",
        "\n",
        "최적의 제품 개발 및 마케팅 전략을 수립하세요.\n",
        "각 단계별 예산 배분과 타임라인을 포함하세요.\n",
        "\"\"\"\n",
        "\n",
        "# reasoning_effort 설정\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-5.1\",\n",
        "    messages=[{\"role\": \"user\", \"content\": question}]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7r6r88pCNaoF",
        "outputId": "b4b85015-dc64-447c-91e5-1f4ddfa09268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "아래에서는 “100만 개(log line) 규모에서 특정 사용자 행동 패턴(예: A → B → C 같은 이벤트 시퀀스)”을 효율적으로 탐지하는 대표적인 알고리즘 설계를 설명합니다.  \n",
            "패턴이 하나냐/여럿이냐, 패턴이 순차 패턴이냐/일반 문자열 패턴이냐에 따라 달라지지만, 가장 일반적인 케이스인 “순서 있는 이벤트 시퀀스 패턴”을 기준으로 설명하겠습니다.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. 문제 정의 정리\n",
            "\n",
            "- 입력:\n",
            "  - 로그 수: N ≈ 1,000,000 (각 로그는 `user_id, timestamp, event_type` 정도)\n",
            "  - 행동 패턴: 길이 m인 이벤트 시퀀스 `P = [e1, e2, ..., em]`  \n",
            "    예: `P = [login, view_product, add_to_cart, purchase]`\n",
            "- 목표:\n",
            "  - 각 사용자별로 타임라인에서 P가 등장하는 모든 위치(또는 존재 여부) 탐지\n",
            "- 제약:\n",
            "  - N이 크므로 최소 O(N)에 가까운 알고리즘이 바람직\n",
            "  - 메모리도 전체 로그를 모두 복제하지 않고 스트리밍 처리 가능하면 좋음\n",
            "\n",
            "---\n",
            "\n",
            "## 2. 기본 아이디어: “사용자별로 상태를 가진 유한 상태 기계(FSM)”\n",
            "\n",
            "패턴 매칭 문제를 “유한 상태 기계(또는 KMP 같은 문자열 매칭 알고리즘)”로 바꿉니다.\n",
            "\n",
            "### 2.1. 단일 패턴(한 개의 행동 시퀀스)인 경우\n",
            "\n",
            "패턴 P 길이 = m.  \n",
            "각 사용자 u에 대해 “현재 패턴에서 어디까지 맞췄는지”를 상태로 관리합니다.\n",
            "\n",
            "#### 알고리즘(스트리밍, 온라인 처리)\n",
            "\n",
            "로그를 시간순(또는 `[user_id, timestamp]` 기준 정렬)으로 한 번씩만 훑습니다.\n",
            "\n",
            "1. **사전 준비 (패턴 전처리)**  \n",
            "   - KMP(Knuth-Morris-Pratt) 방식으로 패턴 P에 대한 실패 함수(failure function)를 계산  \n",
            "   - 시간: O(m), 공간: O(m)\n",
            "\n",
            "2. **로그 스트리밍 처리**\n",
            "   - 해시맵 `state[user_id]`에 “현재 이 사용자가 패턴에서 몇 번째 위치까지 맞췄는가(0~m)”를 저장\n",
            "   - 로그 한 줄 `(u, t, e)`에 대해:\n",
            "     1. 현재 사용자 u의 상태 `k = state[u]`를 가져옴 (없으면 0으로 초기화)\n",
            "     2. KMP 점화식을 사용해 `P[k]`와 이벤트 e를 비교  \n",
            "        - 매칭되면 `k ← k + 1`\n",
            "        - 안 맞으면 실패 함수로 k를 줄이며 재시도\n",
            "     3. k가 m에 도달하면(=패턴 전체 매칭):\n",
            "        - 패턴 발견으로 기록\n",
            "        - 패턴 중 overlapping 허용 시: 실패 함수에 따라 k를 다시 조정\n",
            "     4. 갱신된 k를 `state[u]`에 저장\n",
            "\n",
            "3. **사용자 수가 매우 많을 때**\n",
            "   - 오래 사용되지 않는 `user_id`의 상태는 LRU 정책 등으로 제거해 메모리 사용 최적화 가능  \n",
            "   - 예: 최근 7일 내 로그에 등장한 사용자만 유지\n",
            "\n",
            "---\n",
            "\n",
            "## 3. 시간 복잡도\n",
            "\n",
            "### 3.1. 패턴 전처리\n",
            "\n",
            "- KMP 실패 함수 계산: O(m)\n",
            "\n",
            "### 3.2. 로그 처리\n",
            "\n",
            "- 각 로그는 딱 한 번 읽음\n",
            "- KMP의 핵심: 문자(여기서는 이벤트) 비교 횟수가 O(N + m)에 상한됨\n",
            "  - 즉, 실패 함수 덕분에 한 이벤트를 여러 번 되돌아보지 않음\n",
            "- **단일 사용자**일 때: O(N + m)\n",
            "- **여러 사용자**일 때: 사용자별 상태만 다를 뿐, 각 로그 이벤트는 KMP 전이 계산을 1~몇 번(상수배)만 수행  \n",
            "  => **전체 시간 복잡도 = O(N + m)**\n",
            "\n",
            "실제로는 N이 매우 크고 m은 상대적으로 작으므로, 실질적인 시간은 O(N)에 가깝습니다.\n",
            "\n",
            "---\n",
            "\n",
            "## 4. 공간 복잡도\n",
            "\n",
            "구성 요소별로 나눠서 보면:\n",
            "\n",
            "1. **패턴 저장 및 실패 함수**\n",
            "   - 패턴 길이 m\n",
            "   - 실패 함수 배열 길이 m\n",
            "   - => O(m)\n",
            "\n",
            "2. **사용자 상태 저장**\n",
            "   - `state[user_id]`는 각 사용자에 대해 현재 매칭 인덱스(0~m)의 정수 하나\n",
            "   - 사용자 수를 U라 하면 O(U)\n",
            "   - U가 N보다 작거나 같으므로 최악: O(N)  \n",
            "   - 메모리 최적화를 위해:\n",
            "     - 오래 안 나오는 사용자 u의 entry를 삭제 → 활성 사용자 수 Ua만 유지  \n",
            "     - 이 경우 O(Ua)이고, 일반적으로 Ua ≪ N\n",
            "\n",
            "3. **결과 저장**\n",
            "   - 패턴이 발견될 때마다 (u, t_start, t_end) 를 기록\n",
            "   - 최악: 모든 위치에서 매칭될 수 있다면 O(N)  \n",
            "   - 하지만 이 부분은 “결과가 커서 어쩔 수 없이 드는 비용”이므로 알고리즘적인 오버헤드라기보다 문제 정의상의 필수 비용입니다.\n",
            "\n",
            "**종합 공간 복잡도:**  \n",
            "- 일반적으로: O(m + Ua + 결과 크기)  \n",
            "- 최악(모든 사용자 상태 유지, 패턴/결과 포함): O(N)\n",
            "\n",
            "---\n",
            "\n",
            "## 5. 이 방법이 효율적인 이유 (최적성 직관)\n",
            "\n",
            "1. **한 번의 패스(One-pass)로 처리: O(N)**  \n",
            "   - 로그를 여러 번 돌지 않음 → 디스크 I/O, CPU 모두 절약\n",
            "   - 다중 패스 방식(예: 모든 사용자 로그를 메모리에 모아서 정렬 후 패턴 검색)보다 훨씬 효율적\n",
            "\n",
            "2. **문자열 매칭 이론에서의 최적성**  \n",
            "   - 고전적인 결과: 임의의 패턴 문자열을 임의의 텍스트에서 찾는 데, 비교 기반 알고리즘의 평균/실무 성능은 O(N) 이상이 필요\n",
            "   - KMP는 선형 시간에 동작하는 대표적인 알고리즘이며, 불필요한 되돌아보기를 하지 않아 실제 실행 시간도 안정적\n",
            "\n",
            "3. **사용자 수와 상관없이 선형 시간 유지**  \n",
            "   - 일반적인 비효율적 방법:\n",
            "     - 사용자별로 로그를 따로 모아 정렬 후 각각 패턴 매칭 → 정렬 비용 O(N log N) + 매칭 비용 O(N)\n",
            "   - 제안 알고리즘:\n",
            "     - 전체를 시간순(이미 시간순이라 가정, 아니면 처음에 한 번만 전체 O(N log N) 정렬)으로 스캔하면서, 사용자별 상태만 따로 들고 간다 → 추가 정렬 없이 O(N)  \n",
            "   - 사용자 수가 매우 커져도, 전체 이벤트 수가 N이면 계속 O(N) 유지\n",
            "\n",
            "4. **메모리 사용의 효율성과 확장성**  \n",
            "   - state는 user_id → 정수 하나의 맵이므로, 매우 작음 (예: 4 바이트 * 사용자 수)\n",
            "   - 오래 안 나오는 사용자는 제거 가능 → 실제 활성 사용자만 유지\n",
            "   - 분산 처리에서도 잘 맞음 (샤딩된 로그를 각 노드에서 동일한 FSM 로직으로 처리 후 결과만 합치면 됨)\n",
            "\n",
            "---\n",
            "\n",
            "## 6. 확장: 여러 개의 패턴을 동시에 찾고 싶을 때\n",
            "\n",
            "패턴이 여러 개라면(A, B, C… 여러 행동 시퀀스):\n",
            "\n",
            "1. 이벤트 레벨에서 여러 패턴을 동시에 찾는다면:\n",
            "   - 문자열 매칭에서는 Aho–Corasick 알고리즘 사용\n",
            "   - 여러 패턴을 트라이(trie)에 넣고, 실패 링크를 구성\n",
            "   - 한 번의 스캔으로 모든 패턴 동시 탐지\n",
            "   - 시간: O(N + 패턴 총길이)  \n",
            "   - 공간: 트라이 노드 수 만큼 O(패턴 총길이)\n",
            "\n",
            "2. 사용자별 상태와 결합:\n",
            "   - “유한 상태 기계(FSM)”가 이제 Aho–Corasick의 오토마타가 됨  \n",
            "   - `state[user_id]`는 “현재 어떤 노드(상태)에 있는가”를 저장  \n",
            "   - 나머지 로직은 단일 패턴 대비 거의 동일\n",
            "\n",
            "---\n",
            "\n",
            "## 7. 단계별 요약\n",
            "\n",
            "1. **패턴 정의**: 찾고 싶은 행동 시퀀스를 `[e1, e2, ..., em]`으로 명시\n",
            "2. **패턴 전처리**:\n",
            "   - 단일 패턴: KMP 실패 함수 O(m)\n",
            "   - 다중 패턴: Aho–Corasick 오토마타 구성 O(패턴 총길이)\n",
            "3. **로그 스트리밍 처리(핵심)**:\n",
            "   - 해시맵 `state[user_id]`에 사용자별 현재 매칭 상태 저장\n",
            "   - 각 로그 이벤트를 한 번만 읽으며 패턴 매칭 상태 갱신\n",
            "   - 상태가 패턴 끝에 도달하면 매칭 결과 기록\n",
            "4. **시간 복잡도**:\n",
            "   - 전체: O(N + m)  (또는 O(N + 패턴 총길이))\n",
            "5. **공간 복잡도**:\n",
            "   - O(m + Ua + 결과 크기)  (Ua = 활성 사용자 수)\n",
            "6. **최적성 이유**:\n",
            "   - 단일 패스 O(N): 로그 수에 선형\n",
            "   - 문자열 매칭 이론에서 선형 알고리즘(KMP/Aho–Corasick)은 사실상 최선에 가까운 성능\n",
            "   - 사용자 수와 상관없이 전체 이벤트 수 기준으로 선형 시간 유지\n",
            "\n",
            "---\n",
            "\n",
            "추가로, “패턴이 더 복잡한 조건(예: 시간 간격 제약, 조건부 분기 등)”을 포함하는 경우에는 상태 기계를 조금 더 복잡하게 설계하거나, CEP(Complex Event Processing) 엔진 스타일로 NFA(Non-deterministic Finite Automaton)를 구성하는 방식까지 확장할 수 있습니다. 필요하다면 그 경우의 설계도 따로 정리해 드릴 수 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## No Reasoning 모드\n"
      ],
      "metadata": {
        "id": "amTM3_FgRFsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### No Reasoning이란?\n",
        "- reasoning_effort='none' 설정\n",
        "- 추론 과정 생략\n",
        "- GPT-5.1의 지능 유지\n",
        "- 도구 호출 성능 향상\n",
        "- 예시)\n",
        "  - 간단한 정보 조회\n",
        "  - API 문서 검색\n",
        "  - 실시간 챗봇 응답\n",
        "  - 대량 데이터 처리"
      ],
      "metadata": {
        "id": "Jc0trJCvRM1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. 실시간 애플리케이션 구축\n",
        "챗봇 예제:"
      ],
      "metadata": {
        "id": "KWmu2NhWRv6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def chatbot_response(user_message):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"당신은 고객 지원 봇입니다.\"},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        reasoning_effort=\"none\"  # 빠른 응답\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# 평균 응답 시간: <1초\n",
        "user_message = \"주문한 제품 배송 조회가 가능한가요?\"\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "doOpFw4YRxE-",
        "outputId": "6158faa9-e2fa-48f9-9807-30822db2144a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "아래에서는 “100만 개(log line) 규모에서 특정 사용자 행동 패턴(예: A → B → C 같은 이벤트 시퀀스)”을 효율적으로 탐지하는 대표적인 알고리즘 설계를 설명합니다.  \n",
            "패턴이 하나냐/여럿이냐, 패턴이 순차 패턴이냐/일반 문자열 패턴이냐에 따라 달라지지만, 가장 일반적인 케이스인 “순서 있는 이벤트 시퀀스 패턴”을 기준으로 설명하겠습니다.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. 문제 정의 정리\n",
            "\n",
            "- 입력:\n",
            "  - 로그 수: N ≈ 1,000,000 (각 로그는 `user_id, timestamp, event_type` 정도)\n",
            "  - 행동 패턴: 길이 m인 이벤트 시퀀스 `P = [e1, e2, ..., em]`  \n",
            "    예: `P = [login, view_product, add_to_cart, purchase]`\n",
            "- 목표:\n",
            "  - 각 사용자별로 타임라인에서 P가 등장하는 모든 위치(또는 존재 여부) 탐지\n",
            "- 제약:\n",
            "  - N이 크므로 최소 O(N)에 가까운 알고리즘이 바람직\n",
            "  - 메모리도 전체 로그를 모두 복제하지 않고 스트리밍 처리 가능하면 좋음\n",
            "\n",
            "---\n",
            "\n",
            "## 2. 기본 아이디어: “사용자별로 상태를 가진 유한 상태 기계(FSM)”\n",
            "\n",
            "패턴 매칭 문제를 “유한 상태 기계(또는 KMP 같은 문자열 매칭 알고리즘)”로 바꿉니다.\n",
            "\n",
            "### 2.1. 단일 패턴(한 개의 행동 시퀀스)인 경우\n",
            "\n",
            "패턴 P 길이 = m.  \n",
            "각 사용자 u에 대해 “현재 패턴에서 어디까지 맞췄는지”를 상태로 관리합니다.\n",
            "\n",
            "#### 알고리즘(스트리밍, 온라인 처리)\n",
            "\n",
            "로그를 시간순(또는 `[user_id, timestamp]` 기준 정렬)으로 한 번씩만 훑습니다.\n",
            "\n",
            "1. **사전 준비 (패턴 전처리)**  \n",
            "   - KMP(Knuth-Morris-Pratt) 방식으로 패턴 P에 대한 실패 함수(failure function)를 계산  \n",
            "   - 시간: O(m), 공간: O(m)\n",
            "\n",
            "2. **로그 스트리밍 처리**\n",
            "   - 해시맵 `state[user_id]`에 “현재 이 사용자가 패턴에서 몇 번째 위치까지 맞췄는가(0~m)”를 저장\n",
            "   - 로그 한 줄 `(u, t, e)`에 대해:\n",
            "     1. 현재 사용자 u의 상태 `k = state[u]`를 가져옴 (없으면 0으로 초기화)\n",
            "     2. KMP 점화식을 사용해 `P[k]`와 이벤트 e를 비교  \n",
            "        - 매칭되면 `k ← k + 1`\n",
            "        - 안 맞으면 실패 함수로 k를 줄이며 재시도\n",
            "     3. k가 m에 도달하면(=패턴 전체 매칭):\n",
            "        - 패턴 발견으로 기록\n",
            "        - 패턴 중 overlapping 허용 시: 실패 함수에 따라 k를 다시 조정\n",
            "     4. 갱신된 k를 `state[u]`에 저장\n",
            "\n",
            "3. **사용자 수가 매우 많을 때**\n",
            "   - 오래 사용되지 않는 `user_id`의 상태는 LRU 정책 등으로 제거해 메모리 사용 최적화 가능  \n",
            "   - 예: 최근 7일 내 로그에 등장한 사용자만 유지\n",
            "\n",
            "---\n",
            "\n",
            "## 3. 시간 복잡도\n",
            "\n",
            "### 3.1. 패턴 전처리\n",
            "\n",
            "- KMP 실패 함수 계산: O(m)\n",
            "\n",
            "### 3.2. 로그 처리\n",
            "\n",
            "- 각 로그는 딱 한 번 읽음\n",
            "- KMP의 핵심: 문자(여기서는 이벤트) 비교 횟수가 O(N + m)에 상한됨\n",
            "  - 즉, 실패 함수 덕분에 한 이벤트를 여러 번 되돌아보지 않음\n",
            "- **단일 사용자**일 때: O(N + m)\n",
            "- **여러 사용자**일 때: 사용자별 상태만 다를 뿐, 각 로그 이벤트는 KMP 전이 계산을 1~몇 번(상수배)만 수행  \n",
            "  => **전체 시간 복잡도 = O(N + m)**\n",
            "\n",
            "실제로는 N이 매우 크고 m은 상대적으로 작으므로, 실질적인 시간은 O(N)에 가깝습니다.\n",
            "\n",
            "---\n",
            "\n",
            "## 4. 공간 복잡도\n",
            "\n",
            "구성 요소별로 나눠서 보면:\n",
            "\n",
            "1. **패턴 저장 및 실패 함수**\n",
            "   - 패턴 길이 m\n",
            "   - 실패 함수 배열 길이 m\n",
            "   - => O(m)\n",
            "\n",
            "2. **사용자 상태 저장**\n",
            "   - `state[user_id]`는 각 사용자에 대해 현재 매칭 인덱스(0~m)의 정수 하나\n",
            "   - 사용자 수를 U라 하면 O(U)\n",
            "   - U가 N보다 작거나 같으므로 최악: O(N)  \n",
            "   - 메모리 최적화를 위해:\n",
            "     - 오래 안 나오는 사용자 u의 entry를 삭제 → 활성 사용자 수 Ua만 유지  \n",
            "     - 이 경우 O(Ua)이고, 일반적으로 Ua ≪ N\n",
            "\n",
            "3. **결과 저장**\n",
            "   - 패턴이 발견될 때마다 (u, t_start, t_end) 를 기록\n",
            "   - 최악: 모든 위치에서 매칭될 수 있다면 O(N)  \n",
            "   - 하지만 이 부분은 “결과가 커서 어쩔 수 없이 드는 비용”이므로 알고리즘적인 오버헤드라기보다 문제 정의상의 필수 비용입니다.\n",
            "\n",
            "**종합 공간 복잡도:**  \n",
            "- 일반적으로: O(m + Ua + 결과 크기)  \n",
            "- 최악(모든 사용자 상태 유지, 패턴/결과 포함): O(N)\n",
            "\n",
            "---\n",
            "\n",
            "## 5. 이 방법이 효율적인 이유 (최적성 직관)\n",
            "\n",
            "1. **한 번의 패스(One-pass)로 처리: O(N)**  \n",
            "   - 로그를 여러 번 돌지 않음 → 디스크 I/O, CPU 모두 절약\n",
            "   - 다중 패스 방식(예: 모든 사용자 로그를 메모리에 모아서 정렬 후 패턴 검색)보다 훨씬 효율적\n",
            "\n",
            "2. **문자열 매칭 이론에서의 최적성**  \n",
            "   - 고전적인 결과: 임의의 패턴 문자열을 임의의 텍스트에서 찾는 데, 비교 기반 알고리즘의 평균/실무 성능은 O(N) 이상이 필요\n",
            "   - KMP는 선형 시간에 동작하는 대표적인 알고리즘이며, 불필요한 되돌아보기를 하지 않아 실제 실행 시간도 안정적\n",
            "\n",
            "3. **사용자 수와 상관없이 선형 시간 유지**  \n",
            "   - 일반적인 비효율적 방법:\n",
            "     - 사용자별로 로그를 따로 모아 정렬 후 각각 패턴 매칭 → 정렬 비용 O(N log N) + 매칭 비용 O(N)\n",
            "   - 제안 알고리즘:\n",
            "     - 전체를 시간순(이미 시간순이라 가정, 아니면 처음에 한 번만 전체 O(N log N) 정렬)으로 스캔하면서, 사용자별 상태만 따로 들고 간다 → 추가 정렬 없이 O(N)  \n",
            "   - 사용자 수가 매우 커져도, 전체 이벤트 수가 N이면 계속 O(N) 유지\n",
            "\n",
            "4. **메모리 사용의 효율성과 확장성**  \n",
            "   - state는 user_id → 정수 하나의 맵이므로, 매우 작음 (예: 4 바이트 * 사용자 수)\n",
            "   - 오래 안 나오는 사용자는 제거 가능 → 실제 활성 사용자만 유지\n",
            "   - 분산 처리에서도 잘 맞음 (샤딩된 로그를 각 노드에서 동일한 FSM 로직으로 처리 후 결과만 합치면 됨)\n",
            "\n",
            "---\n",
            "\n",
            "## 6. 확장: 여러 개의 패턴을 동시에 찾고 싶을 때\n",
            "\n",
            "패턴이 여러 개라면(A, B, C… 여러 행동 시퀀스):\n",
            "\n",
            "1. 이벤트 레벨에서 여러 패턴을 동시에 찾는다면:\n",
            "   - 문자열 매칭에서는 Aho–Corasick 알고리즘 사용\n",
            "   - 여러 패턴을 트라이(trie)에 넣고, 실패 링크를 구성\n",
            "   - 한 번의 스캔으로 모든 패턴 동시 탐지\n",
            "   - 시간: O(N + 패턴 총길이)  \n",
            "   - 공간: 트라이 노드 수 만큼 O(패턴 총길이)\n",
            "\n",
            "2. 사용자별 상태와 결합:\n",
            "   - “유한 상태 기계(FSM)”가 이제 Aho–Corasick의 오토마타가 됨  \n",
            "   - `state[user_id]`는 “현재 어떤 노드(상태)에 있는가”를 저장  \n",
            "   - 나머지 로직은 단일 패턴 대비 거의 동일\n",
            "\n",
            "---\n",
            "\n",
            "## 7. 단계별 요약\n",
            "\n",
            "1. **패턴 정의**: 찾고 싶은 행동 시퀀스를 `[e1, e2, ..., em]`으로 명시\n",
            "2. **패턴 전처리**:\n",
            "   - 단일 패턴: KMP 실패 함수 O(m)\n",
            "   - 다중 패턴: Aho–Corasick 오토마타 구성 O(패턴 총길이)\n",
            "3. **로그 스트리밍 처리(핵심)**:\n",
            "   - 해시맵 `state[user_id]`에 사용자별 현재 매칭 상태 저장\n",
            "   - 각 로그 이벤트를 한 번만 읽으며 패턴 매칭 상태 갱신\n",
            "   - 상태가 패턴 끝에 도달하면 매칭 결과 기록\n",
            "4. **시간 복잡도**:\n",
            "   - 전체: O(N + m)  (또는 O(N + 패턴 총길이))\n",
            "5. **공간 복잡도**:\n",
            "   - O(m + Ua + 결과 크기)  (Ua = 활성 사용자 수)\n",
            "6. **최적성 이유**:\n",
            "   - 단일 패스 O(N): 로그 수에 선형\n",
            "   - 문자열 매칭 이론에서 선형 알고리즘(KMP/Aho–Corasick)은 사실상 최선에 가까운 성능\n",
            "   - 사용자 수와 상관없이 전체 이벤트 수 기준으로 선형 시간 유지\n",
            "\n",
            "---\n",
            "\n",
            "추가로, “패턴이 더 복잡한 조건(예: 시간 간격 제약, 조건부 분기 등)”을 포함하는 경우에는 상태 기계를 조금 더 복잡하게 설계하거나, CEP(Complex Event Processing) 엔진 스타일로 NFA(Non-deterministic Finite Automaton)를 구성하는 방식까지 확장할 수 있습니다. 필요하다면 그 경우의 설계도 따로 정리해 드릴 수 있습니다.\n"
          ]
        }
      ]
    }
  ]
}