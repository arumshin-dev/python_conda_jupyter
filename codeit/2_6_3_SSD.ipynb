{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install kaggle --upgrade"],"metadata":{"collapsed":true,"id":"Pb1BH2B04Y3D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740375132191,"user_tz":-540,"elapsed":5349,"user":{"displayName":"Harper Lee","userId":"11182007330807379238"}},"outputId":"544fe000-8f6d-48be-99da-de5066299d73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n","Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.10)\n"]}]},{"cell_type":"code","source":["!kaggle datasets download -d s076923/pytorch-transformer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RSwqyqRt2454","executionInfo":{"status":"ok","timestamp":1740375142550,"user_tz":-540,"elapsed":10355,"user":{"displayName":"Harper Lee","userId":"11182007330807379238"}},"outputId":"f412ccaf-1837-49a0-9ad5-9a8a1b9acbc1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset URL: https://www.kaggle.com/datasets/s076923/pytorch-transformer\n","License(s): other\n","Downloading pytorch-transformer.zip to /content\n"," 99% 908M/916M [00:08<00:00, 108MB/s]\n","100% 916M/916M [00:08<00:00, 111MB/s]\n"]}]},{"cell_type":"code","source":["# 먼저, 압축 파일(pytorch-transformer.zip)을 지정된 경로(현재 디렉토리)로 압축 해제합니다.\n","import shutil\n","shutil.unpack_archive(\n","    filename=\"pytorch-transformer.zip\",  # 압축 해제할 파일 경로\n","    extract_dir=\"./\",                     # 압축 해제할 디렉토리 (현재 디렉토리)\n","    format=\"zip\"                          # 압축 파일 형식 지정\n",")\n"],"metadata":{"id":"ja-TiUfB4sTS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 작업 디렉토리를 변경합니다.\n","import os\n","os.chdir(\"/content/datasets/\")  # 작업할 데이터셋 폴더로 이동 (실행 환경에 따라 경로가 달라질 수 있음)"],"metadata":{"id":"qYB1qCf24zMY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### SSD 백본(Backbone) 정의\n","먼저, ResNet과 같이 사전 학습된 네트워크의 일부 층을 사용하여 SSD의 특징 추출기(backbone)를 구성합니다."],"metadata":{"id":"jfutBazdNFcv"}},{"cell_type":"code","source":["from torch import nn\n","from collections import OrderedDict\n","\n","class SSDBackbone(nn.Module):\n","    def __init__(self, backbone):\n","        super().__init__()\n","        # backbone의 초기 층: conv1, bn1, relu를 하나의 sequential로 묶습니다.\n","        layer0 = nn.Sequential(backbone.conv1, backbone.bn1, backbone.relu)\n","        # ResNet의 레이어들 (layer1 ~ layer4)를 저장\n","        layer1 = backbone.layer1\n","        layer2 = backbone.layer2\n","        layer3 = backbone.layer3\n","        layer4 = backbone.layer4\n","\n","        # features는 layer0부터 layer3까지 연결한 것입니다.\n","        self.features = nn.Sequential(layer0, layer1, layer2, layer3)\n","\n","        # upsampling 모듈: features의 출력 채널 수를 변경하고 활성화 함수(ReLU)를 적용합니다.\n","        self.upsampling= nn.Sequential(\n","            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","        # extra 모듈: SSD에서는 다양한 크기의 특징맵(feature map)을 사용하기 위해 추가적인 계층을 만듭니다.\n","        # ModuleList에 여러 sequential 블록을 추가하여 점진적으로 다운샘플링 하면서 채널 수를 조정합니다.\n","        self.extra = nn.ModuleList(\n","            [\n","                # 첫 번째 extra block: layer4와 1x1 conv를 사용해 채널 수를 늘립니다.\n","                nn.Sequential(\n","                    layer4,\n","                    nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=1),\n","                    nn.ReLU(inplace=True),\n","                ),\n","                # 두 번째 extra block\n","                nn.Sequential(\n","                    nn.Conv2d(1024, 256, kernel_size=1),\n","                    nn.ReLU(inplace=True),\n","                    nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=2),\n","                    nn.ReLU(inplace=True),\n","                ),\n","                # 세 번째 extra block\n","                nn.Sequential(\n","                    nn.Conv2d(512, 128, kernel_size=1),\n","                    nn.ReLU(inplace=True),\n","                    nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=2),\n","                    nn.ReLU(inplace=True),\n","                ),\n","                # 네 번째 extra block\n","                nn.Sequential(\n","                    nn.Conv2d(256, 128, kernel_size=1),\n","                    nn.ReLU(inplace=True),\n","                    nn.Conv2d(128, 256, kernel_size=3),\n","                    nn.ReLU(inplace=True),\n","                ),\n","                # 다섯 번째 extra block\n","                nn.Sequential(\n","                    nn.Conv2d(256, 128, kernel_size=1),\n","                    nn.ReLU(inplace=True),\n","                    nn.Conv2d(128, 256, kernel_size=3),\n","                    nn.ReLU(inplace=True),\n","                ),\n","                # 여섯 번째 extra block: kernel_size가 4로 지정되어 다운샘플링의 정도가 다릅니다.\n","                nn.Sequential(\n","                    nn.Conv2d(256, 128, kernel_size=1),\n","                    nn.ReLU(inplace=True),\n","                    nn.Conv2d(128, 256, kernel_size=4),\n","                    nn.ReLU(inplace=True),\n","                )\n","            ]\n","        )\n","\n","    def forward(self, x):\n","        # 기본 feature 추출 (features: layer0 ~ layer3)\n","        x = self.features(x)\n","        # upsampling을 적용하여 첫 번째 출력 feature map을 만듭니다.\n","        output = [self.upsampling(x)]\n","\n","        # extra 모듈들을 순차적으로 적용하며 추가적인 feature map들을 생성합니다.\n","        for block in self.extra:\n","            x = block(x)\n","            output.append(x)\n","\n","        # 결과를 OrderedDict 형태로 반환 (각 단계별 특징맵에 index를 부여)\n","        return OrderedDict([(str(i), v) for i, v in enumerate(output)])\n"],"metadata":{"id":"VPESEP6gMlhz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- features: ResNet의 앞부분(layer0~layer3)으로 입력 이미지의 기본 특징(feature)을 추출합니다.\n","- upsampling: 기본 특징맵의 채널 수를 512로 변환합니다.\n","- extra: SSD에서 여러 크기의 특징맵을 사용하기 위한 추가 계층들로, 점진적으로 해상도를 줄이면서 채널 수를 조정합니다.\n","- forward: 입력 이미지를 차례로 features, upsampling, extra 모듈에 통과시켜 여러 수준의 특징맵을 생성한 후, 이를 OrderedDict로 반환합니다.\n"],"metadata":{"id":"u9puaW_aNKM2"}},{"cell_type":"markdown","source":["### 모델과 앵커 생성자(Anchor Generator) 초기화\n","사전 학습된 ResNet34를 기반으로 SSD 백본을 구성하고, SSD 모델 및 앵커 박스 생성자를 정의합니다."],"metadata":{"id":"jF6P7pZENOgU"}},{"cell_type":"code","source":["import torch\n","from torchvision.models import resnet34\n","from torchvision.models.detection import ssd\n","from torchvision.models.detection.anchor_utils import DefaultBoxGenerator\n","\n","# 사전 학습된 ResNet34 모델을 불러옵니다.\n","backbone_base = resnet34(weights=\"ResNet34_Weights.IMAGENET1K_V1\")\n","# 위에서 정의한 SSDBackbone으로 ResNet34의 일부 층을 래핑합니다.\n","backbone = SSDBackbone(backbone_base)\n","\n","# DefaultBoxGenerator: SSD의 앵커 박스를 생성하기 위한 객체\n","anchor_generator = DefaultBoxGenerator(\n","    aspect_ratios=[[2], [2, 3], [2, 3], [2, 3], [2, 3], [2], [2]],\n","    scales=[0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05, 1.20],\n","    steps=[8, 16, 32, 64, 100, 300, 512],\n",")\n","\n","# 사용 가능한 device(CPU 또는 GPU)를 선택합니다.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# SSD 모델 생성: backbone, 앵커 생성자, 입력 이미지 크기, 클래스 수를 지정합니다.\n","model = ssd.SSD(\n","    backbone=backbone,\n","    anchor_generator=anchor_generator,\n","    size=(512, 512),\n","    num_classes=3\n",").to(device)\n"],"metadata":{"id":"pYq9W8x8NNAo","executionInfo":{"status":"ok","timestamp":1740375181972,"user_tz":-540,"elapsed":14496,"user":{"displayName":"Harper Lee","userId":"11182007330807379238"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"60f1d49c-149d-4564-95ee-6bf7dc8d609c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n","100%|██████████| 83.3M/83.3M [00:00<00:00, 116MB/s]\n"]}]},{"cell_type":"markdown","source":["- ResNet34를 사전 학습된 가중치와 함께 불러오고, 이를 SSDBackbone으로 감싸 SSD에 맞게 구성합니다.\n","- DefaultBoxGenerator를 통해 SSD에서 사용할 여러 크기의 앵커 박스를 설정합니다.\n","- 최종적으로 SSD 모델을 초기화하고, GPU 또는 CPU로 할당합니다.\n"],"metadata":{"id":"iecoNCoWNTWr"}},{"cell_type":"markdown","source":["### COCO 데이터셋 클래스 정의\n","COCO 데이터셋의 이미지와 어노테이션(JSON 파일)을 읽어와서 모델 학습에 사용할 수 있도록 Dataset 클래스를 정의합니다."],"metadata":{"id":"xB_dolW5NWne"}},{"cell_type":"code","source":["import os\n","import torch\n","from PIL import Image\n","from pycocotools.coco import COCO\n","from torch.utils.data import Dataset\n","\n","class COCODataset(Dataset):\n","    def __init__(self, root, train, transform=None):\n","        super().__init__()\n","        # 학습/검증 데이터를 구분하여 파일 경로 설정\n","        directory = \"train\" if train else \"val\"\n","        annotations = os.path.join(root, \"annotations\", f\"{directory}_annotations.json\")\n","\n","        # COCO 어노테이션 파일을 읽어옵니다.\n","        self.coco = COCO(annotations)\n","        self.image_path = os.path.join(root, directory)\n","        self.transform = transform\n","\n","        # 카테고리 정보 및 데이터셋의 이미지-어노테이션 페어를 로드합니다.\n","        self.categories = self._get_categories()\n","        self.data = self._load_data()\n","\n","    def _get_categories(self):\n","        # COCO의 카테고리 정보를 {id: name} 형태의 딕셔너리로 저장합니다.\n","        categories = {0: \"background\"}\n","        for category in self.coco.cats.values():\n","            categories[category[\"id\"]] = category[\"name\"]\n","        return categories\n","\n","    def _load_data(self):\n","        data = []\n","        # 모든 이미지에 대해 어노테이션 정보를 로드합니다.\n","        for _id in self.coco.imgs:\n","            file_name = self.coco.loadImgs(_id)[0][\"file_name\"]\n","            image_path = os.path.join(self.image_path, file_name)\n","            image = Image.open(image_path).convert(\"RGB\")\n","\n","            boxes = []\n","            labels = []\n","            anns = self.coco.loadAnns(self.coco.getAnnIds(_id))\n","            # 각 어노테이션에서 bbox 좌표와 레이블을 추출합니다.\n","            for ann in anns:\n","                x, y, w, h = ann[\"bbox\"]\n","                boxes.append([x, y, x + w, y + h])\n","                labels.append(ann[\"category_id\"])\n","\n","            target = {\n","                \"image_id\": torch.LongTensor([_id]),\n","                \"boxes\": torch.FloatTensor(boxes),\n","                \"labels\": torch.LongTensor(labels)\n","            }\n","            data.append([image, target])\n","        return data\n","\n","    def __getitem__(self, index):\n","        image, target = self.data[index]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, target\n","\n","    def __len__(self):\n","        return len(self.data)\n"],"metadata":{"id":"DkV7fhTHNS7T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- _get_categories(): COCO 어노테이션 파일에서 카테고리 정보를 읽어와서, 배경 클래스(0번)와 함께 딕셔너리 형태로 저장합니다.\n","- _load_data(): 각 이미지 파일과 해당 이미지에 대한 어노테이션(바운딩 박스, 레이블)을 불러와서 리스트에 저장합니다.\n","- getitem(): DataLoader에서 호출될 때 이미지와 어노테이션을 반환하며, 필요시 transform을 적용합니다."],"metadata":{"id":"OwL6KxuqNbqr"}},{"cell_type":"markdown","source":["### DataLoader와 데이터 전처리\n","이미지를 텐서로 변환하는 transform과 함께 학습 및 테스트 데이터셋을 로드합니다."],"metadata":{"id":"gVn7Ys5LNepf"}},{"cell_type":"code","source":["from torchvision import transforms\n","from torch.utils.data import DataLoader\n","\n","# DataLoader가 배치 데이터를 올바르게 묶을 수 있도록 custom collator 함수 정의\n","def collator(batch):\n","    return tuple(zip(*batch))\n","\n","# 이미지 전처리: PIL 이미지를 텐서로 변환하고 데이터 타입을 float으로 변환\n","transform = transforms.Compose(\n","    [\n","        transforms.PILToTensor(),\n","        transforms.ConvertImageDtype(dtype=torch.float)\n","    ]\n",")\n","\n","# 학습 및 테스트 데이터셋 초기화 (datasets 경로는 사용자 환경에 맞게 수정 필요)\n","train_dataset = COCODataset(\"../datasets/coco\", train=True, transform=transform)\n","test_dataset = COCODataset(\"../datasets/coco\", train=False, transform=transform)\n","\n","# DataLoader: 배치 크기, 셔플링 여부, collate 함수 등을 지정\n","train_dataloader = DataLoader(\n","    train_dataset, batch_size=4, shuffle=True, drop_last=True, collate_fn=collator\n",")\n","test_dataloader = DataLoader(\n","    test_dataset, batch_size=1, shuffle=True, drop_last=True, collate_fn=collator\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_VXkZzo7NbUL","executionInfo":{"status":"ok","timestamp":1740375194459,"user_tz":-540,"elapsed":12457,"user":{"displayName":"Harper Lee","userId":"11182007330807379238"}},"outputId":"3b3b95ff-b763-446f-b998-fd7001230ebf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=0.09s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=0.01s)\n","creating index...\n","index created!\n"]}]},{"cell_type":"markdown","source":["- transform: 이미지를 텐서로 변환하여 모델에 바로 입력할 수 있도록 합니다.\n","- collator: DataLoader에서 여러 샘플을 묶어 배치를 만들 때, 이미지와 어노테이션을 별도로 묶어주는 함수입니다."],"metadata":{"id":"6kaUJZicNjLY"}},{"cell_type":"markdown","source":["### 모델 학습\n","SGD 옵티마이저와 StepLR 스케줄러를 사용하여 모델을 10 에폭(epoch) 동안 학습합니다."],"metadata":{"id":"vfjMKCf3NpFN"}},{"cell_type":"code","source":["from torch import optim\n","from tqdm import tqdm\n","\n","# 학습 가능한 파라미터만 모아서 optimizer에 전달\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n","lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n","\n","# 총 10 에폭 동안 학습\n","for epoch in range(10):\n","    train_cost = 0.0\n","    model.train()  # 학습 모드\n","    # tqdm으로 training loop 진행 상황 표시\n","    for images, targets in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n","        # 배치의 각 이미지와 어노테이션을 device(GPU/CPU)로 이동\n","        images = [image.to(device) for image in images]\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        # 모델에 입력하여 손실(loss) 계산\n","        loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        # 역전파 및 파라미터 업데이트\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        # loss.item()을 사용하여 스칼라 값을 누적\n","        train_cost += losses.item()\n","\n","    # 에폭 종료 후 학습률 갱신\n","    lr_scheduler.step()\n","    avg_train_loss = train_cost / len(train_dataloader)\n","\n","    print(f\"Epoch: {epoch+1:4d}, Train Loss: {avg_train_loss:.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t0RfOejsNjEZ","executionInfo":{"status":"ok","timestamp":1740377650052,"user_tz":-540,"elapsed":2455593,"user":{"displayName":"Harper Lee","userId":"11182007330807379238"}},"outputId":"6f89a4a1-3e2e-4b3d-e35d-e90423d58c52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Training Epoch 1: 100%|██████████| 607/607 [04:04<00:00,  2.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:    1, Train Loss: 6.339\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 2: 100%|██████████| 607/607 [04:05<00:00,  2.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:    2, Train Loss: 5.425\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 3: 100%|██████████| 607/607 [04:05<00:00,  2.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:    3, Train Loss: 5.077\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 4: 100%|██████████| 607/607 [04:05<00:00,  2.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:    4, Train Loss: 4.727\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 5: 100%|██████████| 607/607 [04:05<00:00,  2.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:    5, Train Loss: 4.387\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 6: 100%|██████████| 607/607 [04:05<00:00,  2.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:    6, Train Loss: 3.899\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 7: 100%|██████████| 607/607 [04:05<00:00,  2.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:    7, Train Loss: 3.750\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 8: 100%|██████████| 607/607 [04:05<00:00,  2.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:    8, Train Loss: 3.653\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 9: 100%|██████████| 607/607 [04:05<00:00,  2.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:    9, Train Loss: 3.559\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 10: 100%|██████████| 607/607 [04:05<00:00,  2.47it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch:   10, Train Loss: 3.460\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["- 매 배치마다 모델에 이미지와 타겟(어노테이션)을 입력하고, 손실값을 구한 뒤 역전파 및 업데이트를 수행합니다.\n","- 에폭이 끝나면 학습률을 조정하며, 에폭마다 평균 손실을 출력합니다.\n"],"metadata":{"id":"kW1W7ItaNrWx"}},{"cell_type":"markdown","source":["### 객체 검출 결과 시각화\n","테스트 데이터셋을 이용해 모델이 예측한 바운딩 박스를 시각화합니다.\n","빨간색은 모델의 예측 결과, 파란색은 실제 정답(ground truth)을 나타냅니다."],"metadata":{"id":"fGMOLFqmNs3m"}},{"cell_type":"code","source":["import numpy as np\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","from torchvision.transforms.functional import to_pil_image\n","\n","# 바운딩 박스를 그리는 함수 정의\n","def draw_bbox(ax, box, text, color):\n","    ax.add_patch(\n","        plt.Rectangle(\n","            xy=(box[0], box[1]),\n","            width=box[2] - box[0],\n","            height=box[3] - box[1],\n","            fill=False,\n","            edgecolor=color,\n","            linewidth=2,\n","        )\n","    )\n","    ax.annotate(\n","        text=text,\n","        xy=(box[0] - 5, box[1] - 5),\n","        color=color,\n","        weight=\"bold\",\n","        fontsize=13,\n","    )\n","\n","threshold = 0.5  # 예측 점수 임계값\n","categories = test_dataset.categories\n","\n","with torch.no_grad():\n","    model.eval()  # 평가 모드로 전환\n","    for images, targets in test_dataloader:\n","        images = [image.to(device) for image in images]\n","        outputs = model(images)\n","\n","        # 모델 출력에서 바운딩 박스, 레이블, 점수를 추출\n","        boxes = outputs[0][\"boxes\"].to(\"cpu\").numpy()\n","        labels = outputs[0][\"labels\"].to(\"cpu\").numpy()\n","        scores = outputs[0][\"scores\"].to(\"cpu\").numpy()\n","\n","        # 점수가 threshold 이상인 경우만 선택\n","        boxes = boxes[scores >= threshold].astype(np.int32)\n","        labels = labels[scores >= threshold]\n","        scores = scores[scores >= threshold]\n","\n","        # 이미지 시각화\n","        fig = plt.figure(figsize=(8, 8))\n","        ax = fig.add_subplot(1, 1, 1)\n","        plt.imshow(to_pil_image(images[0]))\n","\n","        # 예측 결과 (빨간색)\n","        for box, label, score in zip(boxes, labels, scores):\n","            draw_bbox(ax, box, f\"{categories[label]} - {score:.4f}\", \"red\")\n","\n","        # 실제 정답 (파란색)\n","        tboxes = targets[0][\"boxes\"].numpy()\n","        tlabels = targets[0][\"labels\"].numpy()\n","        for box, label in zip(tboxes, tlabels):\n","            draw_bbox(ax, box, f\"{categories[label]}\", \"blue\")\n","\n","        plt.show()\n"],"metadata":{"id":"S8-FAXBXNbRX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 모델 예측 결과에서 점수가 일정 임계값 이상인 박스들만 선택하여 시각화합니다.\n","- draw_bbox 함수로 각 박스와 텍스트(클래스 이름 및 점수 혹은 정답)를 그림에 표시합니다."],"metadata":{"id":"XPdrYZDjNw0M"}},{"cell_type":"markdown","source":["### COCO 평가 (COCOeval)\n","모델의 검출 성능을 COCO 평가 방식으로 계산합니다."],"metadata":{"id":"mej9Mu8KN03u"}},{"cell_type":"code","source":["import numpy as np\n","from pycocotools.cocoeval import COCOeval\n","\n","with torch.no_grad():\n","    model.eval()\n","    coco_detections = []\n","    for images, targets in test_dataloader:\n","        images = [img.to(device) for img in images]\n","        outputs = model(images)\n","\n","        for i in range(len(targets)):\n","            # 각 이미지에 대해 image_id, 박스, 점수, 레이블 추출\n","            image_id = targets[i][\"image_id\"].data.cpu().numpy().tolist()[0]\n","            boxes = outputs[i][\"boxes\"].data.cpu().numpy()\n","            # COCO 형식에 맞게 박스 좌표 (x, y, w, h)로 변환\n","            boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n","            boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n","            scores = outputs[i][\"scores\"].data.cpu().numpy()\n","            labels = outputs[i][\"labels\"].data.cpu().numpy()\n","\n","            for instance_id in range(len(boxes)):\n","                box = boxes[instance_id, :].tolist()\n","                prediction = np.array(\n","                    [\n","                        image_id,\n","                        box[0],\n","                        box[1],\n","                        box[2],\n","                        box[3],\n","                        float(scores[instance_id]),\n","                        int(labels[instance_id]),\n","                    ]\n","                )\n","                coco_detections.append(prediction)\n","    coco_detections = np.asarray(coco_detections)\n","\n","    # COCO ground truth와 검출 결과를 로드하여 평가를 수행\n","    coco_gt = test_dataloader.dataset.coco\n","    coco_dt = coco_gt.loadRes(coco_detections)\n","    coco_evaluator = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n","    coco_evaluator.evaluate()\n","    coco_evaluator.accumulate()\n","    coco_evaluator.summarize()\n"],"metadata":{"id":"8i1PO4K3Nwwc","executionInfo":{"status":"ok","timestamp":1740377740294,"user_tz":-540,"elapsed":7649,"user":{"displayName":"Harper Lee","userId":"11182007330807379238"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"722d7ac7-f8a0-4603-e379-ba6af6a65625"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading and preparing results...\n","Converting ndarray to lists...\n","(27088, 7)\n","0/27088\n","DONE (t=0.27s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=0.55s).\n","Accumulating evaluation results...\n","DONE (t=0.18s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.198\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.472\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.115\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.047\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.233\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.192\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.298\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.424\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.438\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.050\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.381\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.490\n"]}]},{"cell_type":"markdown","source":["### 백본 출력 채널 확인 함수\n","모델의 백본에서 각 단계별 출력 채널 수를 확인하기 위한 헬퍼 함수입니다."],"metadata":{"id":"TuDYaWYnN5VY"}},{"cell_type":"code","source":["def retrieve_out_channels(model, size):\n","    model.eval()  # 평가 모드로 전환\n","    with torch.no_grad():\n","        device = next(model.parameters()).device\n","        # 더미 이미지 생성: 지정한 사이즈로 0으로 채워진 텐서\n","        image = torch.zeros((1, 3, size[1], size[0]), device=device)\n","        features = model(image)\n","\n","        # features가 Tensor인 경우 OrderedDict 형태로 변환\n","        if isinstance(features, torch.Tensor):\n","            features = OrderedDict([(\"0\", features)])\n","        # 각 특징맵의 채널 수를 리스트로 반환\n","        out_channels = [x.size(1) for x in features.values()]\n","\n","    model.train()\n","    return out_channels\n","\n","# 백본의 각 출력 채널 수 출력 (예: [512, 1024, 512, 256, ...])\n","print(retrieve_out_channels(backbone, (512, 512)))\n"],"metadata":{"id":"WNJWEpucN4vL","executionInfo":{"status":"ok","timestamp":1740377740306,"user_tz":-540,"elapsed":14,"user":{"displayName":"Harper Lee","userId":"11182007330807379238"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3ca346b7-4330-41b9-b472-22e73a079bd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[512, 1024, 512, 256, 256, 256, 256]\n"]}]},{"cell_type":"markdown","source":["- 더미 입력 이미지를 백본에 통과시켜서 각 단계에서 생성되는 특징맵의 채널 수(Depth)를 반환합니다.\n","- 이 함수는 모델의 구조를 이해하거나 디버깅할 때 유용합니다.\n"],"metadata":{"id":"WLNmcTi0Nwsg"}},{"cell_type":"code","source":["cnt = 0\n","threshold = 0.5  # 예측 점수 임계값\n","categories = test_dataset.categories\n","\n","with torch.no_grad():\n","    model.eval()  # 평가 모드로 전환\n","    for images, targets in test_dataloader:\n","        images = [image.to(device) for image in images]\n","        outputs = model(images)\n","\n","        # 모델 출력에서 바운딩 박스, 레이블, 점수를 추출\n","        boxes = outputs[0][\"boxes\"].to(\"cpu\").numpy()\n","        labels = outputs[0][\"labels\"].to(\"cpu\").numpy()\n","        scores = outputs[0][\"scores\"].to(\"cpu\").numpy()\n","\n","        # 점수가 threshold 이상인 경우만 선택\n","        boxes = boxes[scores >= threshold].astype(np.int32)\n","        labels = labels[scores >= threshold]\n","        scores = scores[scores >= threshold]\n","\n","        # 이미지 시각화\n","        fig = plt.figure(figsize=(8, 8))\n","        ax = fig.add_subplot(1, 1, 1)\n","        plt.imshow(to_pil_image(images[0]))\n","\n","        # 예측 결과 (빨간색)\n","        for box, label, score in zip(boxes, labels, scores):\n","            draw_bbox(ax, box, f\"{categories[label]} - {score:.4f}\", \"red\")\n","\n","        # 실제 정답 (파란색)\n","        tboxes = targets[0][\"boxes\"].numpy()\n","        tlabels = targets[0][\"labels\"].numpy()\n","        for box, label in zip(tboxes, tlabels):\n","            draw_bbox(ax, box, f\"{categories[label]}\", \"blue\")\n","\n","        plt.show()\n","\n","        cnt += 1\n","        if cnt == 5 :\n","            break"],"metadata":{"id":"L5ioTQi2NbMM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Jl5l2Nu_oUee"},"execution_count":null,"outputs":[]}]}