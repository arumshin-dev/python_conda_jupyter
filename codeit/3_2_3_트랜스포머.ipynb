{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumshin-dev/python_conda_jupyter/blob/main/codeit/3_2_3_%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%89%E1%85%B3%E1%84%91%E1%85%A9%E1%84%86%E1%85%A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# íŠ¸ëœìŠ¤í¬ë¨¸\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "rfCaBMnpcqAz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acW2D3ifHfgr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer ëª¨ë¸ êµ¬ì¡°\n",
        "\n",
        "TransformerëŠ” Seq2seqì™€ ë¹„ìŠ·í•˜ê²Œ ê¸°ê³„ë²ˆì—­ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ì¸ì½”ë”ì™€ ë””ì½”ë”êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. Seq2seqì™€ëŠ” ë‹¬ë¦¬ ì¸ì½”ë” ì™€ ë””ì½”ë” ë‚´ë¶€ì—ëŠ” MultiHeadAttention ë¸”ë¡ê³¼ FeedForwaedë¼ëŠ” ë¸”ë¡ìœ¼ë¡œ êµ¬ì„±\n",
        "\n",
        "<center><img src=\"https://arxiv.org/html/1706.03762v7/extracted/1706.03762v7/Figures/ModalNet-21.png\" width=\"300\"/></center>\n",
        "\n",
        "ì¶œì²˜: `https://arxiv.org/abs/1706.03762`"
      ],
      "metadata": {
        "id": "GojRdH_Vdxmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xnPJwvMq-xFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ğŸ§© Transformer êµ¬í˜„ ë‹¨ê³„ë³„ ê°€ì´ë“œ\n",
        "\n",
        "---\n",
        "\n",
        "## âš™ï¸ 1ë‹¨ê³„: *Scaled Dot-Product Attention*\n",
        "íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì‹¬ì¥ì¸ **Attention(Q, K, V)** ê³µì‹ì„ ì½”ë“œë¡œ ì§ì ‘ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "\n",
        "$$\n",
        "Attention(Q, K, V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "\n",
        "- Query(Q), Key(K), Value(V) ë²¡í„° ê°„ ìœ ì‚¬ë„ ê³„ì‚°  \n",
        "- Softmaxë¡œ ì¤‘ìš”ë„ë¥¼ êµ¬í•˜ê³ , Valueì— ê°€ì¤‘í•©  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§  2ë‹¨ê³„: *Multi-Head Attention*\n",
        "1ë‹¨ê³„ì—ì„œ ë§Œë“  ì–´í…ì…˜ì„ ì—¬ëŸ¬ ê°œ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬ â€œë‹¤ì–‘í•œ ê´€ì â€ì—ì„œ ë¬¸ì¥ì„ ë°”ë¼ë³¼ ìˆ˜ ìˆë„ë¡ í™•ì¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "- ì—¬ëŸ¬ í—¤ë“œ(`num_heads`)ë¥¼ í†µí•´ ë³‘ë ¬ ì–´í…ì…˜ ìˆ˜í–‰  \n",
        "- ê° í—¤ë“œì˜ ì¶œë ¥ì„ ê²°í•©(concat) í›„ ì„ í˜• ë³€í™˜  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ›¡ï¸ 3ë‹¨ê³„: *Padding & Look-Ahead Masks*\n",
        "ì–´í…ì…˜ì´ ë¶ˆí•„ìš”í•œ `<pad>` í† í°ì„ ë¬´ì‹œí•˜ê³ , ë””ì½”ë”ê°€ ë¯¸ë˜ ë‹¨ì–´ë¥¼ **ë¯¸ë¦¬ ì—¿ë³´ì§€ ëª»í•˜ê²Œ** í•©ë‹ˆë‹¤.\n",
        "\n",
        "- **Padding Mask** â†’ `<pad>` ìœ„ì¹˜ë¥¼ ê°€ë ¤ ê³„ì‚° ì œì™¸  \n",
        "- **Look-Ahead Mask** â†’ ë””ì½”ë”ê°€ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ë³´ì§€ ëª»í•˜ë„ë¡ ì œí•œ  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ—ï¸ 4ë‹¨ê³„: ì¸ì½”ë” ìœ ë‹› ì¡°ë¦½ â€” *EncoderLayer*\n",
        "**MultiHeadAttention + FeedForwardNetwork + Add & Norm**\n",
        "\n",
        "- Self-Attention â†’ FFN  \n",
        "- Residual Connection + LayerNorm  \n",
        "- ì¸ì½”ë”ì˜ ê¸°ë³¸ ë ˆì´ì–´ êµ¬ì¡° ì™„ì„±  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§© 5ë‹¨ê³„: ë””ì½”ë” ìœ ë‹› ì¡°ë¦½ â€” *DecoderLayer*\n",
        "ì¸ì½”ë”ë³´ë‹¤ ë³µì¡í•œ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.\n",
        "\n",
        "1. Masked Multi-Head Attention (ìê¸° ì–´í…ì…˜)  \n",
        "2. Encoderâ€“Decoder Attention (ì¸ì½”ë” ì¶œë ¥ê³¼ì˜ ì–´í…ì…˜)  \n",
        "3. Feed Forward Network  \n",
        "\n",
        "ê° ë¶€ë¶„ë§ˆë‹¤ Dropout, Residual, LayerNormì´ ì ìš©ë©ë‹ˆë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§± 6ë‹¨ê³„: ìµœì¢… ëª¨ë¸ ì¡°ë¦½ â€” *Encoder, Decoder, Transformer*\n",
        "4, 5ë‹¨ê³„ì—ì„œ ë§Œë“  ì¸µì„ Nê°œì”© ìŒ“ê³ , **Positional Encoding**ì„ ì¶”ê°€í•´ ìˆœì„œ ì •ë³´ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.\n",
        "\n",
        "- Encoder: ì…ë ¥ ë¬¸ì¥ ì¸ì½”ë”©  \n",
        "- Decoder: íƒ€ê¹ƒ ë¬¸ì¥ ìƒì„±  \n",
        "- Transformer: Encoder + Decoder ê²°í•© ëª¨ë¸ ì™„ì„±  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ—‚ï¸ 7ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ â€” *Tokenizer & Dataset*\n",
        "í•œêµ­ì–´ ì±—ë´‡ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ SentencePiece í† í¬ë‚˜ì´ì € í•™ìŠµ\n",
        "\n",
        "- SentencePieceë¡œ ë‹¨ì–´ ë¶„ì ˆ  \n",
        "- í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜  \n",
        "- Padding ì ìš© â†’ `PyTorch Dataset` êµ¬ì„±  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§® 8ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ â€” *Training Loop*\n",
        "Transformer í•™ìŠµì„ ìœ„í•œ í•™ìŠµ ë£¨í”„ êµ¬ì„±\n",
        "\n",
        "- **ëª¨ë¸**: Transformer  \n",
        "- **Optimizer**: Adam  \n",
        "- **Loss Function**: CrossEntropyLoss(ignore_index=pad_token)  \n",
        "- **í›ˆë ¨ ë°˜ë³µ**: forward â†’ loss â†’ backward â†’ optimizer.step()\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§ª 9ë‹¨ê³„: ì¶”ë¡  ë° í‰ê°€ â€” *Generation & BLEU*\n",
        "í•™ìŠµëœ ëª¨ë¸ì´ ë¬¸ì¥ì„ **ìŠ¤ìŠ¤ë¡œ ìƒì„±(Generate)** í•˜ë„ë¡ í•œ í›„, ì‹¤ì œ ì •ë‹µê³¼ ë¹„êµí•˜ì—¬ BLEU ì ìˆ˜ë¡œ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
        "\n",
        "- `generate()` í•¨ìˆ˜ë¡œ ë²ˆì—­ ë¬¸ì¥ ìƒì„±  \n",
        "- **BLEU Score**: ìƒì„±ëœ ë¬¸ì¥ê³¼ ì •ë‹µ ë¬¸ì¥ì˜ n-gram ìœ ì‚¬ë„ í‰ê°€  \n"
      ],
      "metadata": {
        "id": "2AU9WlLo8s8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaled Dot-Product Attention\n",
        "\n",
        "ë¨¼ì € MultiHeadAttentionì´ êµ¬ì„±ë˜ê¸° ìœ„í•´ì„œëŠ” ë‚´ë¶€ì ìœ¼ë¡œ Scaled Dot-Product Attention ì—°ì‚°ì´ ì§„í–‰ì´ ë˜ì–´ì•¼ í•¨.  \n",
        "\n",
        "Scaled Dot-Product AttentionëŠ” Queryì™€ Keyë¥¼ **ë‹¨ìˆœ ë‚´ì **í•˜ì—¬ ìŠ¤ì½”ì–´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "\n",
        "<center><img src=\"https://arxiv.org/html/1706.03762v7/extracted/1706.03762v7/Figures/ModalNet-19.png\" width=\"200\"/></center>\n",
        "\n",
        "ì¶œì²˜: `https://arxiv.org/abs/1706.03762`\n",
        "\n",
        "\n",
        "$${Attention}(Q, K, V) = \\text{softmax}\\Bigl(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigr) \\, V$$\n",
        "\n",
        "- Queryì™€ Key ê°„ì˜ ë‹¨ìˆœ ë‚´ì ì„ í™œìš©í•˜ë¯€ë¡œ, ëŒ€ê·œëª¨ ë³‘ë ¬ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ê³  ì‹œí€€ìŠ¤ ì „ì—­ì— ê±¸ì³ ë¹ ë¥´ê²Œ Attention ê³„ì‚°ì„ ìˆ˜í–‰\n",
        "- ì°¨ì›ì´ ì»¤ì§€ëŠ” ë¬¸ì œë¥¼ ìŠ¤ì¼€ì¼ë§(âˆšd_k)ìœ¼ë¡œ ë³´ì •í•¨ìœ¼ë¡œì¨, ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì´ ê°€ëŠ¥\n",
        "\n",
        "ì´ë•Œ TransformerëŠ” Scaled Dot-Product Attentionì˜  Qurey, Key, Valueë¥¼ ì „ë¶€ ë™ì¼í•œ í…ìŠ¤íŠ¸(ì„ë² ë”©)ì„ ë„£ì–´ ë¬¸ì¥ ë‚´ì˜ ëª¨ë“  ë‹¨ì–´(í† í°) ê°„ ì˜ì¡´ ê´€ê³„ë¥¼ ë™ì‹œì— í•™ìŠµí•˜ëŠ” **Self-Attention** ê¸°ë²•ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1xpFv-xHu2BnFLkaboNybk5KkQ_vgpwO-\" width=\"600\"/></center>\n"
      ],
      "metadata": {
        "id": "1Y8q_oXjhc1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Dot Product: Queryê°€ ëª¨ë“  Keyì™€ ì–¼ë§ˆë‚˜ â€œìœ ì‚¬í•œì§€(=ì—°ê´€ì„±)â€ë¥¼ ë‚˜íƒ€ëƒ„\n",
        "\n",
        "    ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì—ì„œ ì‚¬ìš©ë  ê²½ìš°\n",
        "    q, k, v : shape = (batch_size, n_heads, seq_len, depth)\n",
        "    mask    : shape = (batch_size, 1, seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    # q, k, v: (batch_size, seq_len, em_dim)\n",
        "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))    # Matrix Multiplication (í–‰ë ¬ ê³±ì…ˆ)\n",
        "\n",
        "    # ìŠ¤ì¼€ì¼ë§: ë‚´ì ê°’ì´ ì°¨ì› ìˆ˜ì— ë”°ë¼ ì»¤ì§€ì§€ ì•Šë„ë¡ âˆšdkë¡œ ë‚˜ëˆ„ì–´ ì•ˆì •í™”í•˜ëŠ” ê³¼ì •\n",
        "    dk = q.size(-1)                                     # Query í…ì„œì˜ ë§ˆì§€ë§‰ ì°¨ì›\n",
        "    scaled_attention_logits = matmul_qk / math.sqrt(dk) # ìŠ¤ì½”ì–´ë¥¼ ì•ˆì •í™”(ì •ê·œí™”)\n",
        "\n",
        "    # ë§ˆìŠ¤í¬ ì ìš©(ë©€í‹°í—¤ë“œ ì–´í…ì…˜)\n",
        "    if mask is not None:\n",
        "        # ë§ˆìŠ¤í¬ê°€ Falseì¸ ë¶€ë¶„ì— ë§¤ìš° í° ìŒìˆ˜(-1e9)ë¥¼ ë”í•´ softmaxì—ì„œ ì œì™¸ë˜ë„ë¡ í•¨\n",
        "        scaled_attention_logits = scaled_attention_logits.masked_fill(mask == False, float('-1e9'))\n",
        "\n",
        "    # ì†Œí”„íŠ¸ë§¥ìŠ¤\n",
        "    attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
        "\n",
        "    # Valueì™€ ê³±í•´ì„œ ìµœì¢… ì–´í…ì…˜ ê²°ê³¼\n",
        "    output = torch.matmul(attention_weights, v)       # Matrix Multiplication (í–‰ë ¬ ê³±ì…ˆ)\n",
        "    return output, attention_weights\n",
        "\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "x = torch.randn(3, 20, 64)\n",
        "out, atw = scaled_dot_product_attention(x, x, x, mask=None)\n",
        "# context vector(ë¬¸ë§¥ ë²¡í„°): ì–´í…ì…˜ì„ í†µê³¼í•œ ìµœì¢… ì¶œë ¥ê°’\n",
        "print(f'attention output shape : {out.shape}')\n",
        "# ì–´í…ì…˜ ê°€ì¤‘ì¹˜: ë‹¨ì–´ê°€ ë¬¸ì¥ ì•ˆì˜ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì— ì–¼ë§ˆë‚˜ ì§‘ì¤‘í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„. ì–´í…ì…˜ ìŠ¤ì½”ì–´(ìœ ì‚¬ë„)ë¥¼ Softmaxë¥¼ í†µí•´ í™•ë¥ ë¡œ ë‚˜íƒ€ëƒ„\n",
        "print(f'attention weight shape : {atw.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzDG0JWAHp3n",
        "outputId": "6556893a-396d-4b8d-cb3d-6e7f99b56ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention output shape : torch.Size([3, 20, 64])\n",
            "attention weight shape : torch.Size([3, 20, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiHead Attention\n",
        "\n",
        "TransformerëŠ” Scaled Dot-Product Attentionì„ ëª¨ë“  ì„ë² ë”© ì°¨ì›ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì„ë² ë”© ì°¨ì›ì„ íŠ¹ì • ê°œìˆ˜ë¡œ ë¶„í• í•˜ì—¬ Scaled Dot-Product Attentionì²˜ë¦¬ë¥¼ í•©ë‹ˆë‹¤.  \n",
        "\n",
        "MultiHead Attention  ì„ë² ë”© ì°¨ì›ì„ ì—¬ëŸ¬ê°œì˜ Headë¡œ ë¶„í• í•˜ì—¬ Scaled Dot-Product Attentionì²˜ë¦¬ë¥¼ í•  ë¿ë§Œ ì•„ë‹ˆë¼ ê° Q,K,Vì— ì„ í˜•ë ˆì´ì–´ë¥¼ ì ìš©í•˜ì—¬ í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë¥¼ í• ë‹¹í•´ì£¼ê³  Attention ì¶œë ¥ ì°¨ì›ì—ë„ ì„ í˜•ë ˆì´ì–´ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1YjvM96xK4L3TMWeBfVmDQzTjC_cWZCZs\" width=\"800\"/></center>\n",
        "\n",
        "ì´ëŸ° ë©€í…Œí—¤ë“œ ì–´í…ì…˜ì„ ì ìš©í•¨ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ì–»ìŠµë‹ˆë‹¤.\n",
        "- HeadëŠ” ì„œë¡œ ë‹¤ë¥¸ (ì„ë² ë”©)ë¶€ë¶„ ê³µê°„ì—ì„œ Q, K, Vë¥¼ í•™ìŠµí•˜ì—¬ ë‹¤ì–‘í•œ ê´€ì (íŒ¨í„´, ì—°ê´€ì„±)ì„ ë™ì‹œì— í¬ì°©\n",
        "- ë³‘ë ¬ë¡œ Attentionì„ ìˆ˜í–‰í•˜ì—¬ ë³´ë‹¤ ë³µì¡í•˜ê³  ì„¸ë°€í•œ ê´€ê³„ë¥¼ í•™ìŠµ\n",
        "- Headë“¤ì˜ Attention ê²°ê³¼ë¥¼ ê²°í•©(Summarize)í•¨ìœ¼ë¡œì¨ ë³´ë‹¤ ì•ˆì •ì ì´ê³  ê· í˜• ì¡íŒ ì •ë³´ë¥¼ ì–»ìŒ\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "semfYDYApxCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, em_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()        # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™” í•¨ìˆ˜ í˜¸ì¶œ\n",
        "        self.num_heads = num_heads      # í—¤ë“œ ê°¯ìˆ˜\n",
        "        self.em_dim = em_dim            # ì„ë² ë”© ì°¨ì›\n",
        "\n",
        "        # í—¤ë“œ ë¶„í• ì´ ë¶ˆê°€ëŠ¥ í•˜ë©´ ì—ëŸ¬\n",
        "        # ì„ë² ë”© ì°¨ì›ì´ í—¤ë“œ ê°œìˆ˜ë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ëŠ”ì§€ í™•ì¸ (í—¤ë“œë³„ ë™ì¼ ì°¨ì› ë³´ì¥)\n",
        "        assert em_dim % num_heads == 0, \"em_dim must be divisible by num_heads\"\n",
        "\n",
        "        # ë¶„ë¦¬ëœ ì„ë² ë”© ê¸¸ì´\n",
        "        self.depth = em_dim // num_heads\n",
        "\n",
        "        # ì„ í˜•ë ˆì´ì–´\n",
        "        self.wq = nn.Linear(em_dim, em_dim)\n",
        "        self.wk = nn.Linear(em_dim, em_dim)\n",
        "        self.wv = nn.Linear(em_dim, em_dim)\n",
        "\n",
        "        self.dense = nn.Linear(em_dim, em_dim)\n",
        "\n",
        "    # í—¤ë“œ ë¶„í•  í•¨ìˆ˜: ì…ë ¥ ì„ë² ë”©ì„ ì—¬ëŸ¬ ê°œì˜ â€œí—¤ë“œ(head)â€ë¡œ ë‚˜ëˆ„ëŠ” ì—­í• \n",
        "    # ìµœì¢… ì¶œë ¥ shape: (batch_size, num_heads, seq_len, depth)\n",
        "    def split_heads(self, x):\n",
        "        '''\n",
        "            ì˜ˆ)\n",
        "            [------512ì°¨ì›------]\n",
        "            â†“ ë¶„í•  (8ê°œ í—¤ë“œ)\n",
        "            [64][64][64][64][64][64][64][64]\n",
        "        '''\n",
        "        batch_size, seq_len, em_dim = x.size()                      # ì˜ˆ: (32, 20, 512) (batch_size, seq_len, em_dim)\n",
        "        x = x.view(batch_size, seq_len, self.num_heads, self.depth) # ì˜ˆ: (32, 20, 512) â†’ (32, 20, 8, 64) (batch_size, seq_len, num_heads, depth)\n",
        "        x = x.permute(0, 2, 1, 3)                                   # ì˜ˆ: (32, 20, 8, 64) â†’ (32, 8, 20, 64) (batch_size, num_heads, seq_len, depth)\n",
        "        return x\n",
        "\n",
        "    def forward(self, v, k, q, mask=None):\n",
        "        \"\"\"\n",
        "        v, k, q: (batch_size, seq_len, em_dim)\n",
        "        mask   : (batch_size, 1, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # q,k,v ê°€ì¤‘ì¹˜ í•™ìŠµ: (batch_size, seq_len, em_dim)\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        # q, k, v í—¤ë“œ ë¶„í•  (ê° í—¤ë“œë³„ë¡œ ë‚˜ëˆ”)\n",
        "        # ë‚´ë¶€ ë™ì‘: (batch, seq_len, em_dim) â†’ (batch, seq_len, num_heads, depth) â†’ (batch, num_heads, seq_len, depth)\n",
        "        q = self.split_heads(q)\n",
        "        k = self.split_heads(k)\n",
        "        v = self.split_heads(v)\n",
        "\n",
        "        # ì–´í…ì…˜ ê³„ì‚° ì…ë ¥ shape: (batch_size, num_heads, seq_len, depth)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        # ì–´í…ì…˜ ê²°ê³¼ë¥¼ ë‹¤ì‹œ (batch_size, seq_len, em_dim)ë¡œ ë˜ëŒë¦¼\n",
        "        scaled_attention = scaled_attention.permute(0, 2, 1, 3).contiguous()  # (batch_size, seq_len, num_heads, depth)\n",
        "        concat_attention = scaled_attention.view(batch_size, -1, self.em_dim) # (batch_size, seq_len, em_dim)\n",
        "\n",
        "        # ì—¬ëŸ¬ í—¤ë“œì˜ ê²°ê³¼ë¥¼ í•˜ë‚˜ë¡œ ê²°í•©í•˜ê³ , í•™ìŠµ ê°€ëŠ¥í•œ ì„ í˜• ë³€í™˜ì„ í†µí•´ ìµœì¢… ì–´í…ì…˜ ì¶œë ¥ì„ ë§Œë“œëŠ” ë‹¨ê³„\n",
        "        output = self.dense(concat_attention)\n",
        "        return output, attention_weights\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "x = torch.randn(2, 10, 64)                      # (batch=2, seq_len=10, em_dim=64)\n",
        "mh = MultiHeadAttention(64, 2)                  # (ì„ë² ë”© ì°¨ì› 64, í—¤ë“œ 2ê°œ)\n",
        "out, atw = mh(x,x,x)\n",
        "print(f'attention output shape : {out.shape}')\n",
        "print(f'attention weight shape : {atw.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HB_0ZIyIx5-",
        "outputId": "e50851df-5874-425b-e014-5604ef0c4b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention output shape : torch.Size([2, 10, 64])\n",
            "attention weight shape : torch.Size([2, 2, 10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MultiHeadAttention ì°¨ì› ë³€í™”\n",
        "\n",
        "| ë‹¨ê³„                             | ì—°ì‚° / í•¨ìˆ˜                                       | í…ì„œ shape                                          | ì„¤ëª…                             |\n",
        "| ------------------------------ | --------------------------------------------- | ------------------------------------------------- | ------------------------------ |\n",
        "| â‘  ì…ë ¥                           | `x = torch.randn(2, 10, 64)`                  | **(batch=2, seq_len=10, em_dim=64)**              | 2ê°œ ë¬¸ì¥, ê° ë¬¸ì¥ ê¸¸ì´ 10, ì„ë² ë”© ì°¨ì› 64   |\n",
        "| â‘¡ ì„ í˜• ë³€í™˜                        | `wq(x), wk(x), wv(x)`                         | (2, 10, 64)                                       | ê° í† í°ì„ Query/Key/Value ê³µê°„ìœ¼ë¡œ íˆ¬ì˜  |\n",
        "| â‘¢ í—¤ë“œ ë¶„í•  (view)                 | `.view(batch, seq_len, num_heads, depth)`     | (2, 10, 2, 32)                                    | 64ì°¨ì›ì„ 2í—¤ë“œë¡œ ë¶„í•  (depth = 32)     |\n",
        "| â‘£ ì¶• ì¬ë°°ì—´ (permute)              | `.permute(0, 2, 1, 3)`                        | **(2, 2, 10, 32)**                                | í—¤ë“œ ì°¨ì›ì„ ì•ìœ¼ë¡œ ì´ë™                  |\n",
        "| â‘¤ Scaled Dot-Product Attention | `scaled_dot_product_attention(q, k, v, mask)` | **ì¶œë ¥:** (2, 2, 10, 32)<br>**ê°€ì¤‘ì¹˜:** (2, 2, 10, 10) | í—¤ë“œë³„ ì–´í…ì…˜ ê³„ì‚° (ê°€ì¤‘ì¹˜: QueryÃ—Key ê´€ê³„) |\n",
        "| â‘¥ ì¶• ì¬ë°°ì—´ (permute back)         | `.permute(0, 2, 1, 3)`                        | (2, 10, 2, 32)                                    | ë‹¤ì‹œ seq_len ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬             |\n",
        "| â‘¦ í—¤ë“œ ê²°í•© (view)                 | `.view(batch, seq_len, em_dim)`               | **(2, 10, 64)**                                   | ì—¬ëŸ¬ í—¤ë“œë¥¼ concatí•˜ì—¬ ì›ë˜ ì„ë² ë”© ì°¨ì› ë³µì›   |\n",
        "| â‘§ ìµœì¢… ì„ í˜• ë³€í™˜                     | `self.dense(concat_attention)`                | **(2, 10, 64)**                                   | ëª¨ë“  í—¤ë“œ ê²°ê³¼ë¥¼ í†µí•©í•´ ìµœì¢… ì¶œë ¥ ìƒì„±         |\n"
      ],
      "metadata": {
        "id": "FO-CN9Jv3xWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> pytorchì—ëŠ” ë©€í‹°í—¤ë“œì–´í…ì…˜ì„ êµ¬í˜„í•œ nn.MultiheadAttention ë ˆì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "ZzktRwklw3bZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì–´í…ì…˜ ë§ˆìŠ¤í¬\n",
        "\n",
        "TransformerëŠ” í•™ìŠµ íš¨ìœ¨ì„ ìœ„í•´ í¬ê²Œ ë‘ê°€ì§€ì˜ ë§ˆìŠ¤í¬ë¥¼ í™œìš©í•˜ì—¬ ì—°ì‚°ì˜ íš¨ìœ¨ì„ ì–»ìŠµë‹ˆë‹¤.\n",
        "\n",
        "- padding mask\n",
        "    * í† í° ê¸¸ì´(seq_len)ì— ëª» ë¯¸ì³ íŒ¨ë”©ì´ ëœ ê²½ìš° ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ê°€ ì—°ì‚°ì— ì ìš©ë˜ì§€ ì•Šê²Œ í•˜ê¸° ìœ„í•œ ë§ˆìŠ¤í¬\n",
        "    * ì¸ì½”ë”ì™€ ë””ì½”ë”ì—ì„œ ë‘˜ë‹¤ ì‚¬ìš©\n",
        "\n",
        "- look-ahead mask(ë¯¸ë¦¬ë³´ê¸° ë°©ì§€ ë§ˆìŠ¤í¬)   \n",
        "    * ë””ì½”ë”ì˜ ìê¸°íšŒê·€(attention) ê³¼ì •ì—ì„œ ë¯¸ë˜ í† í°ì„ ì°¸ì¡°í•˜ì§€ ëª»í•˜ë„ë¡ ë§‰ëŠ” ê²ƒ   \n",
        "    * í•™ìŠµ ê³¼ì •ì—ì„œ ë””ì½”ë”ê°€ ë¯¸ë˜ í† í°ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ í•™ìŠµì„ ì§„í–‰ í• ë•Œ ë¼ë²¨ì—ì„œ ë¯¸ë˜ í† í°ì„ ë§ˆìŠ¤í¬ ì²˜ë¦¬í•˜ì—¬ ì§€ì›€    \n",
        "    * ì´ë¥¼ í†µí•´ ë””ì½”ë”ê°€ ë°˜ë³µì ì¸ í•™ìŠµ ì—†ì´ í•œë²ˆì˜ ë³‘ë ¬ ì—°ì‚°ìœ¼ë¡œ ì²˜ë¦¬ë¨   \n",
        "    * Causal Mask ìš©ì–´ë¡œ í‘œí˜„í•˜ê¸°ë„ í•¨"
      ],
      "metadata": {
        "id": "vF922LCRKRrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# íŒ¨ë”©ëœ í† í°(pad)ì´ ìˆëŠ” ë¶€ë¶„ì€ ì–´í…ì…˜ ê³„ì‚°ì—ì„œ ë¬´ì‹œ\n",
        "def create_padding_mask(seq, pad_token=0):\n",
        "  '''\n",
        "    # ì˜ˆ: seq = tensor([[7, 6, 2, 0, 0]])\n",
        "    # pad_token = 0 â†’ 0ì¸ ìœ„ì¹˜ëŠ” íŒ¨ë”©ì„\n",
        "\n",
        "    # (seq != 0) â†’ tensor([[True, True, True, False, False]])\n",
        "    # unsqueeze(1) â†’ (batch_size, 1, seq_len)\n",
        "    # unsqueeze(2) â†’ (batch_size, 1, 1, seq_len)\n",
        "    # True: ì‹¤ì œ ë‹¨ì–´, False: íŒ¨ë”© ìœ„ì¹˜\n",
        "    # ì˜ˆì‹œ ê²°ê³¼: [[[[1, 1, 1, 0, 0]]]]\n",
        "  '''\n",
        "  mask = (seq != pad_token).unsqueeze(1).unsqueeze(2)\n",
        "  # (batch_size, 1, 1, seq_len)\n",
        "  return mask\n",
        "\n",
        "# ë””ì½”ë”ì—ì„œ ë¯¸ë˜ ë‹¨ì–´ë¥¼ ë³´ì§€ ëª»í•˜ê²Œ(look ahead ë°©ì§€): ì‚¼ê°í–‰ë ¬ë¡œ ë§ˆìŠ¤í¬ í–‰ë ¬ êµ¬í˜„\n",
        "def create_look_ahead_mask(size):\n",
        "  '''\n",
        "    ì˜ˆ: size = 5 â†’ ë¬¸ì¥ ê¸¸ì´(seq_len)ê°€ 5ì¸ ê²½ìš°\n",
        "\n",
        "    # torch.tril â†’ í•˜ì‚¼ê°(lower triangle) ë¶€ë¶„ë§Œ Trueë¡œ ìœ ì§€\n",
        "    # [[1,0,0,0,0],\n",
        "    #  [1,1,0,0,0],\n",
        "    #  [1,1,1,0,0],\n",
        "    #  [1,1,1,1,0],\n",
        "    #  [1,1,1,1,1]]\n",
        "  '''\n",
        "  mask = torch.ones(size, size, dtype=torch.bool)\n",
        "  mask = torch.tril(mask)   # í•˜ì‚¼ê° ë¶€ë¶„ë§Œ True\n",
        "  return mask               # (size, size) ì•„ë˜ ì‚¼ê°í˜•ë§Œ True\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "inp = torch.Tensor([[1, 2, 3, 4, 5, 6, 0, 0, 0, 0],\n",
        "                    [3, 5, 6, 2, 6, 5, 8, 0, 0, 0]])\n",
        "\n",
        "pad_mask = create_padding_mask(inp)\n",
        "lh_mask = create_look_ahead_mask(10)\n",
        "print(f'padding mask : \\n{pad_mask}') #(batch_size, 1, seq_len, seq_len)\n",
        "print(f'look ahead mask : \\n{lh_mask}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSytFu8AJlJu",
        "outputId": "fcbdbc4e-d43f-4b2c-fbfa-01a3a0d0b1b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padding mask : \n",
            "tensor([[[[ True,  True,  True,  True,  True,  True, False, False, False, False]]],\n",
            "\n",
            "\n",
            "        [[[ True,  True,  True,  True,  True,  True,  True, False, False, False]]]])\n",
            "look ahead mask : \n",
            "tensor([[ True, False, False, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False, False, False, False, False, False, False, False],\n",
            "        [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "        [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "        [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "        [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
            "        [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> nn.MultiheadAttention ì‚¬ìš©ì‹œ ë§ˆìŠ¤í¬ ì²˜ë¦¬ë  ë¶€ë¶„ì´ Trueê°€ ë˜ì–´ì•¼ í•˜ë¯€ë¡œ ë°˜ëŒ€ë¡œ boolì„ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "YR5zzXNExHbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(2, 10, 64)\n",
        "out, atw = mh(x, x, x, mask=lh_mask) # Look-Ahead Mask (ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬)\n",
        "print(out.shape)\n",
        "atw[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvGf-fXFKEyw",
        "outputId": "c1aa2aa3-cd21-457d-b64e-6a6ac1fad314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 64])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.6490, 0.3510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.4061, 0.2664, 0.3276, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.1988, 0.2802, 0.3023, 0.2187, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.1878, 0.2315, 0.2020, 0.1653, 0.2134, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.2259, 0.1274, 0.2039, 0.1682, 0.1530, 0.1216, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.1236, 0.1272, 0.0756, 0.1110, 0.2058, 0.1677, 0.1891, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0837, 0.1174, 0.1410, 0.1368, 0.1353, 0.1497, 0.0834, 0.1526, 0.0000,\n",
              "         0.0000],\n",
              "        [0.1065, 0.0798, 0.0857, 0.0873, 0.1328, 0.2385, 0.1017, 0.1209, 0.0467,\n",
              "         0.0000],\n",
              "        [0.1393, 0.0915, 0.0663, 0.0895, 0.0988, 0.1138, 0.1209, 0.0550, 0.0595,\n",
              "         0.1655]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì¸ì½”ë” ë ˆì´ì–´\n",
        "\n",
        "- Point-wise Feed Forward Network (PFFN)\n",
        "    * ë©€í‹°í—¤ë“œì–´í…ì…˜ ê³„ì¸µë§Œ í™œìš©í•˜ë©´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ ì‘ìŒ\n",
        "    * ì„ í˜• ë ˆì´ì–´ì™€ í™œì„±í™” í•¨ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° í† í°ì˜ ì„ë² ë”©ì„ ë” í’ë¶€í•˜ê²Œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì„ ë†’ì´ëŠ” ì—­í• \n",
        "\n",
        "- ì¸ì½”ë” ë ˆì´ì–´\n",
        "    * ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•œ íŠ¹ì„±ì„ í•™ìŠµ í•˜ëŠ” ë ˆì´ì–´ë¡œ í•˜ë‚˜ì˜ ë©€í‹°í—¤ë“œì–´í…ì…˜ê³¼ PFFN ë ˆì´ì–´ê°€ ì‚¬ìš©\n",
        "    * ë©€í‹°í—¤ë“œì–´í…ì…˜ì—” í•˜ë‚˜ì˜ ë¬¸ì¥ì´ q,k,vë¡œ ì…ë ¥(ì…€í”„ì–´í…ì…˜)\n",
        "    * ì–´í…ì…˜ ê²°ê³¼ëŠ” ì…ë ¥ ì„ë² ë”©ê³¼ ë”í•˜ì—¬ LayerNormë¥¼ í†µê³¼í•˜ê³  PFFN ë ˆì–´ì— ì…ë ¥\n",
        "    * ì–´í…ì…˜ ê²°ê³¼ì™€ PFFN ê²°ê³¼ë¥¼ ë”í•˜ì—¬ LayerNormì— í†µê³¼í•˜ì—¬ ìµœì¢… ì¶œë ¥\n",
        "\n",
        "> ì–´í…ì…˜ ì¶œë ¥ë¥¼ ì…ë ¥ê³¼ ë”í•˜ëŠ” **Skip Connection**(Residual Connection) ê³¼ì •ì€ ë”¥ëŸ¬ë‹ì—ì„œ ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ê¸°ìš¸ê¸°ê°€ ì†Œì‹¤/í­ì£¼í•˜ëŠ” ë¬¸ì œë¥¼ ì™„í•˜í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„±ì„ í–¥ìƒ ì‹œí‚µë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "ryIlZMFNNOPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê° ë‹¨ì–´ ë²¡í„°ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ë¹„ì„ í˜• í•¨ìˆ˜(GELU)ì— í†µê³¼ì‹œì¼œ, ë‹¨ì–´ë³„ë¡œ ë” í’ë¶€í•œ í‘œí˜„ì„ í•™ìŠµí•˜ê²Œ í•˜ëŠ” ì¸µ\n",
        "def point_wise_feed_forward_network(em_dim, feed_dim):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(em_dim, feed_dim),    # ì…ë ¥ ì„ë² ë”©(em_dim)ì„ ë” í° ì°¨ì›(feed_dim)ìœ¼ë¡œ í™•ì¥\n",
        "        nn.GELU(),                      # ReLUë³´ë‹¤ ë¶€ë“œëŸ½ê³ , BERT/GPT ë“± ìµœì‹  íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ í‘œì¤€ìœ¼ë¡œ ì‚¬ìš©\n",
        "        nn.Linear(feed_dim, em_dim)     # ë‹¤ì‹œ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ì¤„ì´ëŠ” ë‹¨ê³„\n",
        "    )"
      ],
      "metadata": {
        "id": "8cqSjGJgL5Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer ì¸ì½”ë” ë¸”ë¡(Encoder Block) í•˜ë‚˜ë¥¼ êµ¬í˜„í•œ ì½”ë“œ\n",
        "class EncoderLayer(nn.Module):\n",
        "  '''\n",
        "  í•œ ê°œì˜ Encoder LayerëŠ” â‘  Multi-Head Attention â†’ â‘¡ Feed Forward Network ë‘ ë¸”ë¡ìœ¼ë¡œ êµ¬ì„±\n",
        "  ê° ë¸”ë¡ ë’¤ì—ëŠ” Residual Connection(ì”ì°¨ ì—°ê²°) + Layer Normalization ì´ ìˆìŠµë‹ˆë‹¤.\n",
        "  '''\n",
        "  def __init__(self, em_dim, num_heads, feed_dim, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.mha = MultiHeadAttention(em_dim, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(em_dim, feed_dim)\n",
        "\n",
        "    # ë‘ ê°œì˜ ì„œë¸Œì¸µ(sub-layer): Multi-Head Attention, Feed Forward Network\n",
        "    self.layernorm1 = nn.LayerNorm(em_dim, eps=1e-6)  # í•™ìŠµì„ ì•ˆì •í™”í•˜ê³ , ì…ë ¥ ë¶„í¬ë¥¼ ì •ê·œí™”\n",
        "    self.layernorm2 = nn.LayerNorm(em_dim, eps=1e-6)\n",
        "    self.dropout1 = nn.Dropout(rate)                  # ê³¼ì í•© ë°©ì§€ìš©, ì¼ë¶€ ë‰´ëŸ°ì„ í™•ë¥ ì ìœ¼ë¡œ ë¹„í™œì„±í™”\n",
        "    self.dropout2 = nn.Dropout(rate)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    # Multi-head attention: ë¬¸ë§¥ì„ í•™ìŠµ\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, seq_len, em_dim)\n",
        "    attn_output = self.dropout1(attn_output)\n",
        "    out1 = self.layernorm1(x + attn_output)   # (batch_size, seq_len, em_dim)\n",
        "\n",
        "    # Feed Forward: ë‹¨ì–´ë³„ í‘œí˜„ì„ í™•ì¥\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output)\n",
        "    out2 = self.layernorm2(out1 + ffn_output) # (batch_size, seq_len, em_dim)\n",
        "\n",
        "    return out2\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "enc_inp = torch.randn(3, 10, 32)\n",
        "enc_layer = EncoderLayer(32, 4, 64)\n",
        "enc_out = enc_layer(x=enc_inp, mask=None)\n",
        "enc_out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdDovFivNWnd",
        "outputId": "2b025d5c-a518-4609-e78a-ef0b34c86f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 10, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ë””ì½”ë” ë ˆì´ì–´\n",
        "\n",
        "ë””ì½”ë” ë ˆì´ì–´ëŠ” ì˜ˆì¸¡ ë¬¸ì¥ì„ ë§Œë“¤ê¸° ìœ„í•´ ì´ì „ í† í°ìœ¼ë¡œ ë¯¸ë˜ í† í°ì„ í•™ìŠµ í•˜ëŠ” ë ˆì´ì–´ë¡œ 2ê°œì˜ ë©€í‹°í—¤ë“œì–´í…ì…˜ê³¼ í•˜ë‚˜ì˜ PFFNì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "    * ì²«ë²ˆì§¸ ë©€í‹°í—¤ë“œì–´í…ì…˜: ë””ì½”ë” ì…ë ¥ìœ¼ë¡œ ì…€í”„ì–´í…ì…˜ ì§„í–‰\n",
        "    * ì²«ë²ˆì§¸ LayerNorm: ì–´í…ì…˜ ê²°ê³¼ì™€ ë””ì½”ë” ì…ë ¥ì„ ë”í•´ LayerNorm ì§„í–‰\n",
        "    * ë‘ë²ˆì§¸ ë©€í‹°í—¤ë“œì–´í…ì…˜: ì¸ì½”ë”ì˜ ì¶œë ¥ì´ V,Kë¡œ ì…ë ¥ë˜ê³  ì²«ë²ˆì§¸ LayerNorm ì¶œë ¥ì´ Që¡œ ì…ë ¥ë˜ì–´ ì–´í…ì…˜ ì§„í–‰\n",
        "    * ë‘ë²ˆì§¸ LayerNorm: ì²«ë²ˆì§¸ LayerNorm ì¶œë ¥ê³¼ ë‘ë²ˆì§¸ ì–´í…ì…˜ ê²°ê³¼ë¥¼ ë”í•´ LayerNorm ì§„í–‰\n",
        "    * PFFN: ë‘ë²ˆì§¸ LayerNorm ê²°ê³¼ë¥¼ ì…ë ¥í•˜ì—¬ ì¶œë ¥ ê²°ê³¼ë¥¼ ë”í•´ LayerNorm ì§„í–‰"
      ],
      "metadata": {
        "id": "pNYe56bCykie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer ë””ì½”ë” ë¸”ë¡(Decoder Block) í•˜ë‚˜ë¥¼ êµ¬í˜„í•œ ì½”ë“œ: ì¸ì½”ë” ì¶œë ¥ì„ ë°›ì•„ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ìƒì„±í•  ì¤€ë¹„ë¥¼ í•˜ëŠ” í•µì‹¬ ë¸”ë¡\n",
        "class DecoderLayer(nn.Module):\n",
        "  '''\n",
        "    3ê°œì˜ ì„œë¸Œì¸µ(sub-layer)\n",
        "      1) Masked Multi-Head Self-Attention: ë””ì½”ë” ë‚´ë¶€ ë‹¨ì–´ë“¤ë¼ë¦¬ ì–´í…ì…˜ (ë¯¸ë˜ ë‹¨ì–´ ê°€ë¦¬ê¸°)\n",
        "      2) Encoderâ€“Decoder Attention: ì¸ì½”ë”ì˜ ì¶œë ¥ê³¼ ì–´í…ì…˜ (ì…ë ¥ ë¬¸ì¥ ì°¸ê³ )\n",
        "      3) Feed Forward Network (FFN): ê° ë‹¨ì–´ ë²¡í„° ë…ë¦½ì ìœ¼ë¡œ ë¹„ì„ í˜• ë³€í™˜\n",
        "  '''\n",
        "  def __init__(self, em_dim, num_heads, feed_dim, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.mha1 = MultiHeadAttention(em_dim, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(em_dim, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(em_dim, feed_dim)\n",
        "\n",
        "    self.layernorm1 = nn.LayerNorm(em_dim, eps=1e-6)\n",
        "    self.layernorm2 = nn.LayerNorm(em_dim, eps=1e-6)\n",
        "    self.layernorm3 = nn.LayerNorm(em_dim, eps=1e-6)\n",
        "\n",
        "    self.dropout1 = nn.Dropout(rate)\n",
        "    self.dropout2 = nn.Dropout(rate)\n",
        "    self.dropout3 = nn.Dropout(rate)\n",
        "\n",
        "  def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
        "    # Masked Multi-Head Self-Attention: ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ë•Œ, ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì€ ë¯¸ë˜ ë‹¨ì–´ë¥¼ ë³´ì§€ ëª»í•˜ê²Œ í•˜ë©´ì„œ ë¬¸ë§¥(ì´ì „ ë‹¨ì–´ë“¤)ë§Œ ì°¸ê³ í•˜ë„ë¡ ë§Œë“§\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)    # look_ahead_mask: ë¯¸ë˜ ë‹¨ì–´ë¥¼ ë³´ì§€ ì•Šë„ë¡ ë§‰ëŠ” ë§ˆìŠ¤í¬\n",
        "    attn1 = self.dropout1(attn1)\n",
        "    out1 = self.layernorm1(x + attn1)\n",
        "\n",
        "    # ì¸ì½”ë” ì¶œë ¥ê³¼ì˜ ì–´í…ì…˜: ì…ë ¥ ë¬¸ì¥(ì¸ì½”ë” ì¶œë ¥) ì˜ ì •ë³´ì™€ í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ë””ì½”ë” ë¬¸ì¥ì„ ì—°ê²°\n",
        "    attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "    attn2 = self.dropout2(attn2)\n",
        "    out2 = self.layernorm2(out1 + attn2)\n",
        "\n",
        "    # Feed Forward: ê° ë‹¨ì–´ ë²¡í„°ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ë³€í™˜í•´ ë” í’ë¶€í•œ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ì—­í• (GELU ì‹œ)\n",
        "    ffn_output = self.ffn(out2)\n",
        "    ffn_output = self.dropout3(ffn_output)\n",
        "    out3 = self.layernorm3(out2 + ffn_output)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "dec_inp = torch.randn(3, 10, 32)\n",
        "dec_layer = DecoderLayer(32, 4, 64)\n",
        "dec_out, w1, w2 = dec_layer(dec_inp, enc_out, None, None)\n",
        "print(w2.shape)"
      ],
      "metadata": {
        "id": "L02nB6xzNZaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db69063-4f00-4dea-e56c-c258139b634d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 4, 10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### í¬ì§€ì…˜ ì¸ì½”ë”©\n",
        "ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì€ ì…ë ¥ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”(Self-Attention) êµ¬ì¡°ì´ë¯€ë¡œ, ë‹¨ì–´ì˜ ìˆœì„œ ì •ë³´ ì¶”ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
        "ì´ëŸ¬í•œ ìˆœì„œì •ë³´ë¥¼ ì¶”ê°€í•˜ê¸° ìœ„í•´ í† í° ì„ë² ë”© ê²°ê³¼ì— ìˆœì„œì •ë³´ë¥¼ ë”í•´ì¤ë‹ˆë‹¤.\n",
        "\n",
        "í¬ì§€ì…˜ ì¸ì½”ë”©ì€ ê° í¬ì§€ì…˜ì˜ ê°ë„ë¥¼ êµ¬í•œë’¤ sin, cos í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ìƒëŒ€ì ì¸ ìœ„ì¹˜ ì •ë³´ ë§Œë“¤ì–´ ëƒ…ë‹ˆë‹¤. ì´ëŠ” ë³‘ë ¬ì²˜ë¦¬ë¥¼ ì§„í–‰í•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì— ë§¤ìš° íš¨ìœ¨ì ì¸ ìˆœì„œ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "\\begin{aligned}\n",
        "\\text{PE}(pos,\\,2i) &= \\sin\\!\\Bigl(\\tfrac{pos}{10000^{\\,\\frac{2i}{d_{\\text{model}}}}}\\Bigr),\\\\[8pt]\n",
        "\\text{PE}(pos,\\,2i+1) &= \\cos\\!\\Bigl(\\tfrac{pos}{10000^{\\,\\frac{2i}{d_{\\text{model}}}}}\\Bigr).\n",
        "\\end{aligned}\n",
        "\n",
        "- ê° í† í°ì˜ ìœ„ì¹˜(pos) ì— ëŒ€í•´, íŠ¹ì • ê°ë„(ì£¼íŒŒìˆ˜)ë¡œ ë³€í™˜í•œ ë’¤, sin(ì§ìˆ˜)ê³¼ cos(í™€ìˆ˜) í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ì„ë² ë”© ì°¨ì›ë³„ë¡œ ë‹¤ë¥¸ ì£¼ê¸°ë¥¼ ê°–ëŠ” ì‚¬ì¸íŒŒ ìƒì„±\n",
        "- ì„œë¡œ ë‹¤ë¥¸ ìœ„ì¹˜(pos) ê°„ì˜ ìƒëŒ€ì  ê±°ë¦¬ë¥¼ í•¨ìˆ˜ì˜ ìœ„ìƒ ì°¨ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ í•™ìŠµ\n"
      ],
      "metadata": {
        "id": "t0nLkGCYQBMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformerì˜ â€œPositional Encodingâ€ ì„ ì‹œê°í™”\n",
        "# ë¬¸ì¥ ë‚´ ë‹¨ì–´ ìœ„ì¹˜(position)ì— ë”°ë¼ ì‚¬ì¸(sin)ê³¼ ì½”ì‚¬ì¸(cos) ì£¼ê¸°ë¡œ ë³€í•˜ëŠ” ê°’ì„ ë§Œë“¤ì–´, ë‹¨ì–´ì˜ ìœ„ì¹˜ì •ë³´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ê³  ì‹œê°í™”\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ìœ„ì¹˜ì— ë”°ë¥¸ ê°ë„ ì¶”ì¶œ\n",
        "def get_angles(pos, i, em_dim):\n",
        "    # í•˜ë‚˜ì˜ ìœ„ì¹˜ì—ì„œ ì„ë² ë”©ì˜ ê¸¸ì´ ë§Œí¼ ê°ë„ê°€ ì¶”ì¶œë¨\n",
        "    # (ìœ„ì¹˜ ê¸¸ì´, ì„ë² ë”© ê¸¸ì´)\n",
        "    return pos / (10000 ** ((2 * (i // 2)) / em_dim))\n",
        "\n",
        "# ê°ë„ë¡œ ë¶€í„° ì‚¬ì¸íŒŒ ìƒì„±\n",
        "def positional_encoding(position, em_dim):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(em_dim)[np.newaxis, :],\n",
        "        em_dim\n",
        "    )\n",
        "\n",
        "    # ì§ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” sin, í™€ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” cos\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]  # ë°°ì¹˜ ì¶• ì¶”ê°€ (1, position, em_dim)\n",
        "    return torch.tensor(pos_encoding, dtype=torch.float32)\n",
        "\n",
        "position = 50\n",
        "em_dim = 16\n",
        "\n",
        "pos_encoding = positional_encoding(position, em_dim)\n",
        "print(pos_encoding.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YklmZIvRQC4I",
        "outputId": "91c3c412-bd72-4612-b47c-5cc8e9a6a1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 50, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ìµœì¢… ì¸ì½”ë”ì™€ ë””ì½”ë”\n",
        "\n",
        "ì§€ê¸ˆê¹Œì§€ êµ¬í˜„í•œ ì¸ì½”ë” ë ˆì´ì–´ì™€ ë””ì½”ë” ë ˆì´ì–´, ì„ë² ë”© ë ˆì´ì–´, í¬ì§€ì…˜ ì¸ì½”ë”©ì„ ê²°í•©í•˜ì—¬ Transformer ì•„í‚¤í…ì³ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
        "\n",
        "<center><img src=\"https://arxiv.org/html/1706.03762v7/extracted/1706.03762v7/Figures/ModalNet-21.png\" width=\"300\"/></center>"
      ],
      "metadata": {
        "id": "M5ehkEZl3bkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  '''\n",
        "    1) Embedding: ë‹¨ì–´ ID â†’ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ í›„ ìŠ¤ì¼€ì¼ë§\n",
        "    2) Positional Encoding: ìœ„ì¹˜ ì •ë³´(sin/cos)ë¥¼ ì„ë² ë”©ì— ë”í•¨\n",
        "    3) Dropout: ì…ë ¥ ì„ë² ë”©+í¬ì§€ì…˜ì— ë“œë¡­ì•„ì›ƒ ì ìš©\n",
        "    4) Stacked EncoderLayers (ê° ì¸µë§ˆë‹¤):\n",
        "      - Multi-Head Self-Attention\n",
        "      - Dropout + Residual + LayerNorm\n",
        "      - Feed Forward Network (FFN)\n",
        "      - Dropout + Residual + LayerNorm\n",
        "  '''\n",
        "\n",
        "  def __init__(self, num_layers, em_dim, num_heads, feed_dim,\n",
        "                input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.em_dim = em_dim\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # ì„ë² ë”© ë ˆì´ì–´\n",
        "    self.embedding = nn.Embedding(input_vocab_size, em_dim)\n",
        "    # í¬ì§€ì…˜ ì¸ì½”ë”©\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, em_dim)\n",
        "\n",
        "    # ì¸ì½”ë” ë ˆì´ì–´\n",
        "    self.enc_layers = nn.ModuleList([\n",
        "        EncoderLayer(em_dim, num_heads, feed_dim, rate)\n",
        "        for _ in range(num_layers)\n",
        "    ])\n",
        "    self.dropout = nn.Dropout(rate)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    seq_len = x.size(1)\n",
        "\n",
        "    # ì„ë² ë”© í›„ ìŠ¤ì¼€ì¼ë§\n",
        "    x = self.embedding(x)  # (batch_size, seq_len, em_dim)\n",
        "    x = x * math.sqrt(self.em_dim)\n",
        "\n",
        "    # í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
        "    pe = self.pos_encoding[:, :seq_len, :].to(x.device)\n",
        "\n",
        "    # ì„ë² ë”©ê³¼ í¬ì§€ì…˜ ê²°í•©\n",
        "    x = x + pe\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # ì¸ì½”ë” ë ˆì´ì–´ì¸µ í†µê³¼\n",
        "    for i in range(self.num_layers):\n",
        "        x = self.enc_layers[i](x, mask)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "ix9QCJJTPjvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  '''\n",
        "    1) Embedding + ìŠ¤ì¼€ì¼ë§: ë””ì½”ë” ì…ë ¥ í† í°ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³  í¬ê¸°ë¥¼ ë³´ì •\n",
        "    2) Positional Encoding: ìœ„ì¹˜ ì •ë³´ë¥¼ sin/cos íŒ¨í„´ìœ¼ë¡œ ì„ë² ë”©ì— ë”í•¨\n",
        "    3) Dropout: ì„ë² ë”©+í¬ì§€ì…˜ì— ë“œë¡­ì•„ì›ƒ ì ìš© (ê³¼ì í•© ë°©ì§€)\n",
        "    4) Stacked DecoderLayers (ê° ì¸µë§ˆë‹¤)\n",
        "      - Masked Multi-Head Self-Attention: ë¯¸ë˜ í† í°ì„ ê°€ë¦° ìê¸° ì–´í…ì…˜\n",
        "      - Encoderâ€“Decoder Attention: ì¸ì½”ë” ì¶œë ¥ê³¼ì˜ ê´€ê³„ í•™ìŠµ\n",
        "      - Feed Forward Network (FFN): ê° ë‹¨ì–´ ë²¡í„° ë¹„ì„ í˜• ë³€í™˜\n",
        "      - ê° ë¸”ë¡ë§ˆë‹¤ Dropout + Residual + LayerNormìœ¼ë¡œ ì•ˆì •í™”\n",
        "      - Attention Weights ì €ì¥: block1(ìê¸°), block2(ì¸ì½”ë”-ë””ì½”ë”) ê¸°ë¡\n",
        "  '''\n",
        "  def __init__(self, num_layers, em_dim, num_heads, feed_dim,\n",
        "                target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.em_dim = em_dim\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(target_vocab_size, em_dim)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, em_dim)\n",
        "\n",
        "    self.dec_layers = nn.ModuleList([\n",
        "        DecoderLayer(em_dim, num_heads, feed_dim, rate)\n",
        "        for _ in range(num_layers)\n",
        "    ])\n",
        "    self.dropout = nn.Dropout(rate)\n",
        "\n",
        "  def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
        "    seq_len = x.size(1)\n",
        "    attention_weights = {}\n",
        "\n",
        "    # ë””ì½”ë” ì…ë ¥ì— ëŒ€í•œ ì„ë² ë”© ë° í¬ì§€ì…˜ ì¸ì½”ë”©\n",
        "    x = self.embedding(x)\n",
        "    x = x * math.sqrt(self.em_dim)\n",
        "    pe = self.pos_encoding[:, :seq_len, :].to(x.device)\n",
        "    x = x + pe\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # ì¸ì½”ë” ì¶œë ¥ê³¼ í•¨ê»˜ ë””ì½”ë” ë ˆì´ì–´ì— ì…ë ¥\n",
        "    for i in range(self.num_layers):\n",
        "        x, block1, block2 = self.dec_layers[i](x, enc_output,\n",
        "                                                look_ahead_mask,\n",
        "                                                padding_mask)\n",
        "        # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê¸°ë¡(ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í™•ì¸ ìš©ë„)\n",
        "        attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "        attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    return x, attention_weights\n"
      ],
      "metadata": {
        "id": "i_LTg2BMQX6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "eco_inp = torch.Tensor([[1, 2, 3, 4, 5, 6, 0, 0, 0, 0]]).long()\n",
        "dco_inp = torch.Tensor([[1, 2, 3, 4, 5, 6, 7, 8, 0, 0]]).long()\n",
        "pad_mask = create_padding_mask(eco_inp)\n",
        "lh_mask = create_look_ahead_mask(10)\n",
        "\n",
        "ecoder = Encoder(2, 32, 4, 64, 10, 100)\n",
        "eco_out = ecoder(eco_inp, pad_mask)\n",
        "print(f'ecoder out:{eco_out.shape}')\n",
        "decoder = Decoder(2, 32, 4, 64, 10, 100)\n",
        "dec_out, attn = decoder(dco_inp, eco_out, lh_mask, pad_mask)\n",
        "print(f'decoder out:{dec_out.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irIDS_B4QZ2g",
        "outputId": "1f93c016-008e-4fd6-90ba-db6f790d335a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ecoder out:torch.Size([1, 10, 32])\n",
            "decoder out:torch.Size([1, 10, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë””ì½”ë” 2ë²ˆì§¸ ë¸”ëŸ­ì˜ 0ë²ˆ ë°°ì¹˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜\n",
        "print('1ë²ˆ ì–´í…ì…˜')\n",
        "print(attn['decoder_layer2_block1'][0])\n",
        "print('2ë²ˆ ì–´í…ì…˜')\n",
        "print(attn['decoder_layer2_block2'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1Zha8Z2Rpzj",
        "outputId": "a13d0b16-1e19-4ac9-a83b-c63aa702ca20",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1ë²ˆ ì–´í…ì…˜\n",
            "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.5407, 0.4593, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2190, 0.5547, 0.2263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1607, 0.1606, 0.2533, 0.4254, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1312, 0.2599, 0.1176, 0.2697, 0.2217, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1131, 0.1660, 0.1661, 0.2487, 0.1579, 0.1483, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0879, 0.1325, 0.1657, 0.2488, 0.1295, 0.0890, 0.1466, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0638, 0.0764, 0.1059, 0.2394, 0.0804, 0.1274, 0.1662, 0.1406,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0585, 0.1082, 0.1127, 0.2024, 0.1059, 0.0530, 0.0726, 0.1733,\n",
            "          0.1134, 0.0000],\n",
            "         [0.0548, 0.0936, 0.1068, 0.1837, 0.0943, 0.0486, 0.0742, 0.1452,\n",
            "          0.1018, 0.0969]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.5734, 0.4266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.3022, 0.4477, 0.2501, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2561, 0.2566, 0.2240, 0.2634, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2147, 0.1967, 0.2132, 0.2290, 0.1464, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1333, 0.1047, 0.1979, 0.2953, 0.1324, 0.1364, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1470, 0.0938, 0.1436, 0.0918, 0.1627, 0.1876, 0.1736, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1008, 0.1399, 0.1171, 0.1463, 0.0849, 0.0875, 0.2022, 0.1213,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1042, 0.0843, 0.2173, 0.0449, 0.1105, 0.1186, 0.0513, 0.0841,\n",
            "          0.1848, 0.0000],\n",
            "         [0.0867, 0.0683, 0.1841, 0.0360, 0.0947, 0.1020, 0.0425, 0.0720,\n",
            "          0.1613, 0.1524]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.4084, 0.5916, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.3329, 0.2715, 0.3956, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2663, 0.1257, 0.3215, 0.2865, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1235, 0.3423, 0.1215, 0.1256, 0.2870, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1456, 0.2139, 0.1713, 0.1282, 0.1640, 0.1769, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1435, 0.1297, 0.1289, 0.1887, 0.1422, 0.1308, 0.1363, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1375, 0.1118, 0.1526, 0.1250, 0.1413, 0.1308, 0.0909, 0.1100,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1096, 0.0944, 0.1132, 0.1865, 0.0813, 0.0888, 0.0872, 0.1194,\n",
            "          0.1195, 0.0000],\n",
            "         [0.0880, 0.0905, 0.0899, 0.1658, 0.0776, 0.0783, 0.0793, 0.1163,\n",
            "          0.1106, 0.1038]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.5290, 0.4710, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.3401, 0.3501, 0.3098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2873, 0.1867, 0.2890, 0.2370, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2750, 0.1364, 0.3676, 0.1341, 0.0869, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1584, 0.1352, 0.2138, 0.2287, 0.1025, 0.1615, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1838, 0.1167, 0.2290, 0.1184, 0.1065, 0.1237, 0.1219, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1274, 0.0818, 0.1935, 0.2466, 0.0609, 0.0991, 0.0969, 0.0939,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1697, 0.0924, 0.1495, 0.0577, 0.1705, 0.0754, 0.0319, 0.1860,\n",
            "          0.0669, 0.0000],\n",
            "         [0.1577, 0.0878, 0.1399, 0.0529, 0.1538, 0.0734, 0.0327, 0.1774,\n",
            "          0.0624, 0.0619]]], grad_fn=<SelectBackward0>)\n",
            "2ë²ˆ ì–´í…ì…˜\n",
            "tensor([[[0.2106, 0.1307, 0.1450, 0.1620, 0.2101, 0.1416, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1008, 0.2326, 0.1887, 0.1464, 0.2013, 0.1302, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2134, 0.1397, 0.1959, 0.1537, 0.1324, 0.1649, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0564, 0.1783, 0.2235, 0.1704, 0.2059, 0.1656, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.4168, 0.1038, 0.0945, 0.1167, 0.1545, 0.1137, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1205, 0.2464, 0.1951, 0.1309, 0.1709, 0.1362, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0690, 0.3570, 0.1956, 0.1242, 0.1433, 0.1110, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0785, 0.1585, 0.2114, 0.1392, 0.2482, 0.1642, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1887, 0.1331, 0.1427, 0.2220, 0.1660, 0.1474, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1942, 0.1479, 0.1384, 0.2092, 0.1753, 0.1350, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000]],\n",
            "\n",
            "        [[0.0989, 0.2020, 0.2084, 0.2304, 0.0802, 0.1801, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2109, 0.2328, 0.1041, 0.1720, 0.1323, 0.1479, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1484, 0.1957, 0.1255, 0.2085, 0.0967, 0.2252, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1101, 0.1224, 0.3214, 0.1421, 0.2100, 0.0939, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2331, 0.1361, 0.1278, 0.1653, 0.1936, 0.1441, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1772, 0.1461, 0.1576, 0.1904, 0.1639, 0.1648, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1671, 0.1617, 0.1430, 0.1710, 0.1702, 0.1871, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1152, 0.1958, 0.2582, 0.1615, 0.1201, 0.1492, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2384, 0.2177, 0.0961, 0.1373, 0.1184, 0.1921, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2511, 0.2003, 0.0938, 0.1429, 0.1265, 0.1853, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000]],\n",
            "\n",
            "        [[0.1519, 0.2427, 0.1242, 0.1085, 0.1876, 0.1851, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1056, 0.2324, 0.1699, 0.1220, 0.2102, 0.1599, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1995, 0.1650, 0.1346, 0.1568, 0.1746, 0.1694, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2061, 0.1747, 0.1719, 0.1799, 0.1226, 0.1448, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1287, 0.1038, 0.0983, 0.1463, 0.1909, 0.3319, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1114, 0.1697, 0.1112, 0.1272, 0.2136, 0.2669, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1331, 0.2262, 0.1409, 0.1528, 0.1829, 0.1640, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2058, 0.2579, 0.1072, 0.1495, 0.1379, 0.1416, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1744, 0.2559, 0.1104, 0.1532, 0.1664, 0.1398, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1615, 0.2366, 0.1162, 0.1493, 0.1837, 0.1528, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000]],\n",
            "\n",
            "        [[0.1368, 0.1342, 0.1741, 0.2071, 0.1468, 0.2010, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1823, 0.1990, 0.1492, 0.1726, 0.1781, 0.1188, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1693, 0.1509, 0.1193, 0.1607, 0.1787, 0.2210, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1141, 0.1529, 0.1482, 0.2851, 0.0996, 0.2000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1341, 0.1431, 0.1033, 0.1663, 0.2146, 0.2386, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1741, 0.2368, 0.1120, 0.1314, 0.1909, 0.1548, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1774, 0.2897, 0.1320, 0.1385, 0.1482, 0.1142, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1487, 0.1520, 0.1635, 0.2310, 0.1114, 0.1934, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1529, 0.2253, 0.1509, 0.1034, 0.2081, 0.1594, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1618, 0.2179, 0.1496, 0.0960, 0.2216, 0.1530, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000]]], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer ëª¨ë¸ë§\n",
        "\n",
        "ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ê²°í•©í•˜ê³  ìµœì¢… ë¶„ë¥˜ê¸°ë¥¼ ë”í•œ Transformer êµ¬ì¡°ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì´ë•Œ ì¸ì½”ë”ì™€ ë””ì½”ë”ì— ë„£ì–´ì¤„ ë§ˆìŠ¤í¬ë¥¼ `forward` ë‹¨ê³„ì—ì„œ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì™„ì„±ëœ Transformerëª¨ë¸ì˜ `forward`ëŠ” í•™ìŠµì„ ìœ„í•œ ìˆœì „íŒŒë¡œ íƒ€ê²Ÿì´ ê°™ì´ ì…ë ¥ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ í•™ìŠµëœ Transformerëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì‘ì—…ì€ `forward` í•¨ìˆ˜ë¥¼ í™œìš©í•˜ê¸°ì— ì ì ˆ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
        "í•´ë‹¹ ì˜ˆì‹œì—ì„ ëŠ í•™ìŠµëœ Transformerë¥¼ í™œìš©í•´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ `generate` í•¨ìˆ˜ë¥¼ ë”°ë¡œ êµ¬í˜„ í•©ë‹ˆë‹¤.\n",
        "\n",
        "**generate í•¨ìˆ˜**\n",
        "- ì¸ì½”ë” ì…ë ¥ë  í† í°ì…‹ê³¼ ë””ì½”ë”ì— ì…ë ¥ë  ì‹œì‘ í† í° í•˜ë‚˜ë§Œ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ìê¸°íšŒê·€(Autoregressive)ë°©ì‹ìœ¼ë¡œ í† í°ì„ ìƒì„±\n",
        "- ìµœëŒ€ í† í° ê¸¸ì´ë¥¼ ì„¤ì •í•˜ì—¬ ë””ì½”ë”ì˜ í† í° ì˜ˆì¸¡ì„ ë°˜ë³µí•˜ê³  ë¬¸ì¥ ë í† í°ì´ ë‚˜ì˜¤ëŠ” ê²½ìš° ë°˜ë³µì„ ì¤‘ë‹¨\n",
        "- í† í° idë¥¼ ì‹¤ì œ ë‹¨ì–´ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥\n",
        "\n"
      ],
      "metadata": {
        "id": "CV0ADvav4P_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, num_layers, em_dim, num_heads, feed_dim,\n",
        "                input_vocab_size, target_vocab_size,\n",
        "                pe_input, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, em_dim, num_heads, feed_dim,\n",
        "                            input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, em_dim, num_heads, feed_dim,\n",
        "                            target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = nn.Linear(em_dim, target_vocab_size)\n",
        "\n",
        "  def create_masks(self, inp, tar, pad_token=0):\n",
        "    # ì¸ì½”ë” íŒ¨ë”© ë§ˆìŠ¤í¬\n",
        "    enc_padding_mask = create_padding_mask(inp, pad_token)\n",
        "\n",
        "    # ë””ì½”ë” íŒ¨ë”© ë§ˆìŠ¤í¬\n",
        "    dec_padding_mask = create_padding_mask(inp, pad_token)\n",
        "\n",
        "    # look_ahead_mask\n",
        "    seq_len = tar.size(1)\n",
        "    look_ahead = create_look_ahead_mask(seq_len).to(tar.device)  # (seq_len, seq_len)\n",
        "    look_ahead = look_ahead.unsqueeze(0).unsqueeze(0)  # (1,1,seq_len,seq_len)\n",
        "\n",
        "    # íƒ€ê²Ÿ(ë””ì½”ë” ì…ë ¥)ì—ë„ íŒ¨ë”©ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ\n",
        "    dec_target_padding_mask = create_padding_mask(tar, pad_token)  # (batch_size, 1, 1, seq_len)\n",
        "    combined_mask = look_ahead & dec_target_padding_mask  # ë‘˜ ë‹¤ Trueì—¬ì•¼ True\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "  def forward(self, inp, tar, pad_token=0):\n",
        "    \"\"\"\n",
        "    inp, tar: (batch_size, seq_len)\n",
        "    \"\"\"\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = self.create_masks(inp, tar, pad_token)\n",
        "\n",
        "    enc_output = self.encoder(inp, enc_padding_mask)  # (batch_size, inp_seq_len, em_dim)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, look_ahead_mask=combined_mask, padding_mask=dec_padding_mask\n",
        "    )\n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "    return final_output, attention_weights\n",
        "\n",
        "  # í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜(Autoregressive ë°©ì‹)\n",
        "  @torch.no_grad()\n",
        "  def generate(self, enc_input, start_tokens, end_token=None, pad_token=0, max_new_tokens=50, device='cpu'):\n",
        "    '''\n",
        "    enc_input : ì¸ì½”ë”ì— ì…ë ¥ë  í† í°ì…‹(ì§ˆë¬¸)\n",
        "    start_tokens : ë””ì½”ë”ì˜ ì‹œì‘ í† í°\n",
        "    end_token : ë””ì½”ë”ë¥¼ ë©ˆì¶”ê¸° ìœ„í•œ ëí† í°\n",
        "    pad_token : ë§ˆìŠ¤í¬ ìƒì„±ì„ ìœ„í•œ íŒ¨ë”© í† í°\n",
        "    max_new_tokens : ìƒì„±ë  í† í°ì˜ ìµœëŒ€ ê°œìˆ˜\n",
        "    '''\n",
        "\n",
        "    # ====== ì¶”ê°€\n",
        "    enc_input = enc_input.to(device)\n",
        "    # ====== ì¶”ê°€\n",
        "\n",
        "    enc_padding_mask = create_padding_mask(enc_input, pad_token)\n",
        "    enc_output = self.encoder(enc_input, enc_padding_mask.to(device))\n",
        "\n",
        "    # ë™ì ìœ¼ë¡œ ì…ë ¥ í† í°ì„ ë§Œë“¤ì–´ëƒ„\n",
        "    generated_tokens = [start_tokens]\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ í† í°\n",
        "        cur_tokens = torch.tensor(generated_tokens, dtype=torch.long, device=device)\n",
        "        cur_tokens = cur_tokens.unsqueeze(0)  # (1, current_length)\n",
        "\n",
        "        # ë§ˆìŠ¤í¬ ìƒì„±\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = self.create_masks(enc_input, cur_tokens, pad_token)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dec_output, _ = self.decoder(\n",
        "                cur_tokens, enc_output,\n",
        "                look_ahead_mask=combined_mask,\n",
        "                padding_mask=dec_padding_mask\n",
        "            )\n",
        "            # ë§ˆì§€ë§‰ í† í° ìœ„ì¹˜ì˜ ì¶œë ¥ë§Œ ì¶”ì¶œ\n",
        "            logits = self.final_layer(dec_output)  # (1, cur_len, vocab_size)\n",
        "            next_token_logits = logits[:, -1, :]    # (1, vocab_size)\n",
        "\n",
        "            # ê·¸ë¦¬ë””: í™•ë¥ ì´ ê°€ì¥ í° í† í° 1ê°œ\n",
        "            next_token = next_token_logits.argmax(dim=-1).item()\n",
        "\n",
        "        generated_tokens.append(next_token)\n",
        "\n",
        "        # ì¢…ë£Œ í† í°ì„ ìƒì„±í•˜ë©´ ì¤‘ë‹¨\n",
        "        if end_token is not None and next_token == end_token:\n",
        "            break\n",
        "\n",
        "    return generated_tokens\n"
      ],
      "metadata": {
        "id": "5ifuRlPXRWxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ---\n",
        "# ì¸ì½”ë” ì…ë ¥ ì˜ˆì‹œ (ë°°ì¹˜ 1, ì‹œí€€ìŠ¤ 10) / <pad> í† í° 0 ê°€ì •\n",
        "eco_inp = torch.Tensor([[1, 2, 3, 4, 5, 6, 0, 0, 0, 0]]).long()\n",
        "\n",
        "# ë””ì½”ë” ì…ë ¥ ì˜ˆì‹œ (ë°°ì¹˜ 1, ì‹œí€€ìŠ¤ 10) / <pad> í† í° 0 ê°€ì •\n",
        "dco_inp = torch.Tensor([[1, 2, 3, 4, 5, 6, 7, 8, 0, 0]]).long()\n",
        "\n",
        "num_layers = 2          # ì¸ì½”ë”/ë””ì½”ë” ë¸”ë¡(ë ˆì´ì–´)ì˜ ê°œìˆ˜\n",
        "em_dim = 32             # ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›\n",
        "num_heads = 4           # ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ì˜ í—¤ë“œ ê°œìˆ˜\n",
        "feed_dim = 64           # FFN ë‚´ë¶€ ì€ë‹‰ì¸µì˜ ì°¨ì›\n",
        "input_vocab_size = 16   # ì…ë ¥(ì†ŒìŠ¤) ë‹¨ì–´ ì§‘í•©(Vocabulary) í¬ê¸°\n",
        "target_vocab_size = 16  # íƒ€ê²Ÿ(ëŒ€ìƒ) ë‹¨ì–´ ì§‘í•©(Vocabulary) í¬ê¸°\n",
        "pe_input = 100          # ì¸ì½”ë”ì˜ ìµœëŒ€ í¬ì§€ì…˜(ì‹œí€€ìŠ¤ ê¸¸ì´)\n",
        "pe_target = 100         # ë””ì½”ë”ì˜ ìµœëŒ€ í¬ì§€ì…˜(ì‹œí€€ìŠ¤ ê¸¸ì´)\n",
        "\n",
        "transformer = Transformer(num_layers, em_dim, num_heads, feed_dim,\n",
        "                 input_vocab_size, target_vocab_size,\n",
        "                 pe_input, pe_target)\n",
        "\n",
        "out, atw = transformer(eco_inp, dco_inp)\n",
        "generated_token = transformer.generate(eco_inp, 2, 3, 0)\n",
        "print(f'ìµœì¢… ì¶œë ¥ í˜•ìƒ {out.shape}')\n",
        "print(f'í† í° ìƒì„± {generated_token}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsYv-6WGQh4L",
        "outputId": "32bd07c6-2891-46fe-8947-0cdca948f781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìµœì¢… ì¶œë ¥ í˜•ìƒ torch.Size([1, 10, 16])\n",
            "í† í° ìƒì„± [2, 0, 8, 0, 5, 5, 5, 5, 5, 5, 5, 15, 5, 5, 5, 5, 5, 5, 15, 5, 5, 5, 5, 15, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer í•™ìŠµ í•˜ê¸°\n"
      ],
      "metadata": {
        "id": "ZmwRwC3DvO_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ë°ì´í„°ì„¸íŠ¸ ì¤€ë¹„\n",
        "\n",
        "ì´ì „ Seq2seq ëª¨ë¸ í•™ìŠµê³¼ ë™ì¼í•˜ê²Œ ì¤€ë¹„í•œ Transformer ëª¨ë¸ì— [AIHUB](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&dataSetSn=86) í•œêµ­ì–´ ê°ì„± ëŒ€í™” ë§ë­‰ì¹˜ë¥¼ ë¶€ë¶„ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ì§ˆì˜(ì…ë ¥) ì‘ë‹µ(íƒ€ê²Ÿ) ë°ì´í„°ì„¸íŠ¸ë¥¼ í•™ìŠµ í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "s_iQO5HSvYhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "MdodNNvua7my",
        "outputId": "e9a4d3a0-c10c-444c-f6ef-46a7d9799998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6478dc33-899f-4301-af9c-1301e123fc13\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6478dc33-899f-4301-af9c-1301e123fc13\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.csv to train (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(f'./train.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        },
        "id": "VGzkks3Waze5",
        "outputId": "36bd12ab-6d8a-4276-8293-2e37dfdd295e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      label                                               HS01  \\\n",
              "0        E1                          ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.   \n",
              "1        E1     ì´ë²ˆ ë‹¬ì— ë˜ ê¸‰ì—¬ê°€ ê¹ì˜€ì–´! ë¬¼ê°€ëŠ” ì˜¤ë¥´ëŠ”ë° ì›”ê¸‰ë§Œ ìê¾¸ ê¹ì´ë‹ˆê¹Œ ë„ˆë¬´ í™”ê°€ ë‚˜.   \n",
              "2        E1  íšŒì‚¬ì— ì‹ ì…ì´ ë“¤ì–´ì™”ëŠ”ë° ë§íˆ¬ê°€ ê±°ìŠ¬ë ¤. ê·¸ëŸ° ì• ë¥¼ ë§¤ì¼ ë´ì•¼ í•œë‹¤ê³  ìƒê°í•˜ë‹ˆê¹Œ ìŠ¤...   \n",
              "3        E1  ì§ì¥ì—ì„œ ë§‰ë‚´ë¼ëŠ” ì´ìœ ë¡œ ë‚˜ì—ê²Œë§Œ ì˜¨ê°– ì‹¬ë¶€ë¦„ì„ ì‹œì¼œ. ì¼ë„ ë§ì€ ë° ì •ë§ ë¶„í•˜ê³  ...   \n",
              "4        E1              ì–¼ë§ˆ ì „ ì…ì‚¬í•œ ì‹ ì…ì‚¬ì›ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ë„ˆë¬´ í™”ê°€ ë‚˜.   \n",
              "...     ...                                                ...   \n",
              "51623    E1     ë‚˜ì´ê°€ ë¨¹ê³  ì´ì œ ëˆë„ ëª» ë²Œì–´ ì˜¤ë‹ˆê¹Œ ì–´ë–»ê²Œ ì‚´ì•„ê°€ì•¼ í• ì§€ ë§‰ë§‰í•´. ëŠ¥ë ¥ë„ ì—†ê³ .   \n",
              "51624    E3        ëª¸ì´ ë§ì´ ì•½í•´ì¡Œë‚˜ ë´. ì´ì œ ì „ê³¼ ê°™ì´ ì¼í•˜ì§€ ëª»í•  ê²ƒ ê°™ì•„ ë„ˆë¬´ ì§œì¦ ë‚˜.   \n",
              "51625    E4   ì´ì œ ì–´ë–»ê²Œ í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´. ë‚¨í¸ë„ ê·¸ë ‡ê³  ë…¸í›„ ì¤€ë¹„ë„ ì•ˆ ë˜ì–´ì„œ ë¯¸ë˜ê°€ ê±±ì •ë¼.   \n",
              "51626    E3  ëª‡ì‹­ ë…„ì„ í•¨ê»˜ ì‚´ì•˜ë˜ ë‚¨í¸ê³¼ ì´í˜¼í–ˆì–´. ê·¸ë™ì•ˆì˜ ì„¸ì›”ì— ë°°ì‹ ê°ì„ ëŠë¼ê³  ë„ˆë¬´ í™”ê°€ ë‚˜.   \n",
              "51627    E4  ë‚¨í¸ê³¼ ê²°í˜¼í•œ ì§€ ì‚¬ì‹­ ë…„ì´ì•¼. ì´ì œ ì‚¬ëŒ ë§Œë‚˜ëŠ” ê²ƒë„ ë²„ê²ê³  ì•Œë˜ ì‚¬ëŒë„ ì ì  ì‚¬ë¼ì ¸.   \n",
              "\n",
              "                                                    SS01  \\\n",
              "0                            ë§ì´ í˜ë“œì‹œê² ì–´ìš”. ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìˆë‚˜ìš”?   \n",
              "1               ê¸‰ì—¬ê°€ ì¤„ì–´ ì†ìƒí•˜ì‹œê² ì–´ìš”. ì›”ê¸‰ì´ ì¤„ì–´ë“  ê²ƒì„ ì–´ë–»ê²Œ ë³´ì™„í•˜ì‹¤ ê±´ê°€ìš”?   \n",
              "2      íšŒì‚¬ ë™ë£Œ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ëŠ” ê²ƒ ê°™ì•„ìš”. ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì–´ë–¤ ë…¸ë ¥ì„ ...   \n",
              "3      ê´€ë ¨ ì—†ëŠ” ì‹¬ë¶€ë¦„ì„ ëª¨ë‘ í•˜ê²Œ ë˜ì–´ì„œ ë…¸ì—¬ìš°ì‹œêµ°ìš”. ì–´ë–¤ ê²ƒì´ ìƒí™©ì„ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆ...   \n",
              "4      ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì€ íƒœë„ì— í™”ê°€ ë‚˜ì…¨êµ°ìš”. ìƒëŒ€ë°©ì˜ ì–´ë–¤ í–‰ë™ì´ ê·¸ëŸ° ê°ì •ì„ ìœ ë°œí•˜ëŠ”...   \n",
              "...                                                  ...   \n",
              "51623                 ê²½ì œì ì¸ ë¬¸ì œ ë•Œë¬¸ì— ë§‰ë§‰í•˜ì‹œêµ°ìš”. ë§ˆìŒì´ í¸ì¹˜ ì•Šìœ¼ì‹œê² ì–´ìš”.   \n",
              "51624              ê±´ê°•ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê¸°ë¶„ì´ ì¢‹ì§€ ì•Šìœ¼ì‹œêµ°ìš”. ì†ìƒí•˜ì‹œê² ì–´ìš”.   \n",
              "51625                      ë…¸í›„ ì¤€ë¹„ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê±±ì •ì´ ë§ìœ¼ì‹œê² ì–´ìš”.   \n",
              "51626                               ê°€ì¡±ê³¼ì˜ ë¬¸ì œ ë•Œë¬¸ì— ì†ìƒí•˜ì‹œê² ì–´ìš”.   \n",
              "51627                    ëŒ€ì¸ê´€ê³„ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê±±ì •ë˜ì‹œê³  ì†ìƒí•˜ì‹œê² ì–´ìš”.   \n",
              "\n",
              "                                                    HS02  \\\n",
              "0                    ê·¸ëƒ¥ ë‚´ê°€ í•´ê²°í•˜ëŠ” ê²Œ ë‚˜ì•„. ë‚¨ë“¤í•œí…Œ ë¶€ë‹´ ì£¼ê³  ì‹¶ì§€ë„ ì•Šê³ .   \n",
              "1       ìµœëŒ€í•œ ì§€ì¶œì„ ì–µì œí•´ì•¼ê² ì–´. ì›”ê¸‰ì´ ì¤„ì–´ë“¤ì—ˆìœ¼ë‹ˆ ê³ ì •ì§€ì¶œì„ ì¤„ì¼ ìˆ˜ë°–ì— ì—†ì„ ê²ƒ ê°™ì•„.   \n",
              "2      ì˜ ì•ˆ ë§ëŠ” ì‚¬ëŒì´ë‘ ì–µì§€ë¡œ ì˜ ì§€ë‚´ëŠ” ê²ƒë³´ë‹¨ ì¡°ê¸ˆì€ ê±°ë¦¬ë¥¼ ë‘ê³  ì˜ˆì˜ë¥¼ ê°–ì¶°ì„œ ëŒ€...   \n",
              "3                  ì§ì¥ ì‚¬ëŒë“¤ê³¼ ì†”ì§í•˜ê²Œ ì´ì•¼ê¸°í•´ë³´ê³  ì‹¶ì–´. ì¼í•˜ëŠ” ë°ì— ë°©í•´ëœë‹¤ê³ .   \n",
              "4                    ìƒì‚¬ì¸ ë‚˜ì—ê²Œ ë¨¼ì € ì¸ì‚¬í•˜ì§€ ì•Šì•„ì„œ ë§¤ì¼ ë‚´ê°€ ë¨¼ì € ì¸ì‚¬í•œë‹¤ê³ !   \n",
              "...                                                  ...   \n",
              "51623                   ì•„ë¬´ê²ƒë„ í•  ìˆ˜ ì—†ëŠ” ë‚´ê°€ ë¬´ê°€ì¹˜í•˜ê²Œ ëŠê»´ì§€ê³  ì‹¤ë§ìŠ¤ëŸ¬ì›Œ.   \n",
              "51624          ë§ˆìŒ ê°™ì•„ì„œëŠ” ë‹¤ í•  ìˆ˜ ìˆëŠ” ì¼ì¸ë° ì´ì   ëª¸ì´ ì•ˆ ë”°ë¼ì™€ ì£¼ë‹ˆ í™”ë§Œ ë‚˜.   \n",
              "51625  ì£¼ë³€ ì‚¬ëŒë“¤ì€ ë‹¤ ë…¸í›„ ì¤€ë¹„ë„ ì˜í•´ë‘ì—ˆë˜ë° ë‚œ ì–´ë–»ê²Œ í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´. ë§‰ë§‰í•˜ê¸°...   \n",
              "51626             ì´ì œ í•  ìˆ˜ ìˆëŠ” ì¼ë„ ì—†ê³  ì´ë ‡ê²Œ í˜ë“¤ê²Œ ì‚¬ëŠ” ê²Œ ë¶ˆë§ŒìŠ¤ëŸ½ê¸°ë§Œ í•´.   \n",
              "51627             ì‚¬ëŒë“¤ì„ ë§Œë‚˜ëŠ” ê²ƒì´ ì–´ë ¤ì›Œ. ìê¾¸ ì‚¬ëŒë“¤ì„ ì˜ì‹¬í•˜ê²Œë§Œ ë˜ê³  ë§ì´ì•¼.   \n",
              "\n",
              "                                                    SS02  \\\n",
              "0         í˜¼ì í•´ê²°í•˜ê¸°ë¡œ í–ˆêµ°ìš”. í˜¼ìì„œ í•´ê²°í•˜ê¸° í˜ë“¤ë©´ ì£¼ìœ„ì— ì˜ë…¼í•  ì‚¬ëŒì„ ì°¾ì•„ë³´ì„¸ìš”.    \n",
              "1                               ì›”ê¸‰ì´ ì¤„ì–´ë“  ë§Œí¼ ì†Œë¹„ë¥¼ ì¤„ì¼ ê³„íšì´êµ°ìš”.   \n",
              "2              ìŠ¤íŠ¸ë ˆìŠ¤ë°›ì§€ ì•Šê¸° ìœ„í•´ì„  ì¸ê°„ê´€ê³„ì— ìˆì–´ ì•½ê°„ì˜ ê±°ë¦¬ë¥¼ ë‘ëŠ” ê²Œ ì¢‹ê² êµ°ìš”.   \n",
              "3                            ì§ì¥ ì‚¬ëŒë“¤ê³¼ ì´ì•¼ê¸°ë¥¼ í•´ ë³´ê² ë‹¤ê³  ê²°ì‹¬í•˜ì…¨êµ°ìš”.   \n",
              "4      í•­ìƒ ë¨¼ì € ì¸ì‚¬í•˜ê²Œ ë˜ì–´ í™”ê°€ ë‚˜ì…¨êµ°ìš”. ì–´ë–»ê²Œ í•˜ë©´ ì‹ ì…ì‚¬ì›ì—ê²Œ í™”ë‚¬ìŒì„ í‘œí˜„í•  ...   \n",
              "...                                                  ...   \n",
              "51623                       ì§€ê¸ˆ í•  ìˆ˜ ìˆëŠ” ê°€ì¥ í•©ë¦¬ì ì¸ í–‰ë™ì€ ë¬´ì—‡ì¸ê°€ìš”?   \n",
              "51624                      ì–´ë–»ê²Œ í•˜ë©´ ì§€ê¸ˆì˜ ê¸°ë¶„ì„ ë‚˜ì•„ì§€ê²Œ í•  ìˆ˜ ìˆì„ê¹Œìš”?   \n",
              "51625                   ì§€ê¸ˆì˜ ìƒí™©ì—ì„œ í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ì¢‹ì€ í–‰ë™ì´ ë¬´ì—‡ì¼ê¹Œìš”?   \n",
              "51626                   ì§€ê¸ˆì˜ ê°ì •ì„ ë‚˜ì•„ì§€ê²Œ í•  ìˆ˜ ìˆëŠ” ì–´ë–¤ ë°©ë²•ì´ ìˆì„ê¹Œìš”?   \n",
              "51627                    ì–´ë–»ê²Œ í•˜ë©´ ì§€ê¸ˆì˜ ìƒí™©ì— ë³€í™”ë¥¼ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆì„ê¹Œìš”?   \n",
              "\n",
              "                                                    HS03  \\\n",
              "0                                                    NaN   \n",
              "1                                                    NaN   \n",
              "2                                                    NaN   \n",
              "3                                                    NaN   \n",
              "4                                                    NaN   \n",
              "...                                                  ...   \n",
              "51623                 ë…¸ë…„ì¸µì„ ìœ„í•œ ê²½ì œì  ì§€ì›ì´ë‚˜ ë¶€ì—… ê°™ì€ ê²ƒë„ ì•Œì•„ë³´ì•„ì•¼ê² ì–´.   \n",
              "51624            ë‚¨í¸ê³¼ í•¨ê»˜ ê²Œì´íŠ¸ë³¼ì´ë‚˜ ì¹˜ëŸ¬ ê°€ì•¼ê² ì–´. ê·¸ëŸ¼ ê¸°ë¶„ì´ ë‚˜ì•„ì§ˆ ê²ƒ ê°™ì•„.   \n",
              "51625      ë‚¨í¸ê³¼ í•¨ê»˜ ì‹¤ë²„ ì¼ìë¦¬ë‚˜ ë…¸ë…„ì¸µì„ ìœ„í•œ êµ­ê°€ ì§€ì›ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ì•„ì•¼ê² ì–´.   \n",
              "51626            í•¨ê»˜ ì¹œí•˜ê²Œ ì§€ë‚´ë˜ ë™ë„¤ ì–¸ë‹ˆ ë™ìƒë“¤ê³¼ ë¹ˆìë¦¬ë¥¼ ì¡°ê¸ˆì´ë‚˜ë§ˆ ì±„ìš¸ê¹Œ í•´.   \n",
              "51627  ì‚¬ëŒë“¤ì„ ë³¼ ë•Œ ì˜ì‹¬í•˜ê³  ë¶ˆì‹ í•˜ëŠ” ë§ˆìŒì„ ì–µëˆŒëŸ¬ì•¼ê² ì–´. ì‚¬ëŒë“¤ì„ ìƒ‰ì•ˆê²½ì„ ë¼ê³  ë³´ì§€...   \n",
              "\n",
              "                                     SS03  \n",
              "0                                     NaN  \n",
              "1                                     NaN  \n",
              "2                                     NaN  \n",
              "3                                     NaN  \n",
              "4                                     NaN  \n",
              "...                                   ...  \n",
              "51623                    ì¢‹ì€ ê²°ê³¼ ì–»ìœ¼ì‹œê¸¸ ë°”ë„ê²Œìš”.  \n",
              "51624         ë‚¨í¸ê³¼ í•¨ê»˜í•˜ëŠ” ì¢‹ì€ ì™¸ì¶œ ì‹œê°„ ë˜ì‹œê¸¸ ë°”ë„ê²Œìš”.  \n",
              "51625     ì¢‹ì€ ì •ë³´ ë§ì´ ì–»ìœ¼ì…”ì„œ ê±±ì •ì„ ì¢€ ëœìœ¼ì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”.  \n",
              "51626             ì§€ì¸ë¶„ë“¤ê³¼ ì¢‹ì€ ì‹œê°„ ë³´ë‚´ì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”.  \n",
              "51627  ì›í•˜ì‹œëŠ” ëŒ€ë¡œ ê°€ì§€ê³  ê³„ì‹œë˜ ê±±ì •ì´ ì˜ í•´ê²°ë˜ì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”.  \n",
              "\n",
              "[51628 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8343b4fb-c544-4ed5-ac34-87e3bbe1704c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>HS01</th>\n",
              "      <th>SS01</th>\n",
              "      <th>HS02</th>\n",
              "      <th>SS02</th>\n",
              "      <th>HS03</th>\n",
              "      <th>SS03</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>E1</td>\n",
              "      <td>ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.</td>\n",
              "      <td>ë§ì´ í˜ë“œì‹œê² ì–´ìš”. ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìˆë‚˜ìš”?</td>\n",
              "      <td>ê·¸ëƒ¥ ë‚´ê°€ í•´ê²°í•˜ëŠ” ê²Œ ë‚˜ì•„. ë‚¨ë“¤í•œí…Œ ë¶€ë‹´ ì£¼ê³  ì‹¶ì§€ë„ ì•Šê³ .</td>\n",
              "      <td>í˜¼ì í•´ê²°í•˜ê¸°ë¡œ í–ˆêµ°ìš”. í˜¼ìì„œ í•´ê²°í•˜ê¸° í˜ë“¤ë©´ ì£¼ìœ„ì— ì˜ë…¼í•  ì‚¬ëŒì„ ì°¾ì•„ë³´ì„¸ìš”.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>E1</td>\n",
              "      <td>ì´ë²ˆ ë‹¬ì— ë˜ ê¸‰ì—¬ê°€ ê¹ì˜€ì–´! ë¬¼ê°€ëŠ” ì˜¤ë¥´ëŠ”ë° ì›”ê¸‰ë§Œ ìê¾¸ ê¹ì´ë‹ˆê¹Œ ë„ˆë¬´ í™”ê°€ ë‚˜.</td>\n",
              "      <td>ê¸‰ì—¬ê°€ ì¤„ì–´ ì†ìƒí•˜ì‹œê² ì–´ìš”. ì›”ê¸‰ì´ ì¤„ì–´ë“  ê²ƒì„ ì–´ë–»ê²Œ ë³´ì™„í•˜ì‹¤ ê±´ê°€ìš”?</td>\n",
              "      <td>ìµœëŒ€í•œ ì§€ì¶œì„ ì–µì œí•´ì•¼ê² ì–´. ì›”ê¸‰ì´ ì¤„ì–´ë“¤ì—ˆìœ¼ë‹ˆ ê³ ì •ì§€ì¶œì„ ì¤„ì¼ ìˆ˜ë°–ì— ì—†ì„ ê²ƒ ê°™ì•„.</td>\n",
              "      <td>ì›”ê¸‰ì´ ì¤„ì–´ë“  ë§Œí¼ ì†Œë¹„ë¥¼ ì¤„ì¼ ê³„íšì´êµ°ìš”.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>E1</td>\n",
              "      <td>íšŒì‚¬ì— ì‹ ì…ì´ ë“¤ì–´ì™”ëŠ”ë° ë§íˆ¬ê°€ ê±°ìŠ¬ë ¤. ê·¸ëŸ° ì• ë¥¼ ë§¤ì¼ ë´ì•¼ í•œë‹¤ê³  ìƒê°í•˜ë‹ˆê¹Œ ìŠ¤...</td>\n",
              "      <td>íšŒì‚¬ ë™ë£Œ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ëŠ” ê²ƒ ê°™ì•„ìš”. ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì–´ë–¤ ë…¸ë ¥ì„ ...</td>\n",
              "      <td>ì˜ ì•ˆ ë§ëŠ” ì‚¬ëŒì´ë‘ ì–µì§€ë¡œ ì˜ ì§€ë‚´ëŠ” ê²ƒë³´ë‹¨ ì¡°ê¸ˆì€ ê±°ë¦¬ë¥¼ ë‘ê³  ì˜ˆì˜ë¥¼ ê°–ì¶°ì„œ ëŒ€...</td>\n",
              "      <td>ìŠ¤íŠ¸ë ˆìŠ¤ë°›ì§€ ì•Šê¸° ìœ„í•´ì„  ì¸ê°„ê´€ê³„ì— ìˆì–´ ì•½ê°„ì˜ ê±°ë¦¬ë¥¼ ë‘ëŠ” ê²Œ ì¢‹ê² êµ°ìš”.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>E1</td>\n",
              "      <td>ì§ì¥ì—ì„œ ë§‰ë‚´ë¼ëŠ” ì´ìœ ë¡œ ë‚˜ì—ê²Œë§Œ ì˜¨ê°– ì‹¬ë¶€ë¦„ì„ ì‹œì¼œ. ì¼ë„ ë§ì€ ë° ì •ë§ ë¶„í•˜ê³  ...</td>\n",
              "      <td>ê´€ë ¨ ì—†ëŠ” ì‹¬ë¶€ë¦„ì„ ëª¨ë‘ í•˜ê²Œ ë˜ì–´ì„œ ë…¸ì—¬ìš°ì‹œêµ°ìš”. ì–´ë–¤ ê²ƒì´ ìƒí™©ì„ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆ...</td>\n",
              "      <td>ì§ì¥ ì‚¬ëŒë“¤ê³¼ ì†”ì§í•˜ê²Œ ì´ì•¼ê¸°í•´ë³´ê³  ì‹¶ì–´. ì¼í•˜ëŠ” ë°ì— ë°©í•´ëœë‹¤ê³ .</td>\n",
              "      <td>ì§ì¥ ì‚¬ëŒë“¤ê³¼ ì´ì•¼ê¸°ë¥¼ í•´ ë³´ê² ë‹¤ê³  ê²°ì‹¬í•˜ì…¨êµ°ìš”.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>E1</td>\n",
              "      <td>ì–¼ë§ˆ ì „ ì…ì‚¬í•œ ì‹ ì…ì‚¬ì›ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ë„ˆë¬´ í™”ê°€ ë‚˜.</td>\n",
              "      <td>ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì€ íƒœë„ì— í™”ê°€ ë‚˜ì…¨êµ°ìš”. ìƒëŒ€ë°©ì˜ ì–´ë–¤ í–‰ë™ì´ ê·¸ëŸ° ê°ì •ì„ ìœ ë°œí•˜ëŠ”...</td>\n",
              "      <td>ìƒì‚¬ì¸ ë‚˜ì—ê²Œ ë¨¼ì € ì¸ì‚¬í•˜ì§€ ì•Šì•„ì„œ ë§¤ì¼ ë‚´ê°€ ë¨¼ì € ì¸ì‚¬í•œë‹¤ê³ !</td>\n",
              "      <td>í•­ìƒ ë¨¼ì € ì¸ì‚¬í•˜ê²Œ ë˜ì–´ í™”ê°€ ë‚˜ì…¨êµ°ìš”. ì–´ë–»ê²Œ í•˜ë©´ ì‹ ì…ì‚¬ì›ì—ê²Œ í™”ë‚¬ìŒì„ í‘œí˜„í•  ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51623</th>\n",
              "      <td>E1</td>\n",
              "      <td>ë‚˜ì´ê°€ ë¨¹ê³  ì´ì œ ëˆë„ ëª» ë²Œì–´ ì˜¤ë‹ˆê¹Œ ì–´ë–»ê²Œ ì‚´ì•„ê°€ì•¼ í• ì§€ ë§‰ë§‰í•´. ëŠ¥ë ¥ë„ ì—†ê³ .</td>\n",
              "      <td>ê²½ì œì ì¸ ë¬¸ì œ ë•Œë¬¸ì— ë§‰ë§‰í•˜ì‹œêµ°ìš”. ë§ˆìŒì´ í¸ì¹˜ ì•Šìœ¼ì‹œê² ì–´ìš”.</td>\n",
              "      <td>ì•„ë¬´ê²ƒë„ í•  ìˆ˜ ì—†ëŠ” ë‚´ê°€ ë¬´ê°€ì¹˜í•˜ê²Œ ëŠê»´ì§€ê³  ì‹¤ë§ìŠ¤ëŸ¬ì›Œ.</td>\n",
              "      <td>ì§€ê¸ˆ í•  ìˆ˜ ìˆëŠ” ê°€ì¥ í•©ë¦¬ì ì¸ í–‰ë™ì€ ë¬´ì—‡ì¸ê°€ìš”?</td>\n",
              "      <td>ë…¸ë…„ì¸µì„ ìœ„í•œ ê²½ì œì  ì§€ì›ì´ë‚˜ ë¶€ì—… ê°™ì€ ê²ƒë„ ì•Œì•„ë³´ì•„ì•¼ê² ì–´.</td>\n",
              "      <td>ì¢‹ì€ ê²°ê³¼ ì–»ìœ¼ì‹œê¸¸ ë°”ë„ê²Œìš”.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51624</th>\n",
              "      <td>E3</td>\n",
              "      <td>ëª¸ì´ ë§ì´ ì•½í•´ì¡Œë‚˜ ë´. ì´ì œ ì „ê³¼ ê°™ì´ ì¼í•˜ì§€ ëª»í•  ê²ƒ ê°™ì•„ ë„ˆë¬´ ì§œì¦ ë‚˜.</td>\n",
              "      <td>ê±´ê°•ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê¸°ë¶„ì´ ì¢‹ì§€ ì•Šìœ¼ì‹œêµ°ìš”. ì†ìƒí•˜ì‹œê² ì–´ìš”.</td>\n",
              "      <td>ë§ˆìŒ ê°™ì•„ì„œëŠ” ë‹¤ í•  ìˆ˜ ìˆëŠ” ì¼ì¸ë° ì´ì   ëª¸ì´ ì•ˆ ë”°ë¼ì™€ ì£¼ë‹ˆ í™”ë§Œ ë‚˜.</td>\n",
              "      <td>ì–´ë–»ê²Œ í•˜ë©´ ì§€ê¸ˆì˜ ê¸°ë¶„ì„ ë‚˜ì•„ì§€ê²Œ í•  ìˆ˜ ìˆì„ê¹Œìš”?</td>\n",
              "      <td>ë‚¨í¸ê³¼ í•¨ê»˜ ê²Œì´íŠ¸ë³¼ì´ë‚˜ ì¹˜ëŸ¬ ê°€ì•¼ê² ì–´. ê·¸ëŸ¼ ê¸°ë¶„ì´ ë‚˜ì•„ì§ˆ ê²ƒ ê°™ì•„.</td>\n",
              "      <td>ë‚¨í¸ê³¼ í•¨ê»˜í•˜ëŠ” ì¢‹ì€ ì™¸ì¶œ ì‹œê°„ ë˜ì‹œê¸¸ ë°”ë„ê²Œìš”.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51625</th>\n",
              "      <td>E4</td>\n",
              "      <td>ì´ì œ ì–´ë–»ê²Œ í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´. ë‚¨í¸ë„ ê·¸ë ‡ê³  ë…¸í›„ ì¤€ë¹„ë„ ì•ˆ ë˜ì–´ì„œ ë¯¸ë˜ê°€ ê±±ì •ë¼.</td>\n",
              "      <td>ë…¸í›„ ì¤€ë¹„ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê±±ì •ì´ ë§ìœ¼ì‹œê² ì–´ìš”.</td>\n",
              "      <td>ì£¼ë³€ ì‚¬ëŒë“¤ì€ ë‹¤ ë…¸í›„ ì¤€ë¹„ë„ ì˜í•´ë‘ì—ˆë˜ë° ë‚œ ì–´ë–»ê²Œ í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´. ë§‰ë§‰í•˜ê¸°...</td>\n",
              "      <td>ì§€ê¸ˆì˜ ìƒí™©ì—ì„œ í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ì¢‹ì€ í–‰ë™ì´ ë¬´ì—‡ì¼ê¹Œìš”?</td>\n",
              "      <td>ë‚¨í¸ê³¼ í•¨ê»˜ ì‹¤ë²„ ì¼ìë¦¬ë‚˜ ë…¸ë…„ì¸µì„ ìœ„í•œ êµ­ê°€ ì§€ì›ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ì•„ì•¼ê² ì–´.</td>\n",
              "      <td>ì¢‹ì€ ì •ë³´ ë§ì´ ì–»ìœ¼ì…”ì„œ ê±±ì •ì„ ì¢€ ëœìœ¼ì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51626</th>\n",
              "      <td>E3</td>\n",
              "      <td>ëª‡ì‹­ ë…„ì„ í•¨ê»˜ ì‚´ì•˜ë˜ ë‚¨í¸ê³¼ ì´í˜¼í–ˆì–´. ê·¸ë™ì•ˆì˜ ì„¸ì›”ì— ë°°ì‹ ê°ì„ ëŠë¼ê³  ë„ˆë¬´ í™”ê°€ ë‚˜.</td>\n",
              "      <td>ê°€ì¡±ê³¼ì˜ ë¬¸ì œ ë•Œë¬¸ì— ì†ìƒí•˜ì‹œê² ì–´ìš”.</td>\n",
              "      <td>ì´ì œ í•  ìˆ˜ ìˆëŠ” ì¼ë„ ì—†ê³  ì´ë ‡ê²Œ í˜ë“¤ê²Œ ì‚¬ëŠ” ê²Œ ë¶ˆë§ŒìŠ¤ëŸ½ê¸°ë§Œ í•´.</td>\n",
              "      <td>ì§€ê¸ˆì˜ ê°ì •ì„ ë‚˜ì•„ì§€ê²Œ í•  ìˆ˜ ìˆëŠ” ì–´ë–¤ ë°©ë²•ì´ ìˆì„ê¹Œìš”?</td>\n",
              "      <td>í•¨ê»˜ ì¹œí•˜ê²Œ ì§€ë‚´ë˜ ë™ë„¤ ì–¸ë‹ˆ ë™ìƒë“¤ê³¼ ë¹ˆìë¦¬ë¥¼ ì¡°ê¸ˆì´ë‚˜ë§ˆ ì±„ìš¸ê¹Œ í•´.</td>\n",
              "      <td>ì§€ì¸ë¶„ë“¤ê³¼ ì¢‹ì€ ì‹œê°„ ë³´ë‚´ì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51627</th>\n",
              "      <td>E4</td>\n",
              "      <td>ë‚¨í¸ê³¼ ê²°í˜¼í•œ ì§€ ì‚¬ì‹­ ë…„ì´ì•¼. ì´ì œ ì‚¬ëŒ ë§Œë‚˜ëŠ” ê²ƒë„ ë²„ê²ê³  ì•Œë˜ ì‚¬ëŒë„ ì ì  ì‚¬ë¼ì ¸.</td>\n",
              "      <td>ëŒ€ì¸ê´€ê³„ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê±±ì •ë˜ì‹œê³  ì†ìƒí•˜ì‹œê² ì–´ìš”.</td>\n",
              "      <td>ì‚¬ëŒë“¤ì„ ë§Œë‚˜ëŠ” ê²ƒì´ ì–´ë ¤ì›Œ. ìê¾¸ ì‚¬ëŒë“¤ì„ ì˜ì‹¬í•˜ê²Œë§Œ ë˜ê³  ë§ì´ì•¼.</td>\n",
              "      <td>ì–´ë–»ê²Œ í•˜ë©´ ì§€ê¸ˆì˜ ìƒí™©ì— ë³€í™”ë¥¼ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆì„ê¹Œìš”?</td>\n",
              "      <td>ì‚¬ëŒë“¤ì„ ë³¼ ë•Œ ì˜ì‹¬í•˜ê³  ë¶ˆì‹ í•˜ëŠ” ë§ˆìŒì„ ì–µëˆŒëŸ¬ì•¼ê² ì–´. ì‚¬ëŒë“¤ì„ ìƒ‰ì•ˆê²½ì„ ë¼ê³  ë³´ì§€...</td>\n",
              "      <td>ì›í•˜ì‹œëŠ” ëŒ€ë¡œ ê°€ì§€ê³  ê³„ì‹œë˜ ê±±ì •ì´ ì˜ í•´ê²°ë˜ì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>51628 rows Ã— 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8343b4fb-c544-4ed5-ac34-87e3bbe1704c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8343b4fb-c544-4ed5-ac34-87e3bbe1704c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8343b4fb-c544-4ed5-ac34-87e3bbe1704c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-88991d48-fd3f-49cc-9749-3bac9b291160\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-88991d48-fd3f-49cc-9749-3bac9b291160')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-88991d48-fd3f-49cc-9749-3bac9b291160 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_63a4a674-3ce4-4f9e-a138-338e20339e8b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_63a4a674-3ce4-4f9e-a138-338e20339e8b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 51628,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"E1\",\n          \"E6\",\n          \"E4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HS01\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 51600,\n        \"samples\": [\n          \"\\ub0a8\\ud3b8\\uc774 \\ubc8c\\uc5b4\\uc624\\ub294 \\ub3c8\\ub9cc\\uc73c\\ub85c\\ub294 \\ub109\\ub109\\uc9c0\\uac00 \\uc54a\\uc544\\uc11c \\uc77c\\uc744 \\uc54c\\uc544\\ubcf4\\uace0 \\uc788\\ub294\\ub370 \\ud560 \\uc218 \\uc788\\ub294 \\uc77c\\uc774 \\uc5c6\\uc5b4.\",\n          \"\\ub098\\ub294 \\ud328\\ubc30\\uc790\\uc57c.\",\n          \"\\uc624\\ub298 \\uc624\\ub79c\\ub9cc\\uc5d0 \\ub300\\ud559 \\ub3d9\\uae30\\ub4e4\\uacfc \\uc800\\ub141 \\uba39\\uae30\\ub85c \\ud55c \\ub0a0\\uc774\\uc57c. \\uac04\\ub9cc\\uc5d0 \\uce5c\\uad6c\\ub4e4 \\ubcfc \\uc0dd\\uac01\\ud558\\ub2c8 \\ub108\\ubb34 \\uae30\\ubd84 \\uc88b\\uc544.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SS01\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50352,\n        \"samples\": [\n          \"\\uac70\\uc561\\uc744 \\uc190\\uc5d0 \\uc950\\uac8c \\ub418\\uc5b4 \\ubd88\\uc548\\ud558\\uba74\\uc11c\\ub3c4 \\ub5a8\\ub9ac\\uc2dc\\uaca0\\uc5b4\\uc694.\",\n          \"\\ub0a8\\ud3b8\\ubd84\\uc774 \\uc9d1\\uc548\\uc77c\\uc744 \\uc798\\ud55c\\ub2e4\\uace0 \\uc8fc\\uc7a5\\ud574\\uc11c \\ud669\\ub2f9\\ud558\\uc2dc\\uad70\\uc694.\",\n          \"\\uacb0\\ud63c \\uc900\\ube44\\uc758 \\uc5b4\\ub5a4 \\ubd80\\ubd84\\uc5d0 \\ub9c8\\uc74c\\uc774 \\ubd88\\ud3b8\\ud558\\uc2e0\\uac00\\uc694? \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HS02\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50831,\n        \"samples\": [\n          \"\\ub0b4 \\ud314\\uc790\\ub294 \\ub3c4\\ub300\\uccb4 \\uc65c \\uc774\\ub7f0 \\uac74\\uc9c0. \\uadf8 \\uce5c\\uad6c\\ub294 \\ub0a0 \\ubc84\\ub9b0 \\uac83 \\uac19\\uc544.\",\n          \"\\uc77c\\ud558\\ub824\\ub294 \\uc758\\uc9c0\\ub3c4 \\ubcf4\\uc774\\uc9c0 \\uc54a\\uace0 \\uc790\\uc2e0\\uc758 \\ubcd1\\uc744 \\uc774\\uaca8\\ub0b4\\uace0 \\uadf9\\ubcf5\\ud558\\ub824\\ub294 \\uc5b4\\ub5a4 \\ub178\\ub825\\ub3c4 \\ud558\\uc9c0 \\uc54a\\uc544. \\uc2dd\\uad6c\\ub4e4\\uc774 \\uac71\\uc815\\uc744 \\ub9ce\\uc774 \\ud558\\ub354\\ub77c.\",\n          \"\\uc815\\ub9d0 \\uc88b\\uc544\\ud588\\ub358 \\uce5c\\uad6c\\uc778\\ub370 \\uc774\\ub7f0 \\uc2dd\\uc73c\\ub85c \\uba40\\uc5b4\\uc838\\uc11c \\uce5c\\uad6c\\uc5d0\\uac8c \\uc8c4\\ucc45\\uac10\\uc774 \\ub4e4\\uc5b4.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SS02\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 47875,\n        \"samples\": [\n          \"\\uc560\\ub4e4\\ud55c\\ud14c \\ub9d0\\uc744 \\uac78\\uc5b4\\ub3c4 \\ub2f5\\ud574\\uc8fc\\uc9c0 \\uc54a\\uace0 \\ub098\\ud55c\\ud14c \\uc544\\ubb34\\ub3c4 \\ub9d0\\uc744 \\uac78\\uc9c0 \\uc54a\\ub294\\uad70\\uc694. \\uc774 \\uc0c1\\ud669\\uc744 \\ubcc0\\ud654\\uc2dc\\ud0a4\\uae30 \\uc704\\ud574\\uc11c \\uc2dc\\ub3c4\\ud574 \\ubcfc \\uc218 \\uc788\\ub294 \\uc77c\\ub4e4\\uc774 \\uc5b4\\ub5a4 \\uac83\\uc774 \\uc788\\uc744\\uae4c\\uc694?\",\n          \"\\ub108\\ubb34 \\ub180\\ub77c\\uc168\\uaca0\\uc5b4\\uc694. \\uc5b4\\ub5bb\\uac8c \\ud558\\uc2e4 \\uc0dd\\uac01\\uc774\\uc138\\uc694?\",\n          \"\\ud300\\uc7a5\\ub2d8\\uc774 \\ub4e0\\ub4e0\\ud558\\uc154\\uc11c \\uae30\\ubd84\\uc774 \\uc88b\\uc73c\\uc2e0\\uac83 \\uac19\\uc544\\uc694.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HS03\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42294,\n        \"samples\": [\n          \"\\uc5f0\\uc560\\ub97c \\ub2e4 \\uc2e4\\ud328\\ud588\\uc5b4.\",\n          \"\\uc785\\uc6d0 \\uc911\\uc77c \\ub54c \\ud2c8\\ud2c8\\uc774 \\uacf5\\ubd80\\ud574\\uc57c\\uaca0\\uc5b4.\",\n          \"\\ucc98\\uc74c\\uc5d0\\ub294 \\uace0\\ub9d9\\ub2e4\\uac00 \\ub098\\uc911\\uc5d0\\ub294 \\uc9dc\\uc99d\\uc774 \\ub098\\ub354\\ub77c\\uace0.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SS03\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41451,\n        \"samples\": [\n          \"\\uc798\\ud558\\uc168\\uc5b4\\uc694. \\uadf8\\ub7f0 \\uc0c1\\ud669\\uc5d0\\uc11c \\uce5c\\uad6c\\ub97c \\ub3c4\\uc640\\uc8fc\\ub294 \\uac74 \\uba4b\\uc9c4 \\uc77c\\uc774\\uc5d0\\uc694.\",\n          \"\\ucde8\\ubbf8\\ud65c\\ub3d9\\uc744 \\ud1b5\\ud574\\uc11c \\ub098\\uc758 \\ub9c8\\uc74c\\uc5d0 \\uc5ec\\uc720\\uac00 \\uc0dd\\uae30\\uae38 \\ubc14\\ub77c\\uc694.\",\n          \"\\uc6a9\\ub3c8\\uc73c\\ub85c \\uce5c\\uad6c\\ub4e4\\uacfc \\ud568\\uaed8 \\ubcf4\\ub0bc \\uc0dd\\uac01\\uc774\\uc2dc\\uad70\\uc694.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import re\n",
        "import os\n",
        "\n",
        "# ì¶”ê°€ ì“°ê¸°ëª¨ë“œë¡œ í…ìŠ¤íŠ¸ íŒŒì¼ ì—´ê¸°\n",
        "with open('train.txt', 'a', encoding='utf-8') as f:\n",
        "  for text in df['HS01'] :\n",
        "        text = str(text)\n",
        "        try:\n",
        "            f.write(text+'\\n')\n",
        "        except:\n",
        "                pass\n",
        "\n",
        "\n",
        "# ì €ì¥ ê²½ë¡œ ìƒì„±\n",
        "os.makedirs('./model', exist_ok=True)\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input='train.txt',                    # í…ìŠ¤íŠ¸ ë­‰ì¹˜ íŒŒì¼\n",
        "    model_prefix='./model/spm_krsent',    # ì¶œë ¥ ëª¨ë¸ íŒŒì¼ ì´ë¦„\n",
        "    vocab_size=2000                       # í† í° ê°œìˆ˜\n",
        ")\n"
      ],
      "metadata": {
        "id": "laI4fhrKcO6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SPDataSet(Dataset):\n",
        "    def __init__(self, sp, max_len):\n",
        "        self.max_len = max_len\n",
        "        self.df = pd.read_csv(f'./train.csv')[['HS01','SS01']]\n",
        "        self.sp = sp\n",
        "\n",
        "    def zero_pad(self, tok):\n",
        "        if len(tok) >= self.max_len:\n",
        "            return tok[:self.max_len]\n",
        "        else:\n",
        "            padding = np.zeros(self.max_len)\n",
        "            padding[:len(tok)] = tok\n",
        "            return padding\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.df))\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        sent = self.df.iloc[i]\n",
        "        sent1 = self.sp.encode_as_ids(sent['HS01'])\n",
        "        sent2 = self.sp.encode_as_ids(sent['SS01'])\n",
        "\n",
        "        inp = self.zero_pad(sent1 + [self.sp.eos_id()])\n",
        "        tar = self.zero_pad([self.sp.bos_id()] + sent2 + [self.sp.eos_id()])\n",
        "\n",
        "        return torch.Tensor(inp), torch.Tensor(tar)\n",
        "\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file=f'./model/spm_krsent.model')\n",
        "dataset = SPDataSet(sp, 60)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "for inp, tar in dataloader:\n",
        "    print(inp.long())\n",
        "    print(tar.long()[:,:-1])\n",
        "    print(tar.long()[:,1:])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlSBvhmwXsRO",
        "outputId": "94ebfd8c-b613-4aae-e834-ad23fe04a428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  18,   66,    5,   54,  999,   29,  280,  825,    7,  923,  252,   46,\n",
            "            3, 1971,  789,  338,  340,    4,    2,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])\n",
            "tensor([[   1,   66,   29,  280,  825,  923, 1319,   12,  429,  270, 1888,  826,\n",
            "           14,  520,  433,    4,    2,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])\n",
            "tensor([[  66,   29,  280,  825,  923, 1319,   12,  429,  270, 1888,  826,   14,\n",
            "          520,  433,    4,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ëª¨ë¸ í•™ìŠµ\n",
        "\n",
        "Transformer êµ¬ì¡°ëŠ” Seq2seqì™€ ë‹¤ë¥´ê²Œ ë””ì½”ë”ê°€ íƒ€ê²Ÿ í† í°ì„ í•œë²ˆì— í•™ìŠµí•˜ê³  ì˜ˆì¸¡ëœ í† í°ì„ ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ë°˜í™˜í•©ë‹ˆë‹¤.  \n",
        "ë°˜í™˜ëœ ì‹œí€€ìŠ¤ ê¸¸ì´ì˜ ì˜ˆì¸¡ í† í°ë“¤ì„ í•œë²ˆì— ì†ì‹¤ ì—°ì‚° í•˜ê¸° ìœ„í•´ ë°°ì¹˜í¬ê¸°ì™€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê³±í•˜ì—¬ í˜•ìƒì„ ë³€í™˜í•©ë‚˜ë‹¤.\n",
        "\n"
      ],
      "metadata": {
        "id": "71LnD9n3wAD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# batch_size, seq_len => batch_size * seq_len í•˜ì—¬ ì†ì‹¤ ì—°ì‚°\n",
        "def loss_function(real, pred, pad_token=0):\n",
        "    # pred shape: (batch_size, seq_len, vocab_size)\n",
        "    # real shape: (batch_size, seq_len)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token)  # pad=0 ë¬´ì‹œ\n",
        "    # reshape to (batch_size * seq_len, vocab_size)\n",
        "    pred_reshaped = pred.view(-1, pred.size(-1))\n",
        "    real_reshaped = real.reshape(-1)\n",
        "\n",
        "    return loss_fn(pred_reshaped, real_reshaped)\n",
        "\n",
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "sp = spm.SentencePieceProcessor(model_file=f'./model/spm_krsent.model')\n",
        "num_layers = 4\n",
        "d_model = 128                       # emd_dim\n",
        "dff = 256                           # feed_dim\n",
        "num_heads = 4\n",
        "vocab_size = sp.get_piece_size()\n",
        "pe_input = 10000\n",
        "pe_target = 10000\n",
        "dropout_rate = 0.1\n",
        "max_len = 60\n",
        "pad_token = 0\n",
        "lr = 2e-4\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# ëª¨ë¸ ìƒì„±\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                            vocab_size, vocab_size,\n",
        "                            pe_input, pe_target, dropout_rate).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# ê°„ëµí•œ ì‹¤í—˜ì„ ìœ„í•´ í‰ê°€ë°ì´í„° ë¶„ë¦¬ ìƒëµ\n",
        "dataset = SPDataSet(sp, max_len)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for e in range(epochs):\n",
        "    transformer.train()\n",
        "    total_loss = 0.0\n",
        "    for i, (inp, tar) in enumerate(dataloader):\n",
        "        inp = inp.long().to(device)\n",
        "        tar = tar.long().to(device)\n",
        "\n",
        "        tar_in = tar[:, :-1]\n",
        "        tar_out = tar[:, 1:]\n",
        "        optimizer.zero_grad()\n",
        "        pred, _ = transformer(inp, tar_in, pad_token=pad_token)\n",
        "        loss = loss_function(tar_out, pred, pad_token=pad_token)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 200 == 0:\n",
        "            print(f\"Epoch: {e}, Batch: {i+1}, Loss: {total_loss/(i+1)}\")\n",
        "    print(f\"====>Epoch: {e}, Loss: {total_loss/(i+1)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEjFK8hfUp9I",
        "outputId": "7080112a-b06b-43a1-cba5-9522ebd63c72",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Batch: 200, Loss: 5.200121765136719\n",
            "Epoch: 0, Batch: 400, Loss: 4.6147064781188964\n",
            "Epoch: 0, Batch: 600, Loss: 4.299499713182449\n",
            "Epoch: 0, Batch: 800, Loss: 4.092008906304836\n",
            "====>Epoch: 0, Loss: 4.085637493499888\n",
            "Epoch: 1, Batch: 200, Loss: 3.2925089275836945\n",
            "Epoch: 1, Batch: 400, Loss: 3.236822310090065\n",
            "Epoch: 1, Batch: 600, Loss: 3.1873928618431093\n",
            "Epoch: 1, Batch: 800, Loss: 3.146705023944378\n",
            "====>Epoch: 1, Loss: 3.1454577339626155\n",
            "Epoch: 2, Batch: 200, Loss: 2.940320063829422\n",
            "Epoch: 2, Batch: 400, Loss: 2.911688976287842\n",
            "Epoch: 2, Batch: 600, Loss: 2.8843137228488924\n",
            "Epoch: 2, Batch: 800, Loss: 2.8610846373438834\n",
            "====>Epoch: 2, Loss: 2.860541569108266\n",
            "Epoch: 3, Batch: 200, Loss: 2.7388914477825166\n",
            "Epoch: 3, Batch: 400, Loss: 2.7181621623039245\n",
            "Epoch: 3, Batch: 600, Loss: 2.6974220196406047\n",
            "Epoch: 3, Batch: 800, Loss: 2.6788124108314513\n",
            "====>Epoch: 3, Loss: 2.6787941098360917\n",
            "Epoch: 4, Batch: 200, Loss: 2.5630456447601317\n",
            "Epoch: 4, Batch: 400, Loss: 2.5610054504871367\n",
            "Epoch: 4, Batch: 600, Loss: 2.556186930735906\n",
            "Epoch: 4, Batch: 800, Loss: 2.5419135370850565\n",
            "====>Epoch: 4, Loss: 2.5414328696263917\n",
            "Epoch: 5, Batch: 200, Loss: 2.454189521074295\n",
            "Epoch: 5, Batch: 400, Loss: 2.444712650179863\n",
            "Epoch: 5, Batch: 600, Loss: 2.4382340673605603\n",
            "Epoch: 5, Batch: 800, Loss: 2.434461889863014\n",
            "====>Epoch: 5, Loss: 2.4343018156592877\n",
            "Epoch: 6, Batch: 200, Loss: 2.3681314384937284\n",
            "Epoch: 6, Batch: 400, Loss: 2.358032006621361\n",
            "Epoch: 6, Batch: 600, Loss: 2.352237575451533\n",
            "Epoch: 6, Batch: 800, Loss: 2.3483638215065\n",
            "====>Epoch: 6, Loss: 2.3483090158436526\n",
            "Epoch: 7, Batch: 200, Loss: 2.2764690071344376\n",
            "Epoch: 7, Batch: 400, Loss: 2.2775130605697633\n",
            "Epoch: 7, Batch: 600, Loss: 2.275119791428248\n",
            "Epoch: 7, Batch: 800, Loss: 2.2738512712717056\n",
            "====>Epoch: 7, Loss: 2.2743381842361505\n",
            "Epoch: 8, Batch: 200, Loss: 2.2154870396852493\n",
            "Epoch: 8, Batch: 400, Loss: 2.2172581949830055\n",
            "Epoch: 8, Batch: 600, Loss: 2.212039446632067\n",
            "Epoch: 8, Batch: 800, Loss: 2.2093588548898695\n",
            "====>Epoch: 8, Loss: 2.209001415871924\n",
            "Epoch: 9, Batch: 200, Loss: 2.1514542323350905\n",
            "Epoch: 9, Batch: 400, Loss: 2.150042112469673\n",
            "Epoch: 9, Batch: 600, Loss: 2.155378230611483\n",
            "Epoch: 9, Batch: 800, Loss: 2.1544239789247515\n",
            "====>Epoch: 9, Loss: 2.1542868441365464\n",
            "Epoch: 10, Batch: 200, Loss: 2.110554273724556\n",
            "Epoch: 10, Batch: 400, Loss: 2.1048589411377905\n",
            "Epoch: 10, Batch: 600, Loss: 2.1059990749756494\n",
            "Epoch: 10, Batch: 800, Loss: 2.10670303478837\n",
            "====>Epoch: 10, Loss: 2.1062764353320795\n",
            "Epoch: 11, Batch: 200, Loss: 2.0732287323474883\n",
            "Epoch: 11, Batch: 400, Loss: 2.0669481283426285\n",
            "Epoch: 11, Batch: 600, Loss: 2.0640924088160197\n",
            "Epoch: 11, Batch: 800, Loss: 2.064623154103756\n",
            "====>Epoch: 11, Loss: 2.0642742728064345\n",
            "Epoch: 12, Batch: 200, Loss: 2.004902036190033\n",
            "Epoch: 12, Batch: 400, Loss: 2.021442282795906\n",
            "Epoch: 12, Batch: 600, Loss: 2.0256308335065842\n",
            "Epoch: 12, Batch: 800, Loss: 2.026638896018267\n",
            "====>Epoch: 12, Loss: 2.0269880867949794\n",
            "Epoch: 13, Batch: 200, Loss: 1.982023382782936\n",
            "Epoch: 13, Batch: 400, Loss: 1.9894710174202919\n",
            "Epoch: 13, Batch: 600, Loss: 1.9941767438252767\n",
            "Epoch: 13, Batch: 800, Loss: 1.991280926167965\n",
            "====>Epoch: 13, Loss: 1.9917445903669355\n",
            "Epoch: 14, Batch: 200, Loss: 1.9556094413995744\n",
            "Epoch: 14, Batch: 400, Loss: 1.9547484877705574\n",
            "Epoch: 14, Batch: 600, Loss: 1.9594590038061142\n",
            "Epoch: 14, Batch: 800, Loss: 1.96155979052186\n",
            "====>Epoch: 14, Loss: 1.9616741182783426\n",
            "Epoch: 15, Batch: 200, Loss: 1.9229072886705398\n",
            "Epoch: 15, Batch: 400, Loss: 1.9309763664007187\n",
            "Epoch: 15, Batch: 600, Loss: 1.9315330320596695\n",
            "Epoch: 15, Batch: 800, Loss: 1.9320478701591492\n",
            "====>Epoch: 15, Loss: 1.9324253790145174\n",
            "Epoch: 16, Batch: 200, Loss: 1.8829070609807967\n",
            "Epoch: 16, Batch: 400, Loss: 1.8980214369297028\n",
            "Epoch: 16, Batch: 600, Loss: 1.9011003412803014\n",
            "Epoch: 16, Batch: 800, Loss: 1.9067918346822261\n",
            "====>Epoch: 16, Loss: 1.9067203076798676\n",
            "Epoch: 17, Batch: 200, Loss: 1.8842127650976181\n",
            "Epoch: 17, Batch: 400, Loss: 1.8832293313741684\n",
            "Epoch: 17, Batch: 600, Loss: 1.8815728543202082\n",
            "Epoch: 17, Batch: 800, Loss: 1.882027843594551\n",
            "====>Epoch: 17, Loss: 1.882498927866807\n",
            "Epoch: 18, Batch: 200, Loss: 1.8425597608089448\n",
            "Epoch: 18, Batch: 400, Loss: 1.8470877051353454\n",
            "Epoch: 18, Batch: 600, Loss: 1.8538507803281148\n",
            "Epoch: 18, Batch: 800, Loss: 1.8598067423701286\n",
            "====>Epoch: 18, Loss: 1.859940063406839\n",
            "Epoch: 19, Batch: 200, Loss: 1.823163486123085\n",
            "Epoch: 19, Batch: 400, Loss: 1.8292119926214219\n",
            "Epoch: 19, Batch: 600, Loss: 1.8338203581174215\n",
            "Epoch: 19, Batch: 800, Loss: 1.8389678463339805\n",
            "====>Epoch: 19, Loss: 1.8387982851215159\n",
            "Epoch: 20, Batch: 200, Loss: 1.8029517424106598\n",
            "Epoch: 20, Batch: 400, Loss: 1.809139285683632\n",
            "Epoch: 20, Batch: 600, Loss: 1.815976397395134\n",
            "Epoch: 20, Batch: 800, Loss: 1.8185889776051045\n",
            "====>Epoch: 20, Loss: 1.8188767301696531\n",
            "Epoch: 21, Batch: 200, Loss: 1.791360068321228\n",
            "Epoch: 21, Batch: 400, Loss: 1.7988495475053787\n",
            "Epoch: 21, Batch: 600, Loss: 1.7957249885797502\n",
            "Epoch: 21, Batch: 800, Loss: 1.8000156128406524\n",
            "====>Epoch: 21, Loss: 1.7999590627913729\n",
            "Epoch: 22, Batch: 200, Loss: 1.7745511835813523\n",
            "Epoch: 22, Batch: 400, Loss: 1.7745803412795067\n",
            "Epoch: 22, Batch: 600, Loss: 1.7792718837658563\n",
            "Epoch: 22, Batch: 800, Loss: 1.7836436550319195\n",
            "====>Epoch: 22, Loss: 1.7835360790569394\n",
            "Epoch: 23, Batch: 200, Loss: 1.7469940304756164\n",
            "Epoch: 23, Batch: 400, Loss: 1.7588829335570335\n",
            "Epoch: 23, Batch: 600, Loss: 1.7653867942094803\n",
            "Epoch: 23, Batch: 800, Loss: 1.7673084300756454\n",
            "====>Epoch: 23, Loss: 1.7670398551735294\n",
            "Epoch: 24, Batch: 200, Loss: 1.7310836684703828\n",
            "Epoch: 24, Batch: 400, Loss: 1.7453922614455224\n",
            "Epoch: 24, Batch: 600, Loss: 1.7470447903871535\n",
            "Epoch: 24, Batch: 800, Loss: 1.7508001965284348\n",
            "====>Epoch: 24, Loss: 1.7509332425532287\n",
            "Epoch: 25, Batch: 200, Loss: 1.7247499781847\n",
            "Epoch: 25, Batch: 400, Loss: 1.728232364654541\n",
            "Epoch: 25, Batch: 600, Loss: 1.7313891573747\n",
            "Epoch: 25, Batch: 800, Loss: 1.7357490685582162\n",
            "====>Epoch: 25, Loss: 1.7360386598804478\n",
            "Epoch: 26, Batch: 200, Loss: 1.6982008749246598\n",
            "Epoch: 26, Batch: 400, Loss: 1.707686759531498\n",
            "Epoch: 26, Batch: 600, Loss: 1.7136512013276417\n",
            "Epoch: 26, Batch: 800, Loss: 1.7214347717165948\n",
            "====>Epoch: 26, Loss: 1.7214570940648757\n",
            "Epoch: 27, Batch: 200, Loss: 1.697967507839203\n",
            "Epoch: 27, Batch: 400, Loss: 1.699032846391201\n",
            "Epoch: 27, Batch: 600, Loss: 1.701604361931483\n",
            "Epoch: 27, Batch: 800, Loss: 1.7069074037671088\n",
            "====>Epoch: 27, Loss: 1.7072552735034419\n",
            "Epoch: 28, Batch: 200, Loss: 1.6858920931816102\n",
            "Epoch: 28, Batch: 400, Loss: 1.684916661977768\n",
            "Epoch: 28, Batch: 600, Loss: 1.6918825960159303\n",
            "Epoch: 28, Batch: 800, Loss: 1.6967547595500947\n",
            "====>Epoch: 28, Loss: 1.6965090187065663\n",
            "Epoch: 29, Batch: 200, Loss: 1.664450141787529\n",
            "Epoch: 29, Batch: 400, Loss: 1.672325361073017\n",
            "Epoch: 29, Batch: 600, Loss: 1.6794642625252405\n",
            "Epoch: 29, Batch: 800, Loss: 1.6830289335548878\n",
            "====>Epoch: 29, Loss: 1.6830534988176424\n",
            "Epoch: 30, Batch: 200, Loss: 1.6490230745077132\n",
            "Epoch: 30, Batch: 400, Loss: 1.6605391263961793\n",
            "Epoch: 30, Batch: 600, Loss: 1.6689208795626957\n",
            "Epoch: 30, Batch: 800, Loss: 1.671910847723484\n",
            "====>Epoch: 30, Loss: 1.6720071289471385\n",
            "Epoch: 31, Batch: 200, Loss: 1.6315273815393447\n",
            "Epoch: 31, Batch: 400, Loss: 1.6465330943465233\n",
            "Epoch: 31, Batch: 600, Loss: 1.6528750385840734\n",
            "Epoch: 31, Batch: 800, Loss: 1.660282676666975\n",
            "====>Epoch: 31, Loss: 1.6598000906066233\n",
            "Epoch: 32, Batch: 200, Loss: 1.6311411666870117\n",
            "Epoch: 32, Batch: 400, Loss: 1.641254090666771\n",
            "Epoch: 32, Batch: 600, Loss: 1.6418811545769374\n",
            "Epoch: 32, Batch: 800, Loss: 1.64880339294672\n",
            "====>Epoch: 32, Loss: 1.6490998573905917\n",
            "Epoch: 33, Batch: 200, Loss: 1.6144672697782516\n",
            "Epoch: 33, Batch: 400, Loss: 1.625854689180851\n",
            "Epoch: 33, Batch: 600, Loss: 1.633987726767858\n",
            "Epoch: 33, Batch: 800, Loss: 1.6383151756227017\n",
            "====>Epoch: 33, Loss: 1.638403207691481\n",
            "Epoch: 34, Batch: 200, Loss: 1.6094027590751647\n",
            "Epoch: 34, Batch: 400, Loss: 1.6135050871968268\n",
            "Epoch: 34, Batch: 600, Loss: 1.6232298135757446\n",
            "Epoch: 34, Batch: 800, Loss: 1.6286490331590175\n",
            "====>Epoch: 34, Loss: 1.6284174489265923\n",
            "Epoch: 35, Batch: 200, Loss: 1.60568219602108\n",
            "Epoch: 35, Batch: 400, Loss: 1.6091566547751426\n",
            "Epoch: 35, Batch: 600, Loss: 1.6164260814587275\n",
            "Epoch: 35, Batch: 800, Loss: 1.619515336751938\n",
            "====>Epoch: 35, Loss: 1.6194035969997722\n",
            "Epoch: 36, Batch: 200, Loss: 1.5946266424655915\n",
            "Epoch: 36, Batch: 400, Loss: 1.5988871940970422\n",
            "Epoch: 36, Batch: 600, Loss: 1.603387480378151\n",
            "Epoch: 36, Batch: 800, Loss: 1.6095391449332237\n",
            "====>Epoch: 36, Loss: 1.60931858106469\n",
            "Epoch: 37, Batch: 200, Loss: 1.5803225028514862\n",
            "Epoch: 37, Batch: 400, Loss: 1.5857029849290847\n",
            "Epoch: 37, Batch: 600, Loss: 1.59388725399971\n",
            "Epoch: 37, Batch: 800, Loss: 1.6003499187529087\n",
            "====>Epoch: 37, Loss: 1.6005865688513055\n",
            "Epoch: 38, Batch: 200, Loss: 1.5773566299676895\n",
            "Epoch: 38, Batch: 400, Loss: 1.5808048689365386\n",
            "Epoch: 38, Batch: 600, Loss: 1.5868393915891648\n",
            "Epoch: 38, Batch: 800, Loss: 1.5914652687311173\n",
            "====>Epoch: 38, Loss: 1.5919392786947768\n",
            "Epoch: 39, Batch: 200, Loss: 1.5612821990251542\n",
            "Epoch: 39, Batch: 400, Loss: 1.5714523014426232\n",
            "Epoch: 39, Batch: 600, Loss: 1.5774475385745366\n",
            "Epoch: 39, Batch: 800, Loss: 1.5827462768554688\n",
            "====>Epoch: 39, Loss: 1.5828847202669731\n",
            "Epoch: 40, Batch: 200, Loss: 1.5545589971542357\n",
            "Epoch: 40, Batch: 400, Loss: 1.5644536831974982\n",
            "Epoch: 40, Batch: 600, Loss: 1.5683082820971808\n",
            "Epoch: 40, Batch: 800, Loss: 1.5754319116473199\n",
            "====>Epoch: 40, Loss: 1.575556525510483\n",
            "Epoch: 41, Batch: 200, Loss: 1.5394318419694901\n",
            "Epoch: 41, Batch: 400, Loss: 1.5498017022013664\n",
            "Epoch: 41, Batch: 600, Loss: 1.5590070207913718\n",
            "Epoch: 41, Batch: 800, Loss: 1.5660994364321232\n",
            "====>Epoch: 41, Loss: 1.566431158094158\n",
            "Epoch: 42, Batch: 200, Loss: 1.5385364037752152\n",
            "Epoch: 42, Batch: 400, Loss: 1.5468676453828811\n",
            "Epoch: 42, Batch: 600, Loss: 1.5528940949837367\n",
            "Epoch: 42, Batch: 800, Loss: 1.5583014695346356\n",
            "====>Epoch: 42, Loss: 1.5580286192332649\n",
            "Epoch: 43, Batch: 200, Loss: 1.528628647327423\n",
            "Epoch: 43, Batch: 400, Loss: 1.5348699992895127\n",
            "Epoch: 43, Batch: 600, Loss: 1.5465172686179478\n",
            "Epoch: 43, Batch: 800, Loss: 1.5519294196367264\n",
            "====>Epoch: 43, Loss: 1.551974258162866\n",
            "Epoch: 44, Batch: 200, Loss: 1.5200986069440843\n",
            "Epoch: 44, Batch: 400, Loss: 1.5267141735553742\n",
            "Epoch: 44, Batch: 600, Loss: 1.536494634548823\n",
            "Epoch: 44, Batch: 800, Loss: 1.5440229161083698\n",
            "====>Epoch: 44, Loss: 1.5442488072382325\n",
            "Epoch: 45, Batch: 200, Loss: 1.5137908780574798\n",
            "Epoch: 45, Batch: 400, Loss: 1.5264863240718842\n",
            "Epoch: 45, Batch: 600, Loss: 1.53772784948349\n",
            "Epoch: 45, Batch: 800, Loss: 1.5382683877646923\n",
            "====>Epoch: 45, Loss: 1.5385697066710045\n",
            "Epoch: 46, Batch: 200, Loss: 1.4984876000881195\n",
            "Epoch: 46, Batch: 400, Loss: 1.5141043347120284\n",
            "Epoch: 46, Batch: 600, Loss: 1.5238924425840379\n",
            "Epoch: 46, Batch: 800, Loss: 1.530638802945614\n",
            "====>Epoch: 46, Loss: 1.5308788525866162\n",
            "Epoch: 47, Batch: 200, Loss: 1.4975734239816665\n",
            "Epoch: 47, Batch: 400, Loss: 1.5066537338495254\n",
            "Epoch: 47, Batch: 600, Loss: 1.518200157880783\n",
            "Epoch: 47, Batch: 800, Loss: 1.5252958466112614\n",
            "====>Epoch: 47, Loss: 1.524993681553128\n",
            "Epoch: 48, Batch: 200, Loss: 1.4954845643043517\n",
            "Epoch: 48, Batch: 400, Loss: 1.5041237819194793\n",
            "Epoch: 48, Batch: 600, Loss: 1.5126806537310282\n",
            "Epoch: 48, Batch: 800, Loss: 1.5176623405516148\n",
            "====>Epoch: 48, Loss: 1.5173854519148002\n",
            "Epoch: 49, Batch: 200, Loss: 1.4748855596780777\n",
            "Epoch: 49, Batch: 400, Loss: 1.4893252229690552\n",
            "Epoch: 49, Batch: 600, Loss: 1.5035827559232713\n",
            "Epoch: 49, Batch: 800, Loss: 1.5108555264770984\n",
            "====>Epoch: 49, Loss: 1.5112560445343548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Seq2seq ëª¨ë¸ì— ë¹„í•´ ë”ìš± ë¹ ë¥¸ ì†ë„ë¡œ ì†ì‹¤ì´ ì¤„ì–´ë“œëŠ”ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¶©ë¶„í•œ í•™ìŠµì´ ì´ë£¨ì–´ì§€ë©´ ì ì ˆí•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "AFXrAooK-BZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ìƒì„± í…ìŠ¤íŠ¸ í‰ê°€í•˜ê¸°\n",
        "\n",
        "- BLEU(Bilingual Evaluation Understudy)\n",
        "    * n-gram ê¸°ë°˜ìœ¼ë¡œ ì°¸ì¡° ë¬¸ì¥ê³¼ ìƒì„± ë¬¸ì¥ ê°„ **ì •ë°€ë„(Precision)**ë¥¼ ì¸¡ì •í•˜ë©°, ì§§ì€ ë¬¸ì¥ í˜ë„í‹°(Brevity Penalty) ë“±ì„ í†µí•´ ì‹¤ì œ ë²ˆì—­ í’ˆì§ˆì„ ë°˜ì˜í•˜ë„ë¡ ê³ ì•ˆëœ ì§€í‘œ\n",
        "    * ì£¼ë¡œ ê¸°ê³„ ë²ˆì—­ì—ì„œ ëª¨ë¸ì´ ì›ë¬¸ì„ ì–¼ë§ˆë‚˜ ì •í™•íˆ ë²ˆì—­í–ˆëŠ”ì§€ í‰ê°€í•  ë•Œ ì‚¬ìš©\n"
      ],
      "metadata": {
        "id": "zrKY6svDxL6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BLEU í‰ê°€\n",
        "\n",
        "- ê° n-gramë³„ë¡œ **(ìƒì„± ë¬¸ì¥ì— ë“±ì¥í•˜ëŠ” n-gram ì¤‘ ì°¸ì¡°ì™€ ì¼ì¹˜í•˜ëŠ” íšŸìˆ˜) / (ìƒì„± ë¬¸ì¥ ì „ì²´ n-gram ìˆ˜)**ë¥¼ êµ¬í•´ ì •ë°€ë„ë¥¼ ê³„ì‚°\n",
        "\n",
        "- 1,2,3,4-gram ì •ë°€ë„ë¥¼ ê¸°í•˜í‰ê· ìœ¼ë¡œ í•©ì‚°í•˜ê³ , Brevity Penaltyë¥¼ ê³±í•´ ìµœì¢… BLEU ìŠ¤ì½”ì–´ê°€ ì™„ì„±\n",
        "\n",
        "- ìƒì„± ë¬¸ì¥ì´ ì§§ì„ìˆ˜ë¡ n-gram ê°œìˆ˜ê°€ ì ì–´ ì •ë°€ë„ê°€ ë†’ì•„ì§€ëŠ” ë¬¸ì œ ë•Œë¬¸ì— Brevity Penalty(ì§€ìˆ˜ ê°ì†Œ)ë¥¼ ì ìš©í•˜ì—¬ íŒ¨ë„í‹°ë¥¼ ë¶€ì—¬\n",
        "\n",
        "- ë¬¸ì¥ì´ ì§§ì•„ 3-gram,4-gram ì—ì„œ ë¶ˆì¼ì¹˜ê°€ ë§ì„ ë•Œ, BLEU ìŠ¤ì½”ì–´ê°€ 0ìœ¼ë¡œ ê°€ëŠ” í˜„ìƒì„ ì™„í™”í•˜ê¸° ìœ„í•´ ìŠ¤ë¬´ë”© ê¸°ë²•(SmoothingFunction)ì„ ì ìš©.\n",
        "    * `SmoothingFunction.method1` : 1-gram, 2-gram, ... n-gram ì •ë°€ë„ì— ëŒ€í•´, ë¶„ëª¨ì— 1ì„ ë”í•˜ê³  ë¶„ìì—ë„ 1ì„ ë”í•˜ëŠ” ë°©ì‹(+1 smoothing)."
      ],
      "metadata": {
        "id": "_956oq6pSLUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
        "\n",
        "\n",
        "hypo  = [\"ë‚˜ëŠ”\", \"ì €ë…ì—\", \"ë°¥ì„\", \"ë¨¹ì—ˆ\", \".\"]\n",
        "ref   = [[\"ë‚˜ëŠ”\", \"ì €ë…ì—\", \"ë°¥ì„\", \"ë¨¹ì—ˆë‹¤\", \".\"]]\n",
        "bleu_1gram = sentence_bleu(ref, hypo, weights=(1, 0, 0, 0))\n",
        "bleu_2gram = sentence_bleu(ref, hypo, weights=(0, 1, 0, 0))\n",
        "bleu_3gram = sentence_bleu(ref, hypo, weights=(0, 0, 1, 0))\n",
        "bleu_4gram = sentence_bleu(ref, hypo, weights=(0, 0, 0, 1))\n",
        "bleu_scroe = sentence_bleu(ref, hypo, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "\n",
        "print(\"BLEU score 1g:\", bleu_1gram)\n",
        "print(\"BLEU score 2g:\", bleu_2gram)\n",
        "print(\"BLEU score 3g:\", bleu_3gram)\n",
        "print(\"BLEU score 4g:\", bleu_4gram)\n",
        "print(\"BLEU score :\", bleu_scroe)\n",
        "\n",
        "\n",
        "smooth = SmoothingFunction().method1\n",
        "bleu_smooth = sentence_bleu(ref, hypo, smoothing_function=smooth)\n",
        "print(\"BLEU score smooth:\", bleu_smooth)"
      ],
      "metadata": {
        "id": "1YWwxfDQEL7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f890567-949b-4237-8761-34dffd1ee1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score 1g: 0.8\n",
            "BLEU score 2g: 0.5\n",
            "BLEU score 3g: 0.3333333333333333\n",
            "BLEU score 4g: 2.2250738585072626e-308\n",
            "BLEU score : 7.380245217279165e-78\n",
            "BLEU score smooth: 0.28574404296988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_token = transformer.generate(eco_inp, 2, 3, 0, device=device)\n",
        "print(f'ìµœì¢… ì¶œë ¥ í˜•ìƒ {out.shape}')\n",
        "print(f'í† í° ìƒì„± {generated_token}')"
      ],
      "metadata": {
        "id": "v4nO-G0k3Vmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e54c25b-ba4d-4512-8995-01887a82cd1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìµœì¢… ì¶œë ¥ í˜•ìƒ torch.Size([1, 10, 16])\n",
            "í† í° ìƒì„± [2, 6, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate í•¨ìˆ˜ëŠ” í•˜ë‚˜ì˜ ë°°ì¹˜ë§Œ ì…ë ¥\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "all_preds=[]\n",
        "all_tars=[]\n",
        "transformer.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (inp, tar) in enumerate(dataloader):\n",
        "        inp = inp.long().to(device)\n",
        "        tar = tar.long().to(device)\n",
        "\n",
        "        # ì‹œì‘ í† í°ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±\n",
        "        pred_ids = transformer.generate(inp, sp.bos_id(), sp.eos_id(), pad_token=pad_token, device=device)\n",
        "        target_ids = tar.detach().cpu().tolist()\n",
        "\n",
        "        # ì•Œìˆ˜ì—†ìŒ í† í° (unk) ì œê±°\n",
        "        pred_ids = [id for id in pred_ids if id != sp.unk_id()]\n",
        "        target_ids = [id for id in target_ids[0] if id != sp.unk_id()]\n",
        "\n",
        "        # ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜\n",
        "        pred_tokens = sp.decode(pred_ids)\n",
        "        tar_tokens = sp.decode(target_ids)\n",
        "        print(f'pred: {pred_tokens}')\n",
        "        print(f'tar: {tar_tokens} \\n')\n",
        "\n",
        "        all_preds.append(pred_tokens)\n",
        "        all_tars.append([tar_tokens])\n",
        "        if i > 10:\n",
        "            break\n",
        "\n",
        "# ìƒì„± ë¬¸ì¥ì˜ ì „ì²´ í‰ê·  BLEU ì—°ì‚°\n",
        "bleu_score = corpus_bleu(all_tars, all_preds, smoothing_function=smooth)\n",
        "print(f'bleu_score:{bleu_score}')"
      ],
      "metadata": {
        "id": "aPnU9SckIqsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdcc90c0-a5b7-410a-e258-11e11c53d48c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred: ì„ ìƒë‹˜ì´ ì‚¬ìš©ìë‹˜ì„ ëª°ë¼ì„œ ì†ìƒí•˜ì…¨êµ°ìš”.\n",
            "tar: ì„ ìƒë‹˜ì´ ëŒ€ë‹µí•´ ì£¼ì§€ ì•Šì•„ ì†ìƒí•˜ì…¨êµ°ìš”. \n",
            "\n",
            "pred: ë‚˜ì´ê°€ ë“œëŠ” ì™œ ê±±ì •ì´ ë˜ì‹œë‚˜ìš”?\n",
            "tar: ìµœê·¼ ê±±ì •í•  ì¼ì´ ìˆìœ¼ì…¨ë‚˜ìš”? \n",
            "\n",
            "pred: ì¹œêµ¬ì™€ ì—¬í–‰ì„ ê°€ê¸°ë¡œ í•˜ì…¨êµ°ìš”.\n",
            "tar: ì—¬í–‰ì„ ê°ˆ ê³„íšì´ ìˆìœ¼ì‹œêµ°ìš”. ì–´ë–¤ ê¸°ë¶„ì´ì‹ ê°€ìš”? \n",
            "\n",
            "pred: ì•„ë‚´ê°€ ì„ì‹ í•˜ì…¨êµ°ìš”.\n",
            "tar: ì•„ë‚´ê°€ ì„ì‹ í•˜ì…¨êµ°ìš”! \n",
            "\n",
            "pred: ìƒë‹´í•˜ê³  ìƒë‹´ì„ ë°›ìœ¼ì…¨êµ°ìš”.\n",
            "tar: ê·¸ëŸ¬ì…¨êµ°ìš”. ì£¼ë¡œ ì–´ë–¤ ëŒ€í™”ë¥¼ í•˜ê³  ì˜¤ì…¨ë‚˜ìš”? \n",
            "\n",
            "pred: ê°€ì¡±ì„ ë§ì´ ì‹ ê²½ ì¨ì£¼ì‹œëŠ”êµ°ìš”.\n",
            "tar: ì–´ë–¤ ë°©ë²•ìœ¼ë¡œ ì‹ ê²½ì„ ë§ì´ ì“°ì„¸ìš”? \n",
            "\n",
            "pred: ë‚¯ì„  ì‚¬ëŒë“¤ì„ ë§Œë‚˜ê³  ê³„ì‹œëŠ”êµ°ìš”.\n",
            "tar: ì‚´ê³  ìˆëŠ” ë™ë„¤ì— ë§Œì¡±ê°ì´ ì—„ì²­ ë†’ìœ¼ì‹œë„¤ìš”. ì£¼ë³€ ë¶„ë“¤ì´ ê·¸ë ‡ê²Œ ì¢‹ìœ¼ì„¸ìš”? \n",
            "\n",
            "pred: ë‚¨ìì¹œêµ¬ì™€ ë‹¤íˆ¬ì‹œëŠ” ì¼ì´ ìˆìœ¼ì…¨êµ°ìš”.\n",
            "tar: ë‚¨ìì¹œêµ¬ì™€ ìˆ˜í•™ì—¬í–‰ ì´ì•¼ê¸°ë¥¼ í•˜ë‹¤ê°€ ì˜ê²¬ì´ ë§ì§€ ì•Šì•„ ê±±ì •ë˜ì‹œëŠ”êµ°ìš”. \n",
            "\n",
            "pred: ì™œ ê·¸ë ‡ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?\n",
            "tar: ì§œì¦ë‚˜ì‹œê² ì–´ìš”. í—Œë° ì™œ ë•Œë¦¬ëŠ” ê±´ê°€ìš”? \n",
            "\n",
            "pred: ì•½ì„ ë¬¼ì–´ì•¼ í•˜ëŠ” ì•½ì„ ë¬¼ì–´ì•¼ í•˜ëŠ”êµ°ìš”.\n",
            "tar: ì•½ì„ ë§ì´ ë“œì…”ì•¼ í•´ì„œ ë‚œì²˜í•˜ì‹œêµ°ìš”. \n",
            "\n",
            "pred: ë‚¨ìì¹œêµ¬ê°€ ë©‹ì§„ ë‚¨ì ì¹œêµ¬ê°€ ë©‹ì§„ë‹¤ëŠ” ê²ƒì„ ì•Œê²Œ ë˜ì–´ ì¢‹ìœ¼ì‹œêµ°ìš”.\n",
            "tar: ì˜ˆë¹„ ë‚¨í¸ì´ ë©‹ì ¸ì„œ ê°ì‚¬í•œ ë§ˆìŒì´ ë“œëŠ”êµ°ìš”. \n",
            "\n",
            "pred: í•™êµ í­ë ¥ì„ ë‹¹í•´ì„œ í•™êµ í­ë ¥ì„ ë‹¹í•˜ì…¨êµ°ìš”.\n",
            "tar: í•™êµ í­ë ¥ì„ ë³´ê³  ë¶„ë…¸ë¥¼ ëŠë¼ì…¨êµ°ìš”. \n",
            "\n",
            "bleu_score:0.21316968646492845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ê¸°ê³„ ë²ˆì—­ì´ ì•„ë‹Œ, ì¢€ë” ì¼ë°˜ì ì¸ ìì—°ì–´ ìƒì„± ë¬¸ì œì—ì„œëŠ” BLUEê°™ì€ n-gram Overlap ë°©ì‹ ë³´ë‹¤ ë¬¸ì¥ ìœ ì‚¬ë„ë¥¼ íŒŒì•… í• ìˆ˜ ìˆëŠ” ì‹ ê²½ë§ ì„ë² ë”© ê¸°ë°˜ ì§€í‘œê°€ ë§ì´ í™œìš©"
      ],
      "metadata": {
        "id": "N8CTI9BX5dQF"
      }
    }
  ]
}