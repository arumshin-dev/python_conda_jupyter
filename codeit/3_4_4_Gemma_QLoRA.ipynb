{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a3e577023c50437c809c68eb732aa4e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_a91845755b274692a1f7fbfac1f1b99d"
          }
        },
        "fd2896da61f64d08bafe9b3a4a5b6bb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9208fc030ec24e67b20b26ef4a6d521c",
            "placeholder": "​",
            "style": "IPY_MODEL_b6fb3818c3a84957bd3bc656b1c7dc7c",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "c1f4f3a840fa4bd9bb6c29d7f02bcab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c63237b693de451c8ad40d1201215ef6",
            "placeholder": "​",
            "style": "IPY_MODEL_66b4779ecd9f4f9aae0015ebc7fc0491",
            "value": ""
          }
        },
        "c372ee36a7614cbca08c2f201a4adcb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_676ce4d438a64dee9469494331c752b7",
            "style": "IPY_MODEL_7f9af833d8244cf18f0e1cef7c327456",
            "value": true
          }
        },
        "d71cfba00bf6484e9b2fe637d43f78d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_00aafdfdd2b5439c9fb93dbf93adcfb9",
            "style": "IPY_MODEL_a8731b1d1c614c23b9d60aef9746fe69",
            "tooltip": ""
          }
        },
        "6850fef059164acbb72a99519bf86bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_026e07da9b044293b9d380c9d7639035",
            "placeholder": "​",
            "style": "IPY_MODEL_91580b4d6a8e420db41647f610b7ab26",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "a91845755b274692a1f7fbfac1f1b99d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "9208fc030ec24e67b20b26ef4a6d521c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6fb3818c3a84957bd3bc656b1c7dc7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c63237b693de451c8ad40d1201215ef6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66b4779ecd9f4f9aae0015ebc7fc0491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "676ce4d438a64dee9469494331c752b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f9af833d8244cf18f0e1cef7c327456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00aafdfdd2b5439c9fb93dbf93adcfb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8731b1d1c614c23b9d60aef9746fe69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "026e07da9b044293b9d380c9d7639035": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91580b4d6a8e420db41647f610b7ab26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b34794b377241b48c15853404bdd6ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5ba58e0b3dd4b84aba5aeb09034bd7a",
            "placeholder": "​",
            "style": "IPY_MODEL_7011abde29544818a9d460a677641b52",
            "value": "Connecting..."
          }
        },
        "b5ba58e0b3dd4b84aba5aeb09034bd7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7011abde29544818a9d460a677641b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumshin-dev/python_conda_jupyter/blob/main/codeit/3_4_4_Gemma_QLoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lora 기법을 활용한 PEFT\n",
        "\n",
        "LLM과 같은 대규모 모델에서 더욱 효율 적인 미세조정 즉, PEFT(Parameter-Efficient Fine-Tuning)을 하기위한 가장 대표적인 방식이 바로 Lora 방식 입니다.\n",
        "\n",
        "해당 예시에서는 허깅페이스로 모델을 불러온뒤 LoraConfig를 활용하여 구글의 gemma3 모델을 한국어 뉴스 요약 테스크로 PEFT 진행하는 과정을 살펴봅니다.\n"
      ],
      "metadata": {
        "id": "k547qPbZfAOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 초기화 과정\n",
        "\n",
        "#### 라이브러리 설치\n",
        "\n",
        "1. `transformers > 4.50.1` : gemma3 가 포함된 최신 transformers 버전\n",
        "2. `bitsandbytes` : 양자화를 위한 라이브러리\n",
        "3. `trl` : SFTTrainer를 활용하여 학습하기 위한 라이브러리\n",
        "4. `datasets` :  Hugging Face Hub에 저장된 데이터세트를 불러오기 위한 라이브러리\n",
        "\n",
        "다음 라이브러리 설치후 세션 재시작\n"
      ],
      "metadata": {
        "id": "jFH75rz1fLXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hPz0-P6kZnNx",
        "outputId": "b766426a-163e-44a2-9152-b2594b0a4953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Collecting trl\n",
            "  Downloading trl-0.25.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.12/dist-packages (1.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.11.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (1.16.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->matplotlib-venn) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.25.1-py3-none-any.whl (465 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.5/465.5 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes, trl, evaluate\n",
            "Successfully installed bitsandbytes-0.48.2 evaluate-0.4.6 trl-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install \"transformers\" \"bitsandbytes\" \"trl\" \"datasets\" \"evaluate\" \"matplotlib-venn\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "허깅페이스 로그인"
      ],
      "metadata": {
        "id": "YXxLRIE-fabK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "a3e577023c50437c809c68eb732aa4e0",
            "fd2896da61f64d08bafe9b3a4a5b6bb2",
            "c1f4f3a840fa4bd9bb6c29d7f02bcab3",
            "c372ee36a7614cbca08c2f201a4adcb4",
            "d71cfba00bf6484e9b2fe637d43f78d6",
            "6850fef059164acbb72a99519bf86bd8",
            "a91845755b274692a1f7fbfac1f1b99d",
            "9208fc030ec24e67b20b26ef4a6d521c",
            "b6fb3818c3a84957bd3bc656b1c7dc7c",
            "c63237b693de451c8ad40d1201215ef6",
            "66b4779ecd9f4f9aae0015ebc7fc0491",
            "676ce4d438a64dee9469494331c752b7",
            "7f9af833d8244cf18f0e1cef7c327456",
            "00aafdfdd2b5439c9fb93dbf93adcfb9",
            "a8731b1d1c614c23b9d60aef9746fe69",
            "026e07da9b044293b9d380c9d7639035",
            "91580b4d6a8e420db41647f610b7ab26",
            "4b34794b377241b48c15853404bdd6ef",
            "b5ba58e0b3dd4b84aba5aeb09034bd7a",
            "7011abde29544818a9d460a677641b52"
          ]
        },
        "id": "uicALGKnfZj1",
        "outputId": "ef4df6ae-af68-4d7a-cf22-d50f3a578cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3e577023c50437c809c68eb732aa4e0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Gemma 모델 PEFT\n",
        "\n",
        "[Gemma3](https://developers.googleblog.com/en/gemma-explained-whats-new-in-gemma-3/) 모델은 구글에서 Gemini 모델을 만드는 데 사용된 것과 동일한 연구 및 기술로 구축한 GPT 기반의 최신 텍스트 생성 모델입니다.\n",
        "\n",
        "Gemma3 모델은 멀티모달로, 텍스트와 이미지 입력을 처리하고 텍스트 출력을 생성가능 합니다. 예시에서는 이미지 입력은 활용하지 않고 텍스트 입력만을 활용하여 문장 요약에 최적화된 모델로 미세조정 합니다.\n",
        "\n",
        "먼저 모델을 가져오기 전에 [Gemma3](https://huggingface.co/google/gemma-3-1b-it)에서 **라이센스 동의** 후, 로그인 해주어야 합니다."
      ],
      "metadata": {
        "id": "HrocTC27fkN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemma 모델 설명**\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Kr2S_7ufYiiW9uC84OgSWKNfMX12qLhf\" width=500 />\n",
        "\n",
        "\n",
        "https://huggingface.co/models?sort=trending&search=gemma\n",
        "\n",
        "https://www.youtube.com/watch?v=qcjrduz_YS8\n"
      ],
      "metadata": {
        "id": "lCSkv3tQJyC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 양자화 하여 모델 로드\n",
        "\n",
        "LLM 모델을 더욱 특정 테스크에 최적화하여 더욱 효율 적으로 사용하기 위해 모델의 크기를 줄이는 **경량화** 작업을 진행할 수 있습니다.  \n",
        "\n",
        "모델의 경령화 작업중 가장 간단하고 효율적인 방법중 하나가 사용 비트가 큰 실수(float64)타입으로 저장되는 파라미터를 작은 비트의 정수(int4, int8)등으로 줄이는 **양자화** 입니다.\n",
        "\n",
        "예) \"32비트 숫자를 4비트 숫자로 압축해서 메모리 절약\""
      ],
      "metadata": {
        "id": "jl5Ig77_fmzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"google/gemma-3-1b-it\"\n",
        "\n",
        "# 4비트 양자화: https://huggingface.co/docs/transformers/ko/quantization/bitsandbytes#normal-float-4-(nf4)\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                        # 가중치를 4-bit 양자화\n",
        "    bnb_4bit_compute_dtype=torch.float16,     # 연산 텐서는 float16 사용\n",
        "    bnb_4bit_quant_type=\"nf4\",                # NormalFloat4 양자화 타입 (분포를 유연하게)\n",
        "    bnb_4bit_use_double_quant=True            # 더블 양자화 (양자화 노이즈를 보다 잘 보정하려는 목적)\n",
        ")\n",
        "\n",
        "# 양자화를 적용하여 모델 로드: https://huggingface.co/docs/transformers/en/main_classes/model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,    # 양자화 적용\n",
        "    attn_implementation=\"eager\"                 # 안정적인 eager 어텐션 사용\n",
        ").eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5nfd0Bsfjm_",
        "outputId": "688e937f-a3ac-48b9-dd65-f774ddce354d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemma3ForCausalLM(\n",
            "  (model): Gemma3TextModel(\n",
            "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-25): 26 x Gemma3DecoderLayer(\n",
            "        (self_attn): Gemma3Attention(\n",
            "          (q_proj): Linear4bit(in_features=1152, out_features=1024, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=1152, out_features=256, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=1152, out_features=256, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=1024, out_features=1152, bias=False)\n",
            "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "        )\n",
            "        (mlp): Gemma3MLP(\n",
            "          (gate_proj): Linear4bit(in_features=1152, out_features=6912, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=1152, out_features=6912, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=6912, out_features=1152, bias=False)\n",
            "          (act_fn): GELUTanh()\n",
            "        )\n",
            "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
            "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
            "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
            "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
            "    (rotary_emb): Gemma3RotaryEmbedding()\n",
            "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 텍스트 생성해보기\n",
        "\n",
        "양자화된 모델로 부터 텍스트가 잘 생성되는지 테스트해 봅니다.  \n",
        "\n",
        "`gemma-3-1b-it` 모델은 챗봇 역할로써 한번 미세조정이 된 모델이므로 입력 프롬프트 템플릿에 맞춰 텍스트를 입력해주어 좀더 명확하게 사용할 수 있습니다.\n",
        "\n",
        "AutoTokenizer로 토크나이져를 생성한 후 `apply_chat_template()` 함수를 통해 역할이 지정된 프롬프트 텍스트를 토큰화 하고 모델 입력에 맞게 구성해 줍니다."
      ],
      "metadata": {
        "id": "F5godu19leia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "messages = [\n",
        "    [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"마크다운 표기 없이 일반 텍스트 형식으로 답변 작성\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"LLM과 허깅페이스에 대한 간략한 설명\"\n",
        "        },\n",
        "    ],\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device).to(torch.bfloat16)\n",
        "\n",
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It3e2XxrlhFG",
        "outputId": "b166d77a-b828-492c-8dc1-5d872a51e4a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Attempting to cast a BatchEncoding to type torch.bfloat16. This is not supported.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[     2,    105,   2364,    107, 238098, 238572, 164809,  41176, 237351,\n",
              "          89551,  75834, 236743, 243007,  34718, 184412,   7246, 231757, 107854,\n",
              "            108,   2182, 236792, 237842,  92475, 246489, 204451, 237223,  32102,\n",
              "          43255, 241866, 237384,  59587,    106,    107,    105,   4368,    107]],\n",
              "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`generate()` 함수를 통해 텍스트를 생성해 봅니다.   \n",
        "inputs에 배치 형상이 포함되었기 때문에 `batch_decode()` 함수로 디코딩을 합니다."
      ],
      "metadata": {
        "id": "1Id7c1WHmKHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    outputs = model.generate(**inputs,\n",
        "                             max_new_tokens=150,\n",
        "                             no_repeat_ngram_size=2,\n",
        "                             early_stopping=True)\n",
        "\n",
        "outputs = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
        "print(outputs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "033WjVxzmKid",
        "outputId": "14fa1258-7e00-4445-896f-b07b9609775f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "마크다운 표기 없이 일반 텍스트 형식으로 답변 작성\n",
            "\n",
            "LLM과 허깅페이스에 대한 간략한 설명<end_of_turn>\n",
            "<start_of_turn>model\n",
            "## LLM(Large Language Model)과 Hugging Face에 대해 간단히 설명\n",
            "\n",
            "**LLMs (Large language Models)이란?**\n",
            "\n",
            "*   **정의:** 방대한 양의 훈련 데이터를 기반으로 학습된 인공지능 모델로, 인간과 유사한 language(언어)를 생성하고 이해할 수 있습니다. \n",
            "* **기능:** 챗봇처럼 다양한 종류의 질문에 답변하거나, 톤앤매너를 조절하여 글을 작성하거나 번역하는 등, 다양한 작업들을 수행할 수도 있습니다.(텍st로 작성)\n",
            "    *  **예시:** GPT-3, BERT, Llama 등 유명 LLMs이 있습니다 (다양한 종류\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 요약 데이터세트 구성\n",
        "\n",
        "이번 예시에서는 요약 테스크로 미세조정을 하기위해 허깅페이스 Dataset 허브의 [naver-news-summarization-ko](https://huggingface.co/datasets/uiyong/naver_news_summarization_ko_df) 데이터를 `datasets` 라이브러리를 통해 불러오겠습니다."
      ],
      "metadata": {
        "id": "1ReH2rnPnLUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "load_dataset으로 로딩\n",
        "허깅페이스 Dataset 허브에 업로드된 데이터는 datasets의 load_dataset() 함수를 통해 쉽게 가져 올 수 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "y6glfJuZnMKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOBh8wfGnPEA",
        "outputId": "645be550-4321-4a5f-c4b3-c6bdb7606ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
              "        num_rows: 22194\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
              "        num_rows: 2466\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
              "        num_rows: 2740\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI3zCOvoogZP",
        "outputId": "7cc30d3c-9cb6-4082-8a5a-d134e69eee45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'date': '2022-07-03 17:14:37',\n",
              " 'category': 'economy',\n",
              " 'press': 'YTN ',\n",
              " 'title': '추경호 중기 수출지원 총력 무역금융 40조 확대',\n",
              " 'document': '앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.',\n",
              " 'link': 'https://n.news.naver.com/mnews/article/052/0001759333?sid=101',\n",
              " 'summary': '올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.'}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 챗봇 프롬프트 형식 변환\n",
        "\n",
        "gemma-3-1b-it은 챗봇 형식의 프롬프트를 입력으로 받으므로 프롬프트 변환 함수를 정의하고 데이터세트에 `map()` 함수를 통해 변환 과정을 진행합니다.  \n",
        "\n",
        "또한 `remove_columns` 인자를 통해 기존 데이터세트에 존재한 컬럼을 전부 제거합니다."
      ],
      "metadata": {
        "id": "C3KcpiTRojdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_conversation(sample):\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": sample[\"document\"]},\n",
        "            {\"role\":\"assistant\", \"content\": sample[\"summary\"]}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "train_data = dataset['train'].map(create_conversation, remove_columns=dataset['train'].features, batched=False)\n",
        "test_data = dataset['test'].map(create_conversation, remove_columns=dataset['test'].features, batched=False)\n",
        "train_data[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sE7H9t_oizq",
        "outputId": "481b5bdd-3334-4cca-f7f8-f2be69cc014e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'content': '앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.',\n",
              "   'role': 'user'},\n",
              "  {'content': '올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.',\n",
              "   'role': 'assistant'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lora 옵션 세팅\n",
        "\n",
        "Lora는 효율적인 파라미터 업데이트를 위해 각 projection 레이어층에 저차원 행렬 분해가 이루어진 Adapter 층을 추가하고 해당 Adapter 레이어만 학습을 진행합니다.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1aF59YFbuOUM32_ZComXLroBkhNV1bAav\" width=\"600\"/></center>\n",
        "\n",
        "학습이 완료되면 Adapter 층의 분해 행렬을 행렬곱 연산하여 기존 project 레이어와 동일 형상으로 만든뒤 더하기 연산을 통해 가중치를 합칩니다.\n",
        "\n",
        "\n",
        "\n",
        "허깅페이스에서 Lora 방식으로 미세조정을 하기 위해서 peft 모듈의 `LoraConfig` 객체를 통해 Lora 옵션을 설정해 줍니다.\n",
        "\n",
        "- r(Rank)\n",
        "\n",
        "    * LoRA에서 추가 학습 파라미터를 저장할 때 사용되는 저랭크(low-rank) 차원의 크기\n",
        "\n",
        "    * (hidden_size × hidden_size) => (hidden_size × r) × (r × hidden_size)\n",
        "\n",
        "    * r이 클수록 모델이 표현할 수 있는 변화(적응) 폭이 커지지만, 그만큼 추가 파라미터가 늘어나고 계산량이 많아짐\n",
        "\n",
        "- lora_alpha\n",
        "\n",
        "    * LoRA에서 추가로 학습되는 저랭크 가중치 행렬을 스케일링 하기위한 변수 LoRA출력 × (lora_alpha / r)\n",
        "\n",
        "    * 모델이 학습 초기에 급격하게 파라미터 업데이트를 하지 않도록 조정하거나, 학습 후반부에 미세한 조정을 가능케 하는 등 학습 안정성과 성능에 영향을 줌\n",
        "\n",
        "- target_modules\n",
        "\n",
        "    * LoRA를 삽입할 내부 레이어 계층 이름을 입력\n",
        "\n",
        "    * LoRA는 주로 어텐션 계층의 선형 레이어(projection)에 삽입하여 활용\n",
        "\n",
        "    * \"all-linear\" 를 입력하여 모델의 모든 projection 레이어에 추가 가능\n"
      ],
      "metadata": {
        "id": "w_js1S1orJN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                    # LoRA rank: 저차원 분해의 차원 수\n",
        "                             # 값이 클수록 표현력↑, 학습 파라미터↑\n",
        "                             # 일반적 범위: 8~64\n",
        "                             # 16은 성능과 효율의 균형점\n",
        "\n",
        "    lora_alpha=8,            # LoRA 스케일링 계수\n",
        "                             # 실제 적용 시 (lora_alpha/r) 비율로 스케일링\n",
        "                             # 여기서는 8/16 = 0.5배로 적용\n",
        "                             # 보통 r과 같거나 r/2 정도로 설정\n",
        "\n",
        "    lora_dropout=0.05,       # LoRA 레이어에 적용할 드롭아웃 비율\n",
        "                             # 과적합 방지 (5% 확률로 뉴런 비활성화)\n",
        "                             # 일반적 범위: 0.05~0.1\n",
        "\n",
        "    target_modules=[         # LoRA를 적용할 레이어 지정\n",
        "        \"q_proj\",            # Query 프로젝션 (Attention의 Q)\n",
        "        \"k_proj\",            # Key 프로젝션 (Attention의 K)\n",
        "        \"v_proj\",            # Value 프로젝션 (Attention의 V)\n",
        "        \"o_proj\"             # Output 프로젝션 (Attention 결과 변환)\n",
        "    ],                       # Attention의 핵심 4개 가중치 행렬만 튜닝\n",
        "                             # MLP는 동결 → 메모리/시간 절약\n",
        "\n",
        "    task_type=\"CAUSAL_LM\"    # 태스크 유형: 인과적 언어 모델\n",
        "                             # GPT 스타일의 자기회귀 생성 모델\n",
        "                             # 다른 옵션: \"SEQ_CLS\"(분류), \"SEQ_2_SEQ_LM\"(번역) 등\n",
        ")\n"
      ],
      "metadata": {
        "id": "RJ9efMAxrKz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LoRA task type (Low-Rank Adaptation)**\n",
        "\n",
        "- task_type=\"SEQ_CLS\"\n",
        "    - 시퀀스 분류 (Sequence Classification)\n",
        "    - 모델: BERT, RoBERTa 등\n",
        "\n",
        "- task_type=\"SEQ_2_SEQ_LM\"\n",
        "    - 시퀀스-투-시퀀스 언어 모델링\n",
        "    - 모델: T5, BART, mT5 등\n",
        "\n",
        "- task_type=\"CAUSAL_LM\"\n",
        "    - 인과적(자기회귀) 언어 모델링\n",
        "    - 모델: GPT, Llama, Gemma 등\n",
        "\n",
        "https://huggingface.co/learn/smol-course/ko/unit3/2\n"
      ],
      "metadata": {
        "id": "G-uLFsVXsaJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PEFT 학습 진행\n",
        "\n",
        "AutoModelForCausalLM 처럼 교사 주도(supervised) 학습 방식을 사용하는 생성형 모델은 trl 모듈의 `SFTConfig(Supervised Fine-Tuning Config)` 를 통해 하이퍼파라미터를 설정하고 `SFTTrainer`로 쉽게 허깅페이스 프레임에 맞춰 데이터를 학습할 수 있습니다.\n",
        "\n",
        "BERT 모델 예시에서 사용한 `TrainingArguments`, `Trainer`를 상속하여 확장된 객체이기 때문에 기본적으로 동일한 방식으로 사용하며, 기존 파라미터에 텍스트 생성 모델의 미세조정에 특화된 파라미터를 추가로 활용할 수 있습니다.  \n",
        "\n",
        "\n",
        "https://huggingface.co/docs/trl/sft_trainer"
      ],
      "metadata": {
        "id": "PsozfT2BsQKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SFTConfig 하이퍼파라미터 설정**\n",
        "\n",
        "TrainingArguments 과 동일한 인자 이름으로 모델 학습에 필요한 하이퍼파라미터를 설정합니다.\n",
        "\n",
        "- TrainingArguments를 상속받아 확장\n",
        "- LLM Fine-tuning 특화\n"
      ],
      "metadata": {
        "id": "5d2K8e88tuiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "save_model = \"gemma-text-to-sum4\"\n",
        "\n",
        "args = SFTConfig(\n",
        "    # === 기본 설정 ===\n",
        "    output_dir=save_model,                  # 학습된 모델을 저장할 폴더 이름\n",
        "    num_train_epochs=1,                     # 전체 데이터를 몇 번 반복 학습할지 (1번만)\n",
        "    eval_strategy=\"no\",                     # 평가 전략을 'epoch'으로 설정하여 save_strategy와 일치시킴\n",
        "    do_eval=False,                          # 평가 활성화\n",
        "\n",
        "    # === 배치 처리 ===\n",
        "    per_device_train_batch_size=2,          # GPU가 한 번에 처리할 샘플 개수 (2개씩)\n",
        "    gradient_accumulation_steps=4,          # 4번 모아서 한 번에 업데이트 (실제로는 2×4=8개)\n",
        "\n",
        "    # === 메모리 관리 ===\n",
        "    gradient_checkpointing=False,           # 메모리 절약 기능 끄기 (속도 우선)\n",
        "    fp16=True,                              # 16비트 사용 (32비트의 절반, 메모리↓ 속도↑)\n",
        "\n",
        "    # === 학습 속도 ===\n",
        "    learning_rate=2e-4,                     # 학습 보폭 (0.0002, 한 번에 얼마나 크게 변화)\n",
        "    optim=\"adamw_torch_fused\",              # 옵티마이저 종류 (학습 방법, 빠른 버전)\n",
        "    lr_scheduler_type=\"constant\",           # 학습률 고정 (처음부터 끝까지 동일)\n",
        "    warmup_ratio=0.03,                      # 처음 3%는 천천히 시작 (급발진 방지)\n",
        "\n",
        "    # === 안정성 ===\n",
        "    max_grad_norm=0.3,                      # 그래디언트 폭주 방지 (최대 0.3까지만)\n",
        "    weight_decay=2e-4,                      # 과적합 방지 (가중치 조금씩 줄이기)\n",
        "\n",
        "    # === 저장 & 로그 ===\n",
        "    logging_steps=100,                      # 100번마다 진행상황 출력\n",
        "    save_strategy=\"epoch\",                  # 1 에포크 끝날 때마다 저장\n",
        "    save_total_limit=2,                     # 최신 2개만 보관 (나머지 삭제)\n",
        "    load_best_model_at_end=False,            # 학습 끝나면 가장 좋았던 모델 불러오기\n",
        "\n",
        "    # === 외부 연동 ===\n",
        "    push_to_hub=False,                      # 허깅페이스에 자동 업로드 안 함\n",
        "    report_to=\"none\"                        # 외부 실험 추적 도구 안 씀\n",
        ")"
      ],
      "metadata": {
        "id": "oNJtbjtrtwl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SFTTrainer 모델 학습\n",
        "\n",
        "`args`에는 `SFTConfig`을, `peft_config`에는 `LoraConfig`를 입력하여 모델에 Lora 미세조정을 시작합니다. 추가로 토큰의 클래스 정보를 위해 `processing_class` 인자에 토크나이져를 입력합니다."
      ],
      "metadata": {
        "id": "Plf7Pb89vwbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "# Create Trainer object\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_data,\n",
        "    peft_config=lora_config,\n",
        "    processing_class=tokenizer\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqxJqbdLvw-Y",
        "outputId": "8dcc2d3a-ace8-45b3-fa98-192443967d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 시작\n",
        "trainer.train()\n",
        "\n",
        "# 모델 저장\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        },
        "id": "LPWEhA121sVC",
        "outputId": "06599a56-ced4-4e6c-845f-ddb785673220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2775' max='2775' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2775/2775 1:39:25, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.608400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.373900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.323900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.265700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.285800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.241600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.194700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.233500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>2.162200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.156900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>2.180800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>2.161500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>2.156500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>2.171400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>2.123800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>2.128800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>2.105300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>2.098200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>2.104600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.109800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>2.102800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>2.082600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>2.073300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>2.094600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>2.113000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>2.088300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>2.084700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lora 모델 통합\n",
        "\n",
        "Lora 미세조정이 완료되면 결과적으로 추가된 Lora 계층 (저차원 분해 행렬 Adapter)의 가중치를 얻게됩니다.\n",
        "\n",
        "최종 적으로 해당 가중치를 기존 모델에 통합하여 배포 되어야만 미세조정의 결과가 적용된 모델로 활용이 가능합니다.  "
      ],
      "metadata": {
        "id": "QlstYTnK7nW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습된 Lora 가중치 파일 다운로드\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive(save_model,'zip',save_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gG2eiOkB7n9W",
        "outputId": "4cacfbd0-685c-4d09-8d71-aac1a477a8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gemma-text-to-sum4.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 약 26MB 크기의 Lora 체크포인트가 만들어집니다."
      ],
      "metadata": {
        "id": "t7pU8KpR7uBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메모리 초기화\n",
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "vX6i2Rg47wWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lora 가중치 합치기\n",
        "\n",
        "기존 모델에 Lora 학습 결과를 통합하는 방법은 먼저 Lora의 저차원 분해 행렬을 행렬곱 하여 Lora가 추가되었던 projection 레이어와 동일한 형상으로 맞춰준다음 더하기 연산을 진행합니다.\n",
        "\n",
        "Lora 가중치 통합을 위해 peft 모듈의 `PeftModel` 객체에서 기본 모델과 저장 경로를 함께 입력하여 불러오고 `merge_and_unload()` 함수를 호출합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "FML95fkl7x-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# 기본 모델 로드 (이미 로드 되었다면 생략 가능)\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, quantization_config=quantization_config\n",
        ").eval()\n",
        "\n",
        "# 기본 모델에 Lora 가중치 결합 (더하기)\n",
        "peft_model = PeftModel.from_pretrained(model, save_model)\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "\n",
        "# 단일 모델로 저장\n",
        "merged_model.save_pretrained(\"merged_model\",            # 저장 경로(safetensors)\n",
        "                             safe_serialization=True,   # 안정적인 저장방식\n",
        "                             max_shard_size=\"2GB\")      # 셰이딩 저장 단위(용량이 큰경우)\n",
        "\n",
        "# 토크나이져도 함께 저장 (생략 가능)\n",
        "processor = AutoTokenizer.from_pretrained(save_model)\n",
        "processor.save_pretrained(\"merged_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsvqo9zN7y2W",
        "outputId": "ab9690f2-0d78-4e7e-fcbe-a3afc08b18bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:110: UserWarning: Merge lora module to 8-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('merged_model/tokenizer_config.json',\n",
              " 'merged_model/special_tokens_map.json',\n",
              " 'merged_model/chat_template.jinja',\n",
              " 'merged_model/tokenizer.model',\n",
              " 'merged_model/added_tokens.json',\n",
              " 'merged_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| 항목 | pickle (.bin) | safetensors (.safetensors) |\n",
        "|:---:|:---:|:---:|\n",
        "| **안전성** | ❌ 위험 | ✅ 안전 |\n",
        "| **속도** | 빠름 | 더 빠름 ✅ |\n",
        "| **크기** | 보통 | 작음 ✅ |\n",
        "| **HuggingFace 지원** | 옛날 방식 | 최신 권장 ✅ |\n",
        "| **코드 실행** | 가능 (위험) | 불가능 ✅ |\n",
        "| **권장 여부** | ❌ | ✅ |"
      ],
      "metadata": {
        "id": "yZIHKQat9Ivq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 다운로드\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive('merged_model','zip','merged_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3eoxn5aJ-P1c",
        "outputId": "0ae9dfdc-d6ce-409d-8f08-8733fea9200b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/merged_model.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 약 1GB 크기의 통합 모델이 저장됩니다"
      ],
      "metadata": {
        "id": "91f4GPun-SC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# Lora 체크포인트에 기본 모델의 정보가 포함되어 같이 불러와짐\n",
        "# 통합 모델의 경로를 입력해도 사용가능 (merged_model)\n",
        "my_model = AutoModelForCausalLM.from_pretrained(save_model,device_map='auto').eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_model)\n"
      ],
      "metadata": {
        "id": "O6Tx6QcH-Tgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 평가\n",
        "\n",
        "학습된 Lora 가중치를 활용하여 뉴스 요약이 잘 진행되는지 확인해 봅니다."
      ],
      "metadata": {
        "id": "-uIPuKJQ-W-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 로드된 모델을 활용하는 파이프라인 생성\n",
        "pipe = pipeline(\"text-generation\", model=my_model, tokenizer=tokenizer)\n",
        "\n",
        "# 테스트 샘플\n",
        "test_sample = test_data[100]\n",
        "\n",
        "# Gemma 템플릿으로 입력 프롬프트 구성\n",
        "prompt = pipe.tokenizer.apply_chat_template(test_sample[\"messages\"][:1], tokenize=False, add_generation_prompt=True)\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "Kbq3VDeF-Y68",
        "outputId": "25fea446-eb04-45f6-98b7-6b67ad5f38b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<bos><start_of_turn>user\\n산업부·조달청 정책설명회 정부가 중견기업을 우리 산업의 새로운 성장 엔진으로 육성하기 위해 이들이 생산하는 제품을 공공조달을 통해 적극적으로 활용·육성하기로 했다. 중견기업은 우리나라 전체 공공조달 계약 규모 184조원 중 26조3000억원을 담당하고 있다. 산업통상자원부는 조달청과 함께 1일 서울 마포구 한국중견기업연합회에서 중견기업을 대상으로 공공조달 정책설명회를 개최하고 중견기업 제품의 공공조달 강화방안을 논의했다고 밝혔다. 이날 설명회는 중견기업 지원에 대한 정부인식 제고를 위해 마련된 가운데 조달청은 새 정부의 공공조달 정책방향을 설명했고 참석자들은 중견기업의 공공조달 관련 애로 및 건의 사항을 전달했다. 이종욱 조달청장은 “중견기업은 공공조달 시장 전체 기업 수의 0.7% 3487개 에 불과하지만 우리나라 전체 공공조달 계약 규모 184조원 중 26조3000억원을 담당한다”며 “중견기업이 공공조달 시장에 보다 활발하게 참여하고 국내를 넘어 해외조달시장으로 뻗어나갈 수 있도록 다각적인 지원방안을 강구하겠다”고 말했다. 장영진 사진 산업부 제1차관은 “우리 경제 역동성·활력을 제고하기 위해서는 기업 성장사다리의 핵심 연결고리인 중견기업에 대한 정부 지원체계 강화가 필수적”이라며 “이번 설명회를 시작으로 향후 중견기업계의 주요 애로사항과 관련된 부처를 대상으로 지속적인 소통을 하겠다”고 말했다.<end_of_turn>\\n<start_of_turn>model\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 생성을 중단할 토큰 리스트\n",
        "stop_token_ids = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")]\n",
        "\n",
        "# # Generate our SQL query.\n",
        "outputs = pipe(prompt,\n",
        "               max_new_tokens=256,              # 생성 토큰 최대 길이\n",
        "               do_sample=True,                  # 랜덤 샘플링 사용(다양한 후보 생성)\n",
        "               temperature=0.5,                 # 값이 낮을수록 높은 확률을 가진 후보 선택\n",
        "               top_k=50,                        # 샘플링에서 고려할 상위 k개 후보\n",
        "               top_p=0.9,                       # 누적 확률 p 이하의 후보\n",
        "               eos_token_id=stop_token_ids,     # 끝(중단) 토큰 설정\n",
        "               disable_compile=False,           # torch.compile() 최적화를 비활성화\n",
        "               no_repeat_ngram_size=2           # 동일한 엔그램 2번이상 반복 제거\n",
        "               )\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eas073WP-i9h",
        "outputId": "456564c0-a7d5-4a28-b718-0119787f61bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': '<bos><start_of_turn>user\\n산업부·조달청 정책설명회 정부가 중견기업을 우리 산업의 새로운 성장 엔진으로 육성하기 위해 이들이 생산하는 제품을 공공조달을 통해 적극적으로 활용·육성하기로 했다. 중견기업은 우리나라 전체 공공조달 계약 규모 184조원 중 26조3000억원을 담당하고 있다. 산업통상자원부는 조달청과 함께 1일 서울 마포구 한국중견기업연합회에서 중견기업을 대상으로 공공조달 정책설명회를 개최하고 중견기업 제품의 공공조달 강화방안을 논의했다고 밝혔다. 이날 설명회는 중견기업 지원에 대한 정부인식 제고를 위해 마련된 가운데 조달청은 새 정부의 공공조달 정책방향을 설명했고 참석자들은 중견기업의 공공조달 관련 애로 및 건의 사항을 전달했다. 이종욱 조달청장은 “중견기업은 공공조달 시장 전체 기업 수의 0.7% 3487개 에 불과하지만 우리나라 전체 공공조달 계약 규모 184조원 중 26조3000억원을 담당한다”며 “중견기업이 공공조달 시장에 보다 활발하게 참여하고 국내를 넘어 해외조달시장으로 뻗어나갈 수 있도록 다각적인 지원방안을 강구하겠다”고 말했다. 장영진 사진 산업부 제1차관은 “우리 경제 역동성·활력을 제고하기 위해서는 기업 성장사다리의 핵심 연결고리인 중견기업에 대한 정부 지원체계 강화가 필수적”이라며 “이번 설명회를 시작으로 향후 중견기업계의 주요 애로사항과 관련된 부처를 대상으로 지속적인 소통을 하겠다”고 말했다.<end_of_turn>\\n<start_of_turn>model\\n중중기업과 조약청이 중중개기업 기업의 생산 제품에 공중조약 계약을 적극 활용하고 육성을 위해 중소기업 4080개 중 중대기업 중 주관업체 885개에 대해 중기중대 기업을 지원하기 위한 정책을 발표했다는 가운데 중주관 5703개와 중장기 기업 604개 등 중공업을 위한 공조업계와 협력 방안에 중점했다 고 말했다고 밝혔다 가운데 정부는 조약을 통해 중지공공 조업 중 정부와 조화된 공통의 목표를 달성할 수 있다 고 강조했다'}]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[0]['generated_text'].strip('<start_of_turn>model')[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0630E70E-wPX",
        "outputId": "67f7d936-797e-49e4-e06b-0baafc2dce13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'o'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    }
  ]
}